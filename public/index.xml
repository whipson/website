<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Will Hipson</title>
    <link>/</link>
    <description>Recent content on Will Hipson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2020</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>/home/hero_carousel/</link>
      <pubDate>Sun, 15 Oct 2017 00:00:00 -0400</pubDate>
      
      <guid>/home/hero_carousel/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Academic</title>
      <link>/home/hero/</link>
      <pubDate>Sun, 15 Oct 2017 00:00:00 -0400</pubDate>
      
      <guid>/home/hero/</guid>
      <description>&lt;p&gt;The highly flexible website framework for Hugo with an extensible plugin mechanism. Create a beautifully simple site in under 10 minutes 🚀
&lt;div style=&#34;margin-top: -0.5rem;&#34;&gt;
  &lt;a id=&#34;academic-release&#34; href=&#34;https://sourcethemes.com/academic/updates&#34; data-repo=&#34;gcushen/hugo-academic&#34;&gt;
  Latest release &lt;!-- V --&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;div class=&#34;mt-3&#34;&gt;
  &lt;a class=&#34;github-button&#34; href=&#34;https://github.com/gcushen/hugo-academic&#34; data-icon=&#34;octicon-star&#34; data-size=&#34;large&#34; data-show-count=&#34;true&#34; aria-label=&#34;Star this on GitHub&#34;&gt;Star&lt;/a&gt;
&lt;/div&gt;
&lt;script async defer src=&#34;https://buttons.github.io/buttons.js&#34;&gt;&lt;/script&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/home/about/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>/home/about/</guid>
      <description>

&lt;h1 id=&#34;biography&#34;&gt;Biography&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m a graduate student at Carleton University, where I conduct research on emotions. I&amp;rsquo;m interested in how emotions change over time - both from moment-to-moment and over the course of human development. My current projects are focused on children&amp;rsquo;s and adolescents&amp;rsquo; emotion regulation and its links with social behaviors. I&amp;rsquo;m also interested in quantitative methodology, and applying new statistical approaches.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Skills</title>
      <link>/home/skills/</link>
      <pubDate>Wed, 20 Sep 2017 00:00:00 -0400</pubDate>
      
      <guid>/home/skills/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Experience</title>
      <link>/home/experience/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/home/experience/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>/home/projects/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>/home/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Accomplish&amp;shy;ments</title>
      <link>/home/accomplishments/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/home/accomplishments/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent Posts</title>
      <link>/home/posts/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>/home/posts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent Publications</title>
      <link>/home/publications/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>/home/publications/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Selected Talks</title>
      <link>/home/talks_selected/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>/home/talks_selected/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Selected Publications</title>
      <link>/home/publications_selected/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>/home/publications_selected/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Talks &amp; Posters</title>
      <link>/home/talks/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>/home/talks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>/home/contact/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>/home/contact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Teaching</title>
      <link>/home/teaching/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0400</pubDate>
      
      <guid>/home/teaching/</guid>
      <description>&lt;p&gt;This is an example of using the &lt;em&gt;custom&lt;/em&gt; widget to create your own homepage section.&lt;/p&gt;

&lt;p&gt;I am a teaching instructor for the following courses at University X:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CS101: An intro to computer science&lt;/li&gt;
&lt;li&gt;CS102: An intro to computer science&lt;/li&gt;
&lt;li&gt;CS103: An intro to computer science&lt;/li&gt;
&lt;li&gt;CS104: An intro to computer science&lt;/li&gt;
&lt;li&gt;CS105: An intro to computer science&lt;/li&gt;
&lt;li&gt;CS106: An intro to computer science&lt;/li&gt;
&lt;li&gt;CS107: An intro to computer science&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tags</title>
      <link>/home/tags/</link>
      <pubDate>Wed, 20 Sep 2017 00:00:00 -0400</pubDate>
      
      <guid>/home/tags/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Intuitive Look at Binomial Probability in a Bayesian Context</title>
      <link>/post/bayesian_intro/binomial_gold/</link>
      <pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian_intro/binomial_gold/</guid>
      <description>


&lt;p&gt;Binomial probability is the relatively simple case of estimating the proportion of &lt;em&gt;successes&lt;/em&gt; in a series of yes/no trials. The perennial example is estimating the proportion of heads in a series of coin flips where each trial is independent and has possibility of heads or tails. Because of its relative simplicity, the binomial case is a great place to start when learning about Bayesian analysis. In this post, I will provide a gentle introduction to Bayesian analysis using binomial probability as an example. Instead of coin flips, we’ll imagine a scenario where we are mining for gold!&lt;/p&gt;
&lt;div id=&#34;setting-the-scene&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting the Scene&lt;/h2&gt;
&lt;p&gt;There’s talk in the town that gold is to be found in the nearby hills! We focus on a local merchant who buys gold from prospectors to trade with neighbouring towns. The merchant needs to know &lt;em&gt;how much gold is out there&lt;/em&gt; so that they can set a competitive price for buying and selling. The merchant decides to investigate the proportion of gold in the hills by collecting data. It is impossible to survey the entire landscape, but the merchant assumes that a sample of the hills will provide a good estimate of the total proportion of gold.&lt;/p&gt;
&lt;p&gt;The merchant’s goal is to estimate the true proportion of gold in the hills. We will use &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; to represent this quantity that we are trying to estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating the Problem&lt;/h2&gt;
&lt;p&gt;For this example, we will simulate the entire landscape as a 100 x 100 grid. We will allow each of the 10,000 points on the grid to have a probability of .10 (10%) of containing gold. This is our chosen value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; for the simulation. We’ll also identify a sample 10 x 10 grid. Here’s a plot of the simulated landscape identifying the location of gold and the sample area.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Remember we are omniscient but our merchant knows nothing about where the gold is and how much there truly is. Bayesian statistics is all about dealing with uncertainty by incorporating information from new data &lt;em&gt;and&lt;/em&gt; prior sources of information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayes’ Theorem&lt;/h2&gt;
&lt;p&gt;I’m sure that most readers have encountered Bayes’ theorem at some point in their stats journey, although maybe the notation was different. We are most interested in computing the &lt;em&gt;posterior probability&lt;/em&gt;, which we denote as &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt;. It combines information obtained from the data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and our prior information (what we already know about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;). Expressed more formally,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=\frac{p(y|\theta)p(\theta)}{p(y)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt; is the posterior probability; &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt; is the data-generating process, usually referred to as the &lt;em&gt;likelihood&lt;/em&gt;; &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; is the prior probability of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;; and &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; is a normalizing constant. We won’t worry ourselves with the normalizing constant, &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; as it doesn’t affect our conclusions. Thus, we’ll stick with the nonnormalized posterior likelihood:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=p(y|\theta)p(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;At first, this formula can be quite confusing. When I first encountered it, I was confused by what &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; represented (and this stemmed from a fuzzy understanding of likelihood estimation I had at the time). What’s important to understand is that &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is an unknown parameter, but in order to estimate our uncertainty about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; we are going to try out &lt;strong&gt;different values&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For example, we want to know the proportion of gold in the area which ranges from 0 to 1 (0 being no gold; 1 being all gold). So in our equation, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; would represent all values between 0 and 1. However, there are infinite values between 0 and 1, so in practice we could try out values between 0 and 1. In this example, we will use a relatively simple process called &lt;em&gt;grid approximation&lt;/em&gt; where we use an equally spaced grid of values between 0 and 1 for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;binomial-likelihood&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Binomial Likelihood&lt;/h3&gt;
&lt;p&gt;Let’s tackle the first piece of Bayes’ theorem: the likelihood, &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt;. Here, we are essentially asking the question: “how likely is the data, given a particular value for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;?” Here, we introduce the binomial likelihood function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta)=\theta^y(1-\theta)^{n-y}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the number of successes and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of trials.&lt;/p&gt;
&lt;p&gt;Let’s return to our gold merchant and see how we can express the likelihood in terms of the data the merchant observes. Imagine our merchant collects data by digging each of the 10 x 10 squares in the sample. The merchant finds that 14 out of 100 spaces in the sample contain gold:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s how we express the data in terms of the binomial likelihood function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta)=\theta^{14}(1-\theta)^{100-14}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we compute &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt; across the grid of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values from 0 to 1, we get the following probability density:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Reproducing this in R is fairly simple - we could substitute the values into the binomial formula, or just use the built-in &lt;em&gt;dbinom&lt;/em&gt; function. We then create a dataframe containing the likelihood for each theta and use &lt;em&gt;ggplot2&lt;/em&gt; from the &lt;em&gt;tidyverse&lt;/em&gt; to draw the plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

# use seq(.01, 1, .01) to generate a sequence from .01 to 1 in .01 increments
likelihood &amp;lt;- dbinom(x = 14, size = 100, prob = seq(.01, 1, .01))

# creating a data frame with &amp;#39;likelihood&amp;#39; for plotting
df &amp;lt;- data.frame(theta = seq(.01, 1, .01),
                 likelihood)

# create the plot
ggplot(df, aes(x = theta, y = likelihood)) +
  geom_line(size = 1) +
  labs(x = TeX(&amp;quot;$\\theta$&amp;quot;),
       y = &amp;quot;Likelihood&amp;quot;,
       title = TeX(&amp;quot;$p(y|\\theta) = \\theta^{14}(1 - \\theta)^{100 - 14}$&amp;quot;),
       subtitle = &amp;quot;Dashed red line indicates maximum likelihood estimate.&amp;quot;,
       caption = &amp;quot;Using nonnormalized likelihood.&amp;quot;) +
  geom_vline(xintercept = .14, size = 1.25, color = &amp;quot;red&amp;quot;, linetype = 2) +
  scale_x_continuous(breaks = seq(0, 1, by = .10)) +
  theme_minimal(base_size = 14) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.title = element_text(hjust = .5, size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not surprisingly, the most likely value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; (the maximum likelihood estimate) is .14. In Bayesian statistics, however, we aren’t content with just the point estimate and instead we use the entire density to express uncertainty about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;incorporating-prior-information&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Incorporating Prior Information&lt;/h3&gt;
&lt;p&gt;Imagine that our merchant hears rumours that each 100 square meters of land has a 20% probability of containing gold. We now want to incorporate this prior information into our model. &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; is the prior probability of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and it can represent past data or even subjective knowledge about the likely value and uncertainty of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A common approach when choosing priors is to identify a &lt;em&gt;conjugate prior&lt;/em&gt;: a formula for expressing the prior that has a similar data structure to that of the likelihood. In this case, a conjugate prior for the binomial likelihood is the beta function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{beta}(a,b)=\theta^{a-1}(1-\theta)^{b-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; can represent successes and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; can represent failures.&lt;/p&gt;
&lt;p&gt;Notice the similarity between the formulas for the binomial and beta functions. They have identical data structures, which makes the beta a conjugate prior for the binomial likelihood. Let’s use a &lt;span class=&#34;math inline&#34;&gt;\(\text{beta}(2,8)\)&lt;/span&gt; as a prior, representing our knowledge of a rumored 20% probability of gold.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly, you can reproduce this using the &lt;em&gt;dbeta&lt;/em&gt; function in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# here we use x as a sequence from .01-1
b_likelihood &amp;lt;- dbeta(x = seq(.01, 1, .01), shape1 = 2, shape2 = 8)

df &amp;lt;- data.frame(theta = seq(.01, 1, .01),
                 b_likelihood)

df %&amp;gt;%
  ggplot(aes(x = theta, y = b_likelihood)) +
  geom_line(size = 1) +
  labs(x = TeX(&amp;quot;$\\theta$&amp;quot;),
       y = NULL,
       title = TeX(&amp;quot;beta(2,8)&amp;quot;)) +
  scale_x_continuous(breaks = seq(0, 1, by = .10)) +
  theme_minimal(base_size = 14) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.title = element_text(hjust = .5, size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why use a beta prior instead of another binomial density? The reason is that the binomial density is dependent on a sample size parameter &lt;em&gt;n&lt;/em&gt;, whereas the beta density is not. It is more convenient to express our prior knowledge without &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-the-posterior&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting the Posterior&lt;/h3&gt;
&lt;p&gt;The posterior probability, &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt; takes into account the data and the prior. It is often viewed as a &lt;em&gt;compromise&lt;/em&gt; between the data likelihood and the prior probability. This is more easily understood graphically:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s how to produce this figure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theta &amp;lt;- seq(.01, 1, .01)
prior &amp;lt;- dbeta(x = theta, shape1 = 2, shape2 = 8)

#multiply the binomial likelihood by 100 to express it in &amp;#39;probability terms&amp;#39; like the beta
likelihood &amp;lt;- dbinom(x = 14, size = 100, prob = theta) * 100
posterior_density &amp;lt;- likelihood * prior

df &amp;lt;- data.frame(theta = theta,
                 likelihood = likelihood,
                 prior = prior,
                 pd = posterior_density)

# use pivot_longer to create a variable out of likelihood vs. prior vs. posterior
data_long &amp;lt;- pivot_longer(df, cols = c(likelihood, prior, pd))

data_long %&amp;gt;%
  ggplot(aes(x = theta, y = value, color = name)) +
  geom_line(size = 1.5) +
  labs(x = TeX(&amp;quot;$\\theta$&amp;quot;),
       y = NULL,
       color = NULL,
       title = &amp;quot;Posterior, Likelihood, and Prior&amp;quot;) +
  scale_color_discrete(labels = c(&amp;quot;Likelihood&amp;quot;, &amp;quot;Posterior&amp;quot;, &amp;quot;Prior&amp;quot;)) +
  scale_x_continuous(breaks = seq(0, 1, by = .10)) +
  theme_minimal(base_size = 16) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.title = element_text(hjust = .5, size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above chart shows the posterior, likelihood, and prior side by side. Notice how the prior (in blue) contains less certainty than the likelihood. The posterior density is simply the prior multiplied by the likelihood, thus it contains information from both sources. It’s a general rule that in instances when there is a lot of data &lt;em&gt;and&lt;/em&gt; our prior is mostly uninformative that the likelihood will overwhelm the prior.&lt;/p&gt;
&lt;p&gt;If we were to extend this further to make actual conclusions about our uncertainty of the proportion of gold in the hills, we would need to draw random samples from the posterior distribution and generate interval estimates based on these draws. For this post, however, we will stick to a graphical evaluation of &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;an-animated-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Animated Example&lt;/h2&gt;
&lt;p&gt;I’m a huge proponent of using animations to illustrate complicated topics (if you haven’t yet, you should consider watching &lt;a href=&#34;https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw&#34;&gt;3Blue1Brown’s videos&lt;/a&gt; covering mathematics concepts). So I want to end by bringing to life our example of a gold miner estimating the proportion of gold in a section of land. We’ll use the same information as before, but the difference this time is that the gold miner will investigate each 1 x 1 section of land at a time and update the likelihood.&lt;/p&gt;
&lt;p&gt;The animation begins with our merchant-miner (indicated by the red square) on square 1,1. As the merchant covers the sample, the influence of the likelihood over the prior increases and there is more certainty in the estimate. If we expanded our sample to cover more of the territory, our estimate would get even closer to the true value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and we’d be more certain about it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-10-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The binomial likelihood serves as a great introductory case into Bayesian statistics. It may seem like overkill to use a Bayesian approach to estimate a binomial proportion, indeed the point estimate equals the sample proportion. But remember that it’s far more important to get an estimate of uncertainty as opposed to a simple point estimate.&lt;/p&gt;
&lt;p&gt;There’s a lot we didn’t cover here; namely, making inferences from the posterior distribution, which typically involves sampling from the posterior distribution. As well, I hope to soon extend into more practical cases such as logistic regression, mixture modeling, etc, with demonstrations using Stan.&lt;/p&gt;
&lt;p&gt;If you’re eager for more examples and tutorials with Bayesian statistics, I’d recommend watching this &lt;a href=&#34;https://www.youtube.com/watch?v=3OJEae7Qb_o&#34;&gt;video series&lt;/a&gt; by Rasmus Bååth. John Kruschke’s book &lt;a href=&#34;http://doingbayesiandataanalysis.blogspot.com/&#34;&gt;Doing Bayesian Data Analysis&lt;/a&gt; is a great introductory text. If you’re up for the challenge, consider also &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;Bayesian Data Analysis, 3rd edition&lt;/a&gt; by Gelman et al, widely considered to be the &lt;em&gt;Bayesian Bible&lt;/em&gt;. Much of the content for this blog post was inspired from the first few chapters of that book.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Building a Shiny App for Cycling in Ottawa</title>
      <link>/post/bicycle_app/bicycle_app/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bicycle_app/bicycle_app/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/leaflet/leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/leafletfix/leafletfix.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/Proj4Leaflet/proj4-compressed.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/Proj4Leaflet/proj4leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/rstudio_leaflet/rstudio_leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet-binding/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet-providers/leaflet-providers.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet-providers-plugin/leaflet-providers-plugin.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a different kind of post, but one that I think is kind of fun. I currently live in Ottawa, which for those who don’t know, is the capital city of Canada. For a capital city, it’s fairly small, but it’s increasingly urbanizing (we just got lightrail transit). Segregated bicycle lanes and paths are becoming more common too and many of these paths have trackers on them that count how many bicycles cross a particular street or path each day. What’s great is that this &lt;a href=&#34;https://open.ottawa.ca/datasets/bicycle-trip-counters&#34;&gt;data is shared publicly&lt;/a&gt; by the city.&lt;/p&gt;
&lt;p&gt;I started looking into this data, &lt;a href=&#34;https://github.com/whipson/Ottawa_Bicycles/blob/master/cleaning.R&#34;&gt;cleaned it up&lt;/a&gt;, and eventually put it together in an &lt;a href=&#34;https://whipson.shinyapps.io/Ottawa_Bike_Counters/&#34;&gt;interactive web app&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/bicycle_app/Bicycle_App_files/ottawa%20bike%20counters.png&#34; alt=&#34;Click here to go the app.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Click &lt;a href=&#34;https://whipson.shinyapps.io/Ottawa_Bike_Counters/&#34;&gt;here&lt;/a&gt; to go the app.&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(leaflet)
library(leafpop)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll start by reading in the data from the GitHub repositiory. There’s a lot of missing data, so much that R gets confused about the data structure of some of the columns. We need to add another argument to &lt;em&gt;read_csv&lt;/em&gt; telling it the type of data in each column. The &lt;em&gt;col_types&lt;/em&gt; argument takes a letter for each column, with &lt;em&gt;?&lt;/em&gt; meaning that we let R decide what the data is and &lt;em&gt;n&lt;/em&gt; meaning ‘numeric’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bikes &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/whipson/Ottawa_Bicycles/master/bikes_app.csv&amp;quot;, col_types = c(&amp;quot;?nnnnnnnnnnnnnn&amp;quot;))

bikes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3,560 x 15
##    date                alexandra_bridge eastern_canal ottawa_river
##    &amp;lt;dttm&amp;gt;                         &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 2010-01-01 00:00:00                0             0            0
##  2 2010-01-02 00:00:00                0             0            0
##  3 2010-01-03 00:00:00                0             0            0
##  4 2010-01-04 00:00:00                0             0            0
##  5 2010-01-05 00:00:00                0             0            0
##  6 2010-01-06 00:00:00                0             0            0
##  7 2010-01-07 00:00:00                0             0            0
##  8 2010-01-08 00:00:00                0             0            0
##  9 2010-01-09 00:00:00                0             0            0
## 10 2010-01-10 00:00:00                0             0            0
## # ... with 3,550 more rows, and 11 more variables: western_canal &amp;lt;dbl&amp;gt;,
## #   laurier_bay &amp;lt;dbl&amp;gt;, laurier_lyon &amp;lt;dbl&amp;gt;, laurier_metcalfe &amp;lt;dbl&amp;gt;,
## #   somerset_bridge &amp;lt;dbl&amp;gt;, otrain_young &amp;lt;dbl&amp;gt;, otrain_gladstone &amp;lt;dbl&amp;gt;,
## #   otrain_bayview &amp;lt;dbl&amp;gt;, portage_bridge &amp;lt;dbl&amp;gt;, adawe_crossing_a &amp;lt;dbl&amp;gt;,
## #   adawe_crossing_b &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row is a day and the columns are bicycle counters spread across the city. Let’s start by creating the graphs we want in the Shiny app. It’s easier to do this outside of the Shiny framework first. We’ll start by plotting total bicycle counts over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bikes_total &amp;lt;- bikes %&amp;gt;%
  pivot_longer(names_to = &amp;quot;counter&amp;quot;, values_to = &amp;quot;count&amp;quot;, -date) %&amp;gt;%
  group_by(date) %&amp;gt;%
  mutate(daily_total = sum(count, na.rm = TRUE))

bikes_total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 49,840 x 4
## # Groups:   date [3,560]
##    date                counter          count daily_total
##    &amp;lt;dttm&amp;gt;              &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 2010-01-01 00:00:00 alexandra_bridge     0           0
##  2 2010-01-01 00:00:00 eastern_canal        0           0
##  3 2010-01-01 00:00:00 ottawa_river         0           0
##  4 2010-01-01 00:00:00 western_canal       NA           0
##  5 2010-01-01 00:00:00 laurier_bay         NA           0
##  6 2010-01-01 00:00:00 laurier_lyon        NA           0
##  7 2010-01-01 00:00:00 laurier_metcalfe    NA           0
##  8 2010-01-01 00:00:00 somerset_bridge     NA           0
##  9 2010-01-01 00:00:00 otrain_young        NA           0
## 10 2010-01-01 00:00:00 otrain_gladstone    NA           0
## # ... with 49,830 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now to plot it over time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bikes_total %&amp;gt;%
  ggplot(aes(x = date, y = daily_total)) +
  geom_line(size = .5, alpha = .80, color = &amp;quot;#36648B&amp;quot;) +
  scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
  labs(x = NULL,
       y = &amp;quot;Count&amp;quot;,
       title = &amp;quot;Total Bicycle Crossings in Ottawa&amp;quot;,
       subtitle = &amp;quot;Jan 2010 - Sep 2019&amp;quot;) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5),
        axis.text.x = element_text(size = 16))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bicycle_app/Bicycle_App_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s clear seasonality, with bicycle crossings peaking in the summer months and troughing in the winter. There also appears to be a trend, increasing from 2010 to 2017, then leveling out. Does this mean that bicycling is leveling off in Ottawa? We may want to look at specific counters to get a better sense of this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bikes %&amp;gt;%
  pivot_longer(names_to = &amp;quot;counter&amp;quot;, values_to = &amp;quot;count&amp;quot;, -date) %&amp;gt;% 
  ggplot(aes(x = date, y = count)) +
  geom_line(size = .5, alpha = .80, color = &amp;quot;#36648B&amp;quot;) +
  labs(x = NULL,
       y = &amp;quot;Count&amp;quot;,
       title = &amp;quot;Bicycle Crossings in Ottawa by Location&amp;quot;,
       subtitle = &amp;quot;Jan 2010 - Sep 2019&amp;quot;) +
  facet_wrap(~counter) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5),
        axis.text.x = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2191 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bicycle_app/Bicycle_App_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This graph tells us that we have to be a bit careful about interpreting the total count because some counters are introduced later or go out of commission. The drop in total counts for 2018 could be due to the Western Canal counter going offline that year. What about average counts over time?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bikes %&amp;gt;%
  pivot_longer(names_to = &amp;quot;counter&amp;quot;, values_to = &amp;quot;count&amp;quot;, -date) %&amp;gt;%
  group_by(date) %&amp;gt;%
  mutate(daily_average = mean(count, na.rm = TRUE)) %&amp;gt;%
  ggplot(aes(x = date, y = daily_average)) +
  geom_line(size = .5, alpha = .80, color = &amp;quot;#36648B&amp;quot;) +
  scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
  labs(x = NULL,
       y = &amp;quot;Count&amp;quot;,
       title = &amp;quot;Average Bicycle Crossings in Ottawa&amp;quot;,
       subtitle = &amp;quot;Jan 2010 - Sep 2019&amp;quot;) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5),
        axis.text.x = element_text(size = 16))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bicycle_app/Bicycle_App_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There may be an upward trend, but it’s less clear compared to the total count. We again have to be careful because earlier years have fewer counters online so the average is based on less data. However, knowing both the total and the average counts gives us a pretty clear picture of how cycling is changing over time in Ottawa.&lt;/p&gt;
&lt;div id=&#34;maps-with-leaflet&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maps with Leaflet&lt;/h2&gt;
&lt;p&gt;Now we’ll add the functionality of an interactive map - one that shows where the counters are located geographically and allows the user to select specific counters. Earlier we loaded up the &lt;em&gt;leaflet&lt;/em&gt; and &lt;em&gt;leafpop&lt;/em&gt; packages. These will help us construct our map of Ottawa.&lt;/p&gt;
&lt;p&gt;We’ll also need the latitude and longitude coordinates of the counters. Using information from the &lt;a href=&#34;https://open.ottawa.ca/datasets/bicycle-trip-counters&#34;&gt;Open Data Ottawa&lt;/a&gt;, I found the location of each counter and obtained its latitude and longitude using Google Maps. I also added a bit of descriptive information for each counter. We can put all of this in a dataframe as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coords &amp;lt;- data.frame(counter = names(bikes[,2:15]),
                     name = c(&amp;quot;Alexandra Bridge&amp;quot;, &amp;quot;Eastern Canal Pathway&amp;quot;, &amp;quot;Ottawa River Pathway&amp;quot;, &amp;quot;Western Canal Pathway&amp;quot;,
                              &amp;quot;Laurier at Bay&amp;quot;, &amp;quot;Laurier at Lyon&amp;quot;, &amp;quot;Laurier at Metcalfe&amp;quot;, &amp;quot;Somerset Bridge&amp;quot;, &amp;quot;OTrain at Young&amp;quot;,
                              &amp;quot;OTrain at Gladstone&amp;quot;, &amp;quot;OTrain at Bayview&amp;quot;, &amp;quot;Portage Bridge&amp;quot;, &amp;quot;Adawe Crossing A&amp;quot;, &amp;quot;Adawe Crossing B&amp;quot;),
                     lat = c(45.430366, 45.420924, 45.411959, 45.406280,
                             45.415893, 45.417036, 45.419790, 45.420512,
                             45.402859, 45.404599, 45.408636, 45.421980, 
                             45.426282, 45.426575),
                     long = c(-75.704761, -75.685060, -75.723424, -75.681814,
                              -75.705328, -75.702613, -75.697623, -75.684625,
                              -75.712760, -75.714812, -75.723644, -75.713324,
                              -75.670234, -75.669765),
                     desc = c(&amp;quot;Ottawa approach to the NCC Alexandra Bridge Bikeway. This counter was not operational for most of 2010
                              due to bridge construction. This is one of the more consistent counters, until the internal battery
                              failed in August 2019.&amp;quot;,
                              &amp;quot;NCC Eastern Canal Pathway approximately 100m north of the Corktown Bridge.&amp;quot;,
                              &amp;quot;NCC Ottawa River Pathway approximately 100m east of the Prince of Wales Bridge. Canada Day in 2011
                              boasts the highest single day count of any counter.&amp;quot;,
                              &amp;quot;NCC Western Canal Pathway approximately 200m north of “The Ritz”. Out of operation for much of 2018.
                              MEC Bikefest on May 17, 2015 accounts for the large spike that day.&amp;quot;,
                              &amp;quot;Laurier Segregated Bike lane just west of Bay. Minimal data available due to inactivity after 2014.&amp;quot;,
                              &amp;quot;Laurier Segregated Bike lane just east of Lyon. No longer in operation since 2016.&amp;quot;,
                              &amp;quot;Laurier Segregated Bike lane just west of Metcalfe. Construction in late 2012 accounts for unusual dip
                              in counts.&amp;quot;,
                              &amp;quot;Somerset bridge over O-Train west-bound direction only. Inexplicably large spike in 2012 followed by a
                              typical seasonal pattern. Inactive since late 2018.&amp;quot;,
                              &amp;quot;O-Train Pathway just north of Young Street. Minimal data available due to inactivity after 2016. See
                              O-Train at Gladstone counter for a better estimate.&amp;quot;,
                              &amp;quot;O-Train Pathway just north of Gladstone Avenue. In operation since mid-2013. Shows unusual spike in
                              November of 2017.&amp;quot;,
                              &amp;quot;O-Train Pathway just north of Bayview Station. In operation since mid-2013. Trending upward.&amp;quot;,
                              &amp;quot;Portage Bridge connecting Gatineau to Ottawa. Installed in late 2013, this counter registered
                              relatively high traffic but seems to have experienced outages during Winter months. Inactive since early
                              2016.&amp;quot;,
                              &amp;quot;Adàwe Crossing Bridge bike lane. This counter is one of a pair on this pedestrian bridge. Installed in
                              2016, it seems to have experienced an outage during the Winter of its inaugural year.&amp;quot;,
                              &amp;quot;The second of two counters on the Adàwe Crossing Bridge. This counter may pick up more pedestrian than
                              bike traffic, as suggested by the trend over time.&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we just pipe the coordinate data into leaflet.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;leaflet(data = coords) %&amp;gt;%
  addTiles() %&amp;gt;%
  addMarkers(~long, ~lat)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;leaflet html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;options&#34;:{&#34;crs&#34;:{&#34;crsClass&#34;:&#34;L.CRS.EPSG3857&#34;,&#34;code&#34;:null,&#34;proj4def&#34;:null,&#34;projectedBounds&#34;:null,&#34;options&#34;:{}}},&#34;calls&#34;:[{&#34;method&#34;:&#34;addTiles&#34;,&#34;args&#34;:[&#34;//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&#34;,null,null,{&#34;minZoom&#34;:0,&#34;maxZoom&#34;:18,&#34;tileSize&#34;:256,&#34;subdomains&#34;:&#34;abc&#34;,&#34;errorTileUrl&#34;:&#34;&#34;,&#34;tms&#34;:false,&#34;noWrap&#34;:false,&#34;zoomOffset&#34;:0,&#34;zoomReverse&#34;:false,&#34;opacity&#34;:1,&#34;zIndex&#34;:1,&#34;detectRetina&#34;:false,&#34;attribution&#34;:&#34;&amp;copy; &lt;a href=\&#34;http://openstreetmap.org\&#34;&gt;OpenStreetMap&lt;\/a&gt; contributors, &lt;a href=\&#34;http://creativecommons.org/licenses/by-sa/2.0/\&#34;&gt;CC-BY-SA&lt;\/a&gt;&#34;}]},{&#34;method&#34;:&#34;addMarkers&#34;,&#34;args&#34;:[[45.430366,45.420924,45.411959,45.40628,45.415893,45.417036,45.41979,45.420512,45.402859,45.404599,45.408636,45.42198,45.426282,45.426575],[-75.704761,-75.68506,-75.723424,-75.681814,-75.705328,-75.702613,-75.697623,-75.684625,-75.71276,-75.714812,-75.723644,-75.713324,-75.670234,-75.669765],null,null,null,{&#34;interactive&#34;:true,&#34;draggable&#34;:false,&#34;keyboard&#34;:true,&#34;title&#34;:&#34;&#34;,&#34;alt&#34;:&#34;&#34;,&#34;zIndexOffset&#34;:0,&#34;opacity&#34;:1,&#34;riseOnHover&#34;:false,&#34;riseOffset&#34;:250},null,null,null,null,null,{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null]}],&#34;limits&#34;:{&#34;lat&#34;:[45.402859,45.430366],&#34;lng&#34;:[-75.723644,-75.669765]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Leaflet automatically generates a map of size to fit all the markers. There are a few modifications to make though. One is to have it so that when the user hovers the mouse over a marker a label pops up with the name of that counter. Another is to make the map more aesthetically pleasing. Finally, we may want to add some bounds so that the user can’t scroll too far away from the markers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;leaflet(data = coords) %&amp;gt;%
  addTiles() %&amp;gt;%
  addMarkers(~long, ~lat, label = ~name) %&amp;gt;%
  setMaxBounds(-75.65, 45.38, -75.75, 45.46) %&amp;gt;%
  addProviderTiles(providers$CartoDB.Positron)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;leaflet html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;options&#34;:{&#34;crs&#34;:{&#34;crsClass&#34;:&#34;L.CRS.EPSG3857&#34;,&#34;code&#34;:null,&#34;proj4def&#34;:null,&#34;projectedBounds&#34;:null,&#34;options&#34;:{}}},&#34;calls&#34;:[{&#34;method&#34;:&#34;addTiles&#34;,&#34;args&#34;:[&#34;//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&#34;,null,null,{&#34;minZoom&#34;:0,&#34;maxZoom&#34;:18,&#34;tileSize&#34;:256,&#34;subdomains&#34;:&#34;abc&#34;,&#34;errorTileUrl&#34;:&#34;&#34;,&#34;tms&#34;:false,&#34;noWrap&#34;:false,&#34;zoomOffset&#34;:0,&#34;zoomReverse&#34;:false,&#34;opacity&#34;:1,&#34;zIndex&#34;:1,&#34;detectRetina&#34;:false,&#34;attribution&#34;:&#34;&amp;copy; &lt;a href=\&#34;http://openstreetmap.org\&#34;&gt;OpenStreetMap&lt;\/a&gt; contributors, &lt;a href=\&#34;http://creativecommons.org/licenses/by-sa/2.0/\&#34;&gt;CC-BY-SA&lt;\/a&gt;&#34;}]},{&#34;method&#34;:&#34;addMarkers&#34;,&#34;args&#34;:[[45.430366,45.420924,45.411959,45.40628,45.415893,45.417036,45.41979,45.420512,45.402859,45.404599,45.408636,45.42198,45.426282,45.426575],[-75.704761,-75.68506,-75.723424,-75.681814,-75.705328,-75.702613,-75.697623,-75.684625,-75.71276,-75.714812,-75.723644,-75.713324,-75.670234,-75.669765],null,null,null,{&#34;interactive&#34;:true,&#34;draggable&#34;:false,&#34;keyboard&#34;:true,&#34;title&#34;:&#34;&#34;,&#34;alt&#34;:&#34;&#34;,&#34;zIndexOffset&#34;:0,&#34;opacity&#34;:1,&#34;riseOnHover&#34;:false,&#34;riseOffset&#34;:250},null,null,null,null,[&#34;Alexandra Bridge&#34;,&#34;Eastern Canal Pathway&#34;,&#34;Ottawa River Pathway&#34;,&#34;Western Canal Pathway&#34;,&#34;Laurier at Bay&#34;,&#34;Laurier at Lyon&#34;,&#34;Laurier at Metcalfe&#34;,&#34;Somerset Bridge&#34;,&#34;OTrain at Young&#34;,&#34;OTrain at Gladstone&#34;,&#34;OTrain at Bayview&#34;,&#34;Portage Bridge&#34;,&#34;Adawe Crossing A&#34;,&#34;Adawe Crossing B&#34;],{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null]},{&#34;method&#34;:&#34;setMaxBounds&#34;,&#34;args&#34;:[45.38,-75.65,45.46,-75.75]},{&#34;method&#34;:&#34;addProviderTiles&#34;,&#34;args&#34;:[&#34;CartoDB.Positron&#34;,null,null,{&#34;errorTileUrl&#34;:&#34;&#34;,&#34;noWrap&#34;:false,&#34;detectRetina&#34;:false}]}],&#34;limits&#34;:{&#34;lat&#34;:[45.402859,45.430366],&#34;lng&#34;:[-75.723644,-75.669765]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Great. So we now have the two components of the app: the time plots and the map. Time to bring in Shiny and put it all together. Now, if you have never used Shiny before, this probably isn’t the easiest example to start with. I’d highly recommend this &lt;a href=&#34;https://shiny.rstudio.com/tutorial/&#34;&gt;set of tutorial videos by Garrett Grolemund&lt;/a&gt; to get started.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-shiny-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating the Shiny App&lt;/h2&gt;
&lt;p&gt;There are two parts to every Shiny app: the &lt;em&gt;UI&lt;/em&gt; or User Interface and the &lt;em&gt;Server&lt;/em&gt;. The UI is like the look and feel of the app, it’s where we tell Shiny what kinds of inputs and outputs we want, how we want to organize the panels, and so on. In contrast, the Server is the engine of the app. We’ll start by constructing the UI. It’s important to note that it’s easier to build a Shiny app in a new R script. So we’re basically going to start over in a new script, which means we’ll reload the packages and the data as if we were starting new:&lt;/p&gt;
&lt;div id=&#34;create-a-new-r-script&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create a new R script&lt;/h3&gt;
&lt;p&gt;We’ll start with the packages and data. We haven’t done anything with the UI or Server yet. We usually want to keep the data outside the UI. We’ll also transform our data as we did earlier to generate the total and average time plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(leaflet)
library(leafpop)
library(shiny)
library(shinythemes)
library(shinyWidgets)

bikes &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/whipson/Ottawa_Bicycles/master/bikes_app.csv&amp;quot;, col_types = c(&amp;quot;?nnnnnnnnnnnnnn&amp;quot;))

#For ease, I&amp;#39;ve put the coordinates in a separate file, but you could just as easily rerun the &amp;#39;coords&amp;#39; object above

coords &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/whipson/Ottawa_Bicycles/master/coords.csv&amp;quot;)

bikes_plot &amp;lt;- bikes %&amp;gt;%
  pivot_longer(names_to = &amp;quot;counter&amp;quot;, values_to = &amp;quot;count&amp;quot;, -date) %&amp;gt;%
  left_join(coords, by = &amp;quot;counter&amp;quot;)

bikes_total &amp;lt;- bikes_plot %&amp;gt;%
  group_by(date) %&amp;gt;%
  summarize(count = sum(count, na.rm = TRUE))

bikes_mean &amp;lt;- bikes_plot %&amp;gt;%
  group_by(date) %&amp;gt;%
  summarize(count = mean(count, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, still in the same R script, we can build the UI. It’s going to look a bit strange with parentheses all over the place. It’s just customary Shiny scripting to use hanging parentheses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specifying-the-ui&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specifying the UI&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ui &amp;lt;- fluidPage(theme = shinytheme(&amp;quot;flatly&amp;quot;),

  sidebarLayout(  #Layout
    
    sidebarPanel(id = &amp;quot;Sidebar&amp;quot;,  #Side panel
                 h2(&amp;quot;Ottawa Bicycle Counters&amp;quot;, align = &amp;quot;center&amp;quot;, tags$style(&amp;quot;#Sidebar{font-family: Verdana;}&amp;quot;)),
                 fluidRow(  # Row 1 of side panel
                   htmlOutput(&amp;quot;caption&amp;quot;),  # Caption output, provides descriptive text
                   tags$style(&amp;quot;#caption{font-size: 16px; height: 200px; font-family: Verdana;}&amp;quot;)
                 ),
                 fluidRow(  # Row 2 of side panel
                   htmlOutput(&amp;quot;stats&amp;quot;),  # Statistics output, provides descriptive statistics 
                   tags$style(&amp;quot;#stats{font-size: 16px; height: 125px; font-family: Verdana;}&amp;quot;)
                 ),
                 fluidRow(  # Row 3 of side panel
                   switchInput(&amp;quot;average&amp;quot;,  # User input, allows the user to turn a switch to display the average
                               &amp;quot;Display Average&amp;quot;,
                               value = FALSE)
                 ),
                 fluidRow(  # Row 4 of side panel
                   htmlOutput(&amp;quot;caption2&amp;quot;),  # More caption output
                   tags$style(&amp;quot;#caption2{font-size: 12px; height: 80px; font-family: Verdana;}&amp;quot;)
                   ),
                 fluidRow(  # Row 5 of side panel 
                   downloadButton(&amp;quot;download&amp;quot;, &amp;quot;Download Data&amp;quot;)  # A button so that users can download the data
                   )
                 ),
    mainPanel(id = &amp;quot;Main&amp;quot;,  # Main panel (this is where the plots and map go)
              fluidRow(  # Row 1 of main panel
                leafletOutput(&amp;quot;map&amp;quot;, height = 400)  # Here&amp;#39;s the output for the map
                ),
              fluidRow(  # Row 2 of main panel
                plotOutput(&amp;quot;timeplot&amp;quot;, height = 300)  # Here&amp;#39;s the output for the time plots
                )
              )
    )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There’s the code for the UI. Starting from the top, we use the &lt;em&gt;FluidPage&lt;/em&gt; function and here I’m using the theme &lt;em&gt;flatly&lt;/em&gt;. Then I say that I want to use a &lt;em&gt;sidebarLayout&lt;/em&gt;. From here, I split the code into a &lt;em&gt;sidebarPanel&lt;/em&gt; and a &lt;em&gt;mainPanel&lt;/em&gt;. I further split things into &lt;em&gt;fluidRows&lt;/em&gt; which just helps to organize the layout. All of the #s are notes, of course, and will not actually be run.&lt;/p&gt;
&lt;p&gt;The big thing to notice is that there are inputs and outputs. The only input is a &lt;em&gt;switchInput&lt;/em&gt; which lets the user choose whether to display totals or averages. Everything else is an output. Each of these gets a name, for example, I’m calling the leafletOutput &lt;em&gt;map&lt;/em&gt;. These names are important, as they will correspond with what we provide in the server part.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specifying-the-server&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specifying the Server&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;server &amp;lt;- function(input, output) {
  
  output$map &amp;lt;- renderLeaflet({  # Map output
      leaflet(data = coords) %&amp;gt;%
         addTiles() %&amp;gt;%
         addMarkers(~long, ~lat, label = ~name) %&amp;gt;%
         setMaxBounds(-75.65, 45.38, -75.75, 45.46) %&amp;gt;%
         addProviderTiles(providers$CartoDB.Positron)
    })
  
  output$caption2 &amp;lt;- renderUI({  # Lower caption output
    str1 &amp;lt;- paste(&amp;quot;Created by &amp;quot;, a(&amp;quot;Will Hipson.&amp;quot;, href = &amp;quot;https://willhipson.netlify.com/&amp;quot;))
    str2 &amp;lt;- paste(&amp;quot;Data courtesy of &amp;quot;, a(&amp;quot;Open Data Ottawa.&amp;quot;, href = &amp;quot;https://open.ottawa.ca/datasets/bicycle-trip-counters&amp;quot;))
    str3 &amp;lt;- &amp;quot;2010-01-01 - 2019-09-30&amp;quot;
    str4 &amp;lt;- &amp;quot;Updated on 2019-10-24&amp;quot;
    HTML(paste(str1, str2, str3, str4, sep = &amp;#39;&amp;lt;br/&amp;gt;&amp;#39;))
  })
  
  observeEvent(input$map_marker_click, { # If the user clicks a marker, this line is run.
    output$timeplot &amp;lt;- renderPlot({
      if(input$average == TRUE) { # if average is selected we get average overlayed
        ggplot() +
          geom_line(data = bikes_plot[bikes_plot$lat == input$map_marker_click$lat, ], 
                    aes(x = date, y = count), size = .5, alpha = .70, color = &amp;quot;#36648B&amp;quot;) +
          geom_line(data = bikes_mean, aes(x = date, y = count), alpha = .50, color = &amp;quot;#9F79EE&amp;quot;) +
          scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
          scale_y_continuous(limits = c(0, 6000)) +
          labs(x = NULL,
               y = &amp;quot;Count&amp;quot;,
               title = paste(bikes_plot[bikes_plot$lat == input$map_marker_click$lat,]$name)) +
          theme_minimal(base_size = 16) +
          theme(plot.title = element_text(hjust = .5),
                axis.text.x = element_text(size = 16),
                text = element_text(family = &amp;quot;Verdana&amp;quot;))
      } else { # if average is not selected, then it&amp;#39;s just the total
        ggplot() +
          geom_line(data = bikes_plot[bikes_plot$lat == input$map_marker_click$lat, ], 
                    aes(x = date, y = count), size = .5, alpha = .70, color = &amp;quot;#36648B&amp;quot;) +
          scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
          scale_y_continuous(limits = c(0, 6000)) +
          labs(x = NULL,
               y = &amp;quot;Count&amp;quot;,
               title = paste(bikes_plot[bikes_plot$lat == input$map_marker_click$lat,]$name)) +
          theme_minimal(base_size = 16) +
          theme(plot.title = element_text(hjust = .5),
                axis.text.x = element_text(size = 16),
                text = element_text(family = &amp;quot;Verdana&amp;quot;))
      }
    })
    
    output$caption &amp;lt;- renderUI({ # counter specific description
      str1 &amp;lt;- coords[coords$lat == input$map_marker_click$lat, ]$desc
      HTML(str1)
    })
    
    output$stats &amp;lt;- renderUI({ # counter specific statistics
      str1 &amp;lt;- &amp;quot;&amp;lt;b&amp;gt;Statistics&amp;lt;/b&amp;gt;&amp;quot;
      str2 &amp;lt;- paste(&amp;quot;Total count: &amp;quot;, format(round(sum(bikes_plot[bikes_plot$lat == input$map_marker_click$lat,]$count, na.rm = TRUE)), big.mark = &amp;quot;,&amp;quot;))
      str3 &amp;lt;- paste(&amp;quot;Average count: &amp;quot;, format(round(mean(bikes_plot[bikes_plot$lat == input$map_marker_click$lat,]$count, na.rm = TRUE), 1), big.mark = &amp;quot;,&amp;quot;))
      str4 &amp;lt;- paste(&amp;quot;Busiest day: &amp;quot;, bikes_plot[which.max(bikes_plot[bikes_plot$lat == input$map_marker_click$lat,]$count),]$date)
      HTML(paste(str1, str2, str3, str4, sep = &amp;#39;&amp;lt;br/&amp;gt;&amp;#39;))
      })
  })
    
  observeEvent(input$map_click, ignoreNULL = FALSE, {  # If the user clicks on the map it goes back to the cumulative data
    output$timeplot &amp;lt;- renderPlot({
      if(input$average == TRUE) {  # if the average is selected, it displays average
      ggplot(data = bikes_mean, aes(x = date, y = count)) +
          geom_line(size = .5, alpha = .70, color = &amp;quot;#36648B&amp;quot;) +
          scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
          labs(x = NULL,
               y = &amp;quot;Count&amp;quot;) +
          theme_minimal(base_size = 16) +
          theme(plot.title = element_text(hjust = .5),
                axis.text.x = element_text(size = 16),
                text = element_text(family = &amp;quot;Verdana&amp;quot;))
      } else { # if average is not selected it is the total
        ggplot(data = bikes_total, aes(x = date, y = count)) +
          geom_line(size = .5, alpha = .70, color = &amp;quot;#36648B&amp;quot;) +
          scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
          labs(x = NULL,
               y = &amp;quot;Count&amp;quot;) +
          theme_minimal(base_size = 16) +
          theme(plot.title = element_text(hjust = .5),
                axis.text.x = element_text(size = 16),
                text = element_text(family = &amp;quot;Verdana&amp;quot;))
      }
    })
    
    output$caption &amp;lt;- renderUI({  # the default caption
      str1 &amp;lt;- &amp;quot;Presenting data from bicycle counters across Ottawa. There are 14 counters spread across the city. The graph below displays how daily counts change over time. Click on a map marker to select a specific counter.&amp;quot;
      HTML(str1)
    })
    
    output$stats &amp;lt;- renderUI({  # Statistics output
      str1 &amp;lt;- &amp;quot;&amp;lt;b&amp;gt;Statistics&amp;lt;/b&amp;gt;&amp;quot;
      str2 &amp;lt;- paste(&amp;quot;Total count: &amp;quot;, format(round(sum(bikes_total$count, na.rm = TRUE)), big.mark = &amp;quot;,&amp;quot;))
      str3 &amp;lt;- paste(&amp;quot;Average count: &amp;quot;, format(round(mean(bikes_total$count, na.rm = TRUE), 1), big.mark = &amp;quot;,&amp;quot;))
      str4 &amp;lt;- paste(&amp;quot;Busiest day: &amp;quot;, bikes_total[which.max(bikes_total$count),]$date)
      HTML(paste(str1, str2, str3, str4, sep = &amp;#39;&amp;lt;br/&amp;gt;&amp;#39;))
    })
  })
  
  output$download &amp;lt;- downloadHandler( # download button. Will turn &amp;#39;bikes&amp;#39; object into a csv file.
    filename = function() {
      paste(&amp;quot;ottawa_bikes&amp;quot;, &amp;quot;.csv&amp;quot;, sep = &amp;quot;&amp;quot;)
    },
    
    content = function(file) {
      write.csv(bikes, file)
    }
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code for the server is much busier and it can be overwhelming. Essentially we’re just saying what we want to do with the inputs and outputs. We generate a little code chunk for each output. Look at the first one for &lt;em&gt;map&lt;/em&gt;. This is where we generate the map. We say we want to &lt;em&gt;renderLeaflet&lt;/em&gt; and then we just copy the code that we made earlier into this block.&lt;/p&gt;
&lt;p&gt;Where things get a bit more complicated is when we want our output to change based on user input. If the user selects the switch that converts the data to averages, for example. I used if and else statements to modulate the output based on whether ‘average’ is selected. What happens, is when the user clicks on the switch, the value of input$average changes to TRUE. Using if and else functions, I just say what I want to happen when ‘average’ is TRUE and what happens if it’s FALSE.&lt;/p&gt;
&lt;p&gt;Finally, we want the user to be able to click on specific markers and have the output change to that specific marker. We use the &lt;em&gt;observeEvent&lt;/em&gt; function and specify the input, ‘map_marker_click’. We also want the user to be able to click off the marker to go back to the default output. Again, we use &lt;em&gt;observeEvent&lt;/em&gt; but now with ‘click_map’.&lt;/p&gt;
&lt;p&gt;Once we have all the other outputs in place for the downloads and the captions, we put it all together using the &lt;em&gt;shinyApp&lt;/em&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shinyApp(ui, server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And there it is, a user-friendly app for exploring bicycling data in Ottawa. Future avenues include building in some time-series forecasting. It would be cool to show the user how the trend is expected to change over time.&lt;/p&gt;
&lt;p&gt;One last shout out to &lt;a href=&#34;https://open.ottawa.ca/datasets/bicycle-trip-counters&#34;&gt;Open Data Ottawa&lt;/a&gt; for sharing this data!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Linear Mixed Models: Random Intercepts, Slopes, and Missing Data</title>
      <link>/post/bayesian_mlm/bayesian_mlm/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian_mlm/bayesian_mlm/</guid>
      <description>


&lt;p&gt;This past summer, I watched a brilliant &lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&amp;amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&#34;&gt;lecture series by Richard McElreath&lt;/a&gt; on Bayesian statistics. It honestly changed my whole outlook on statistics, so I couldn’t recommend it more (plus, McElreath is an engaging instructor). One of the most compelling cases for using Bayesian statistics is with a collection of statistical tools called &lt;em&gt;linear mixed models&lt;/em&gt; or &lt;em&gt;multilevel/hierarchical models&lt;/em&gt;. It’s common that data are grouped or clustered in some way. Often in psychology we have repeated observations nested within participants, so we know that data coming from the same participant will share some variance. Linear mixed models are powerful tools for dealing with multilevel data, usually in the form of modeling &lt;em&gt;random intercepts&lt;/em&gt; and &lt;em&gt;random slopes&lt;/em&gt;. In this tutorial I assume familiarity with linear regression and some background knowledge in Bayesian inference, such that you should have some familiarity with &lt;em&gt;priors&lt;/em&gt; and &lt;em&gt;posterior distributions&lt;/em&gt; (if not, go &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0732118X18300746?via%3Dihub&#34;&gt;here&lt;/a&gt;) or watch McElreath’s videos.&lt;/p&gt;
&lt;p&gt;Why use Bayesian instead of Frequentist statistics? One reason is that &lt;em&gt;prior&lt;/em&gt; knowledge about a parameter - for example, it’s distribution - can be incorporated in the model. Another reason especially relevant to linear mixed models is that we can easily include multiple random intercepts and slopes without running into the same stringent sample size requirements as with frequentist approaches. This is not an exhaustive list; more can be found &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0732118X18300746?via%3Dihub&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;setting-it-all-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting it All Up&lt;/h2&gt;
&lt;p&gt;Installing and running &lt;em&gt;brms&lt;/em&gt; is a bit more complicated than your run-of-the-mill R packages. Because &lt;em&gt;brms&lt;/em&gt; uses STAN as its back-end engine to perform Bayesian analysis, you will need to install &lt;em&gt;rstan&lt;/em&gt;. Carefully follow the instructions at this &lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34;&gt;link&lt;/a&gt; and you should have no problem. Once you’ve done that you should be able to install &lt;em&gt;brms&lt;/em&gt; and load it up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Rcpp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading &amp;#39;brms&amp;#39; package (version 2.10.0). Useful instructions
## can be found by typing help(&amp;#39;brms&amp;#39;). A more detailed introduction
## to the package is available through vignette(&amp;#39;brms_overview&amp;#39;).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s normal to get some warning messages upon loading the package. That’s fine. We can now load up the &lt;em&gt;tidyverse&lt;/em&gt; for data manipulation and visualization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is available on GitHub &lt;a href=&#34;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%205/Curran/CurranLong.sav&#34;&gt;here&lt;/a&gt; in a .sav format. You’ll need the &lt;em&gt;haven&lt;/em&gt; package to read it into R. Make sure you import it from your working directory or specify the path to your downloads folder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(haven)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset was curated by &lt;a href=&#34;http://curran.web.unc.edu/&#34;&gt;Patrick Curran&lt;/a&gt; for the purpose of demonstrating different approaches for handling multilevel, longitudinal data (see &lt;a href=&#34;https://jflournoy.github.io/assets/pdf/srcdmeth.pdf&#34;&gt;here&lt;/a&gt; for more info). The dataset consists of 405 children in the first two years of elementary school measured over four assessment points on reading recognition and antisocial behaviour. As well, it included time invariant covariates, such as emotional support and cognitive stimulation. For this post, we’ll focus on reading and cognitive stimulation, and we’ll use Bayesian Linear Mixed Models to address a number of questions about children’s reading ability.&lt;/p&gt;
&lt;p&gt;For now, we’ll omit assessment periods with missing data, but we’ll return to the issue of missing data at the end of this demonstration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat &amp;lt;- read_sav(&amp;quot;CurranLong.sav&amp;quot;) %&amp;gt;%
  select(id, occasion, read, homecog) %&amp;gt;%
  filter(complete.cases(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,325 x 4
##       id occasion  read homecog
##    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1    22        0   2.1      13
##  2    22        1   3.9      13
##  3    34        0   2.1       9
##  4    34        1   2.9       9
##  5    34        2   4.5       9
##  6    34        3   4.5       9
##  7    58        0   2.3       9
##  8    58        1   4.5       9
##  9    58        2   4.2       9
## 10    58        3   4.6       9
## # ... with 1,315 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is conveniently in long form already, which means that each row represents a single &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; assessment period for an &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; child. With no other data cleaning to do, we can move ahead with our research questions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-1-one-random-effect-no-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 1: One Random Effect, No Covariates&lt;/h2&gt;
&lt;p&gt;We’ll start with a population-level intercept and a random intercept for participants. Our formula looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the population intercept, &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; is a random intercept term for an &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; participant, and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ij}\)&lt;/span&gt; is a random noise term for an &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; participant and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; assessment period.&lt;/p&gt;
&lt;p&gt;If you find formulae distasteful, that’s OK. They’re not essential to understanding how linear mixed models work. conceptually, think of the &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; in the above formula as giving a little &lt;em&gt;nudge&lt;/em&gt; to the intercept for each participant.&lt;/p&gt;
&lt;p&gt;Here I’ll walk carefully through the &lt;em&gt;brms&lt;/em&gt; code and then each subsequent model will just build one this one. We start by providing our data. The &lt;em&gt;family&lt;/em&gt; argument is where we supply a distribution for our outcome variable, reading. We’ll use the default &lt;em&gt;gaussian&lt;/em&gt; to indicate that reading should be normally distributed. The next line is where we specify our model formula. If you’re familiar with mixed effects models in the &lt;em&gt;lme4&lt;/em&gt; package you’ll be happy to see there are no differences here. I’ve only included the population level intercept ‘1’ after ‘read ~’ to be precise, but this will be included by default. The random intercept is indicated by the ‘(1 | id)’ as in typical mixed effects notation. We then specify some priors. Our first prior is for the population-level &lt;em&gt;Intercept&lt;/em&gt;. We’ll specify a fairly wide normal distribution for the intercept.&lt;/p&gt;
&lt;p&gt;If this whole business of deciding on priors seems strange and arbitrary, just note that a wide prior such as this one will have little bearing on the posterior distribution. If instead we were extremely confident that the intercept was zero, we could use a narrow prior of ‘normal(0, .05)’ which would have a strong effect on the resulting posterior distribution.&lt;/p&gt;
&lt;p&gt;The next prior is for the &lt;em&gt;standard deviation&lt;/em&gt; of the random effects. Unlike an intercept which can be positive or negative, variance (and by association, standard deviation) can only be positive, so we specify a &lt;em&gt;cauchy&lt;/em&gt; distribution that constrains the sd to be positive. We do the same for &lt;em&gt;sigma&lt;/em&gt; which is the overall variability.&lt;/p&gt;
&lt;p&gt;The rest of the code has to do with how the Markov Chain Monte Carlo (MCMC) algorithm runs. Without going too much into the &lt;em&gt;whys&lt;/em&gt; (a more detailed treatise can be found &lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/sampling-the-imaginary.html#sampling-to-simulate-prediction&#34;&gt;here&lt;/a&gt;), we will run 2,000 chained simulations. The first 1,000 of these will be in the warmup period and we will be rejected. We will run 4 parallel chains on 4 cores (if your computer has fewer cores you will want to reduce this). To help the model converge, I’ve increased the adapt_delta and max_treedepth arguments. Finally, an arbitrary seed number for reproducibility.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read1 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             formula = read ~ 1 + (1 | id),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + (1 | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.54      0.08     0.37     0.68 1.00      992     1847
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     4.11      0.05     4.01     4.21 1.00     4866     3424
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.55      0.04     1.48     1.62 1.00     2219     2827
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does the output tell us? Right away, we see an estimate of an Intercept of 4.11 within a 95% &lt;em&gt;credible interval&lt;/em&gt; (you might call it the Bayesian version of a confidence interval, although they’re not quite the same thing) of 4.01 and 4.21. What’s more interesting is the estimate of standard deviation of the random intercepts ‘sd(Intercept)’. So there is person-level variability in reading scores. In turn, &lt;em&gt;sigma&lt;/em&gt; is the estimate of the overall variability in reading scores.&lt;/p&gt;
&lt;p&gt;We can further inspect these estimates by looking at the traceplots and the posterior distributions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(read1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each row corresponds with a parameter in the model. The traceplot shows the MCMC samples. These should resemble ‘hairy caterpillars’ - we don’t want to see the chain getting stuck near a particular value.&lt;/p&gt;
&lt;p&gt;For a more intuitive understanding of what the mixed effects model is doing, let’s compare a sample of the observed values with the model predicted values. &lt;em&gt;brms&lt;/em&gt; outputs fitted values using the &lt;em&gt;fitted&lt;/em&gt; function. We’ll sample 6 participants and see their observed reading scores and model-derived reading scores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read1))) %&amp;gt;%
  group_by(id) %&amp;gt;%
  nest() %&amp;gt;%
  sample_n(6) %&amp;gt;%
  unnest() %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 1: One Random Effect, No Covariates&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that each estimate is drawn toward the population-level mean and its cluster-level mean. This phenomenon is called &lt;em&gt;partial-pooling&lt;/em&gt; or &lt;em&gt;shrinkage&lt;/em&gt;. If we’d run the model without the random effect, all of the estimates (black circles) would be on a single horizontal line. But here we have some variation between individuals. Notice too how points further away from the means (population- and cluster-level) are pulled more strongly. However, there is clearly an increase in reading ability over time that is being ignored by this model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-2-two-random-effects-no-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 2: Two Random Effects, No Covariates&lt;/h2&gt;
&lt;p&gt;Let’s add a random intercept for assessment period (labelled ‘occasion’ in this dataset). This model will recognize that observations are nested within participants &lt;em&gt;and&lt;/em&gt; assessment periods. For now, we’ll consider assessment as a random effect because we are still only interested in the variability in reading scores. Here’s our new formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+w_{0j}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we have a term &lt;span class=&#34;math inline&#34;&gt;\(w_{0j}\)&lt;/span&gt; for the random intercept for the &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; assessment period.&lt;/p&gt;
&lt;p&gt;The only change to our model code is that we’ve added a second random intercept: ‘(1 | occasion)’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read2 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             read ~ 1 + (1 | id) + (1 | occasion),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + (1 | id) + (1 | occasion) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.89      0.04     0.82     0.96 1.00     1270     2122
## 
## ~occasion (Number of levels: 4) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.71      0.86     0.79     3.99 1.00     1701     1878
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     4.32      0.89     2.46     6.08 1.00     1426     1848
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.64      0.01     0.61     0.67 1.00     3630     3056
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking again at our output now, we see variability in the random intercepts for id, but even larger variability across occasions (assessment periods). In fact, the overall variability, sigma, is quite reduced from the previous model. This demonstrates an important lesson about mixed models: that they decompose the overall variance (sigma) into cluster-level variance. If we just ran a linear regression, it wouldn’t differentiate between measurement error and variation between participants and time points.&lt;/p&gt;
&lt;p&gt;Let’s again compare the estimates with the observed values. We’ll use the same subsample as before to illustrate the effect of adding a second random effect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read2))) %&amp;gt;%
  group_by(id) %&amp;gt;%
  nest() %&amp;gt;%
  sample_n(6) %&amp;gt;%
  unnest() %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 2: Two Random Effects, No Covariates&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the model now adjusts the points based on the assessment-level intercept. The fact that the black circles follow a nearly linear increase is telling us that there is probably an effect of time, such that children get better at reading over time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-3-one-random-effect-one-covariate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 3: One Random Effect, One Covariate&lt;/h2&gt;
&lt;p&gt;Now let’s make a conceptual shift. Maybe as researchers we decide that we’re interested in the effect of time (assessment period) on reading ability. Since each assessment was spaced equally apart in time and we have good reason to think that time has a global effect on reading, it makes sense to include it as a fixed effect (this sort of conceptual decision should be made beforehand, but for illustrative purposes I’ve shown it as a random and a fixed effect).&lt;/p&gt;
&lt;p&gt;It’s always a good idea to visualize what’s going on so that we know if our assumptions are tenable. we’ll create a lineplot showing the change in reading ability over time, but with each line representing an individual in our sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  ggplot(aes(x = occasion, y = read, group = id)) +
  geom_line(size = .75, alpha = .20) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;) +
  theme_minimal(base_size = 16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s a clear upward trend - most children improve in their reading over time. We can now proceed with formally testing the growth in reading:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+\beta_1Time_j+u_{0i}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s the model code. Note that we are still modeling the random effect of id, but now occasion is a covariate (predictor). We need to specify a new prior, &lt;em&gt;b&lt;/em&gt; for beta. The beta estimate for the effect of time on reading ability is assumed to derive from a normal distribution with mean 0 and sd 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read3 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             read ~ 1 + occasion + (1 | id),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(normal(0, 1), class = b),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + occasion + (1 | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.89      0.04     0.82     0.96 1.00     1204     1906
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.70      0.05     2.60     2.81 1.00     1036     1692
## occasion      1.10      0.02     1.07     1.14 1.00     6696     3112
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.68      0.02     0.65     0.71 1.00     3004     3138
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our output gives a smaller intercept because it’s not telling us the mean any more, instead it’s the y-intercept of the regression line. We have an estimate of beta as 1.10 within a 95% credible interval of 1.07 and 1.14. So there’s strong evidence of a positive effect of time on reading ability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read3))) %&amp;gt;%
  group_by(id) %&amp;gt;%
  nest() %&amp;gt;%
  sample_n(6) %&amp;gt;%
  unnest() %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 3: One Random Effect, One Covariate&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The estimates aren’t actually that different from the model with two random effects. The main difference now is that the black circles show a strictly linear trend that is the same across participants. But maybe this assumption of equal effects across the board isn’t tenable either.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-4-one-random-slope-one-covariate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 4: One Random Slope, One Covariate&lt;/h2&gt;
&lt;p&gt;Let’s ramp things up by modeling a random slope for each participant:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+\beta_1Time_j+u_{1ij}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(u_{1ij}\)&lt;/span&gt; is the random slope for the effect of assessment period between participants.&lt;/p&gt;
&lt;p&gt;A random slope model also has a random intercept, but now, the slope for time on reading ability will be different for each participant: ‘(1 + occasion | id)’. Another change to our model code is a new prior specifying the correlation between the Intercept and the beta for occasion. For an overview of the Cholesky Distribution I’d recommend this &lt;a href=&#34;https://www.psychstatistics.com/2014/12/27/d-lkj-priors/&#34;&gt;post&lt;/a&gt;, but all you need to know here is that the higher above 1 the value is, the more “speculative” the model is of strong correlations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read4 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             read ~ 1 + occasion + (1 + occasion | id),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(normal(0, 1), class = b),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma),
                       prior(lkj_corr_cholesky(1.5), class = cor)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + occasion + (1 + occasion | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)               0.76      0.04     0.68     0.84 1.00     1279
## sd(occasion)                0.27      0.02     0.22     0.32 1.00      614
## cor(Intercept,occasion)     0.29      0.12     0.07     0.53 1.00      664
##                         Tail_ESS
## sd(Intercept)               2218
## sd(occasion)                1082
## cor(Intercept,occasion)     1070
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.70      0.05     2.60     2.79 1.00     1465     2297
## occasion      1.12      0.02     1.08     1.16 1.00     2963     3299
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.59      0.02     0.56     0.62 1.00      908     1758
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read4))) %&amp;gt;%
  group_by(id) %&amp;gt;%
  nest() %&amp;gt;%
  sample_n(6) %&amp;gt;%
  unnest() %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 4: One Random Slope, One Covariate&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I won’t belabour the output here. Just notice how the strength of the linear trend for the fitted values is different for each participant. We can see this across the whole sample with the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read4))) %&amp;gt;%
  ggplot() +
  geom_line(aes(x = occasion, y = Estimate, group = id), size = .75, alpha = .30) +
  geom_line(aes(x = occasion, y = read, group = id), size = .75, alpha = .15, color = &amp;quot;dodgerblue2&amp;quot;) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 4: One Random Slope, One Covariate&amp;quot;,
       subtitle = &amp;quot;Blue lines are observed values. Black lines are fitted.&amp;quot;) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-5-one-random-slope-two-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 5: One Random Slope, Two Covariates&lt;/h2&gt;
&lt;p&gt;Suppose we wanted to see whether there’s an effect of the amount of &lt;em&gt;cognitive stimulation&lt;/em&gt; that children are exposed to at home on reading ability over time. Maybe cognitive stimulation impacts some children more strongly than others. Our last formula would thus be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+\beta_1Time_j+u_{1ij}+\beta_{2}Cog_i+u_{2ij}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Does the hypothesis hold up to a visual inspection? For illustrative purposes, we can split the data into equal bins of low, medium, and high cognitive stimulation. Of course, because stimulation is a continuous variable, it will stay continuous in our Bayesian model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  mutate(homecog_grp = factor(ntile(homecog, 3))) %&amp;gt;%
  ggplot(aes(x = occasion, y = read)) +
  geom_line(aes(group = id, color = homecog_grp), size = .75, alpha = .50) +
  geom_smooth(method = &amp;#39;lm&amp;#39;, color = &amp;quot;black&amp;quot;, size = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Cognitive Home Environment&amp;quot;) +
  facet_wrap(~homecog_grp, labeller = as_labeller(c(&amp;quot;1&amp;quot; = &amp;quot;Low&amp;quot;,
                                                    &amp;quot;2&amp;quot; = &amp;quot;Medium&amp;quot;,
                                                    &amp;quot;3&amp;quot; = &amp;quot;High&amp;quot;))) +
  guides(color = FALSE) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5, size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If there is an effect of cognitive stimulation, it’s a small one. So let’s find out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read5 &amp;lt;- brm(data = curran_dat,
            family = gaussian,
            read ~ 1 + occasion + homecog + (1 + occasion + homecog | id),
            prior = c(prior(normal(0, 5), class = Intercept),
                      prior(normal(0, 1), class = b),
                      prior(cauchy(0, 1), class = sd),
                      prior(cauchy(0, 1), class = sigma),
                      prior(lkj_corr_cholesky(1.5), class = cor)),
            iter = 2000, warmup = 1000, chains = 4, cores = 4,
            control = list(adapt_delta = .975, max_treedepth = 20),
            seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The model has not converged (some Rhats are &amp;gt; 1.1). Do not analyse the results! 
## We recommend running more iterations and/or setting stronger priors.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: There were 150 divergent transitions after warmup. Increasing adapt_delta above 0.975 may help.
## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + occasion + homecog + (1 + occasion + homecog | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)               0.72      0.11     0.50     0.99 1.02      456
## sd(occasion)                0.27      0.03     0.22     0.32 1.00     1010
## sd(homecog)                 0.04      0.02     0.00     0.09 1.10       39
## cor(Intercept,occasion)     0.28      0.20    -0.14     0.70 1.02      178
## cor(Intercept,homecog)     -0.07      0.39    -0.73     0.73 1.05       70
## cor(occasion,homecog)      -0.02      0.38    -0.72     0.73 1.01      247
##                         Tail_ESS
## sd(Intercept)                165
## sd(occasion)                1608
## sd(homecog)                   54
## cor(Intercept,occasion)      319
## cor(Intercept,homecog)       369
## cor(occasion,homecog)        418
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.15      0.16     1.84     2.46 1.00     2211     2188
## occasion      1.12      0.02     1.07     1.16 1.00     3363     3237
## homecog       0.06      0.02     0.03     0.10 1.00     2290     2399
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.59      0.02     0.56     0.63 1.00     1120     2369
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running this model, we get a few warnings about large Rhats and divergent transitions. We would be better off running a few more iterations (maybe 4000) if we wanted more confidence in the results. Nevertheless, we see a pretty weak effect of cognitive stimulation. The credible interval for the population effect of cognitive stimulation doesn’t include zero, but the evidence is weak that it’s doing anything at all in this model. There’s some indication that the intercepts vary by cognitive stimulation, such that at higher stimulation children have a higher initial reading ability. But there’s no evidence that the slopes vary by cognitive stimulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read5))) %&amp;gt;%
  mutate(homecog_grp = factor(ntile(homecog, 3))) %&amp;gt;%
  ggplot(aes(x = occasion, y = Estimate, group = id, color = homecog_grp)) +
  geom_line(size = .75, alpha = .50) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Cognitive Home Environment&amp;quot;) +
  facet_wrap(~homecog_grp, labeller = as_labeller(c(&amp;quot;1&amp;quot; = &amp;quot;Low&amp;quot;,
                                                    &amp;quot;2&amp;quot; = &amp;quot;Medium&amp;quot;,
                                                    &amp;quot;3&amp;quot; = &amp;quot;High&amp;quot;))) +
  guides(color = FALSE) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5, size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-6-dealing-with-missing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 6: Dealing with Missing Data&lt;/h2&gt;
&lt;p&gt;So far we’ve been dealing with complete data - where each assessment has no missing values. In doing so, we are effectively claiming that the data are missing completely at random (MCAR). This may not be a tenable assumption. Missingness on one variable may be correlated with another variable (Missing at Random; MAR) or, even worse, correlated with the variable itself (Missing Not At Random; MNAR). Without diving into the theoretical aspects of missing data (a more thoughtful discussion can be found &lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-MCAR.html&#34;&gt;here&lt;/a&gt;) let’s end by running Bayesian imputation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat_missing &amp;lt;- read_sav(&amp;quot;CurranLong.sav&amp;quot;) %&amp;gt;%
  select(id, occasion, read, homecog)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;em&gt;brms&lt;/em&gt; we indicate missingness with &lt;em&gt;mi()&lt;/em&gt;. Here we’re rerunning Model 5, but we’re also imputing missingness on reading ability and cognitive stimulation. We actually have two model formulae - one for each variable in the model with missing data. I’ll also note quickly that we have hidden missingness in this data as missing assessment points do not show up as rows in the dataset. For the purpose of this demonstration, it won’t be a problem, but in the real world, we’d want to impute these as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read6 &amp;lt;- brm(data = curran_dat_missing,
             family = gaussian,
             bf(read | mi() ~ 1 + occasion + mi(homecog) + (1 + occasion | id)) +
               bf(homecog | mi() ~ 1) + 
               set_rescor(FALSE),
             prior = c(prior(normal(0, 5), class = Intercept),
                       prior(normal(0, 1), class = b),
                       prior(lkj_corr_cholesky(1.5), class = cor)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: read | mi() ~ 1 + occasion + mi(homecog) + (1 + occasion | id) 
##          homecog | mi() ~ 1 
##    Data: curran_dat_missing (Number of observations: 1393) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##                                   Estimate Est.Error l-95% CI u-95% CI
## sd(read_Intercept)                    0.75      0.04     0.68     0.83
## sd(read_occasion)                     0.27      0.02     0.22     0.32
## cor(read_Intercept,read_occasion)     0.25      0.12     0.03     0.51
##                                   Rhat Bulk_ESS Tail_ESS
## sd(read_Intercept)                1.00     1360     2987
## sd(read_occasion)                 1.01      770     1589
## cor(read_Intercept,read_occasion) 1.00      960     1492
## 
## Population-Level Effects: 
##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## read_Intercept        2.15      0.16     1.84     2.47 1.00     2028
## homecog_Intercept     8.91      0.07     8.77     9.05 1.00     9040
## read_occasion         1.12      0.02     1.08     1.16 1.00     4599
## read_mihomecog        0.06      0.02     0.03     0.09 1.00     2066
##                   Tail_ESS
## read_Intercept        2534
## homecog_Intercept     2687
## read_occasion         2956
## read_mihomecog        2596
## 
## Family Specific Parameters: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_read        0.59      0.02     0.56     0.62 1.00      943     2256
## sigma_homecog     2.57      0.05     2.47     2.66 1.00    12854     2668
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Output is all well and good, but to really see what’s going on we’ll return to our sampling technique. Here are the fitted values on 3 individuals from Model 5 (non-imputed) and the same 3 from Model 6 (imputed).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read5))) %&amp;gt;%
  filter(id %in% c(1048, 1050, 1498)) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 5, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 5, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Unimputed Data&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.1, .1), breaks = 0:3) +
  scale_y_continuous(limits = c(2, 9)) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 12) +
  theme(plot.title = element_text(hjust = .5))

p2 &amp;lt;- curran_dat_missing %&amp;gt;%
  bind_cols(as_tibble(fitted(read6))) %&amp;gt;%
  filter(id %in% c(1048, 1050, 1498)) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 5, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate.read), shape = 1, size = 5, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Bayesian Imputation&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.1, .1), breaks = 0:3) +
  scale_y_continuous(limits = c(2, 9)) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 12) +
  theme(plot.title = element_text(hjust = .5))

library(ggpubr)

ggarrange(p1, p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we get reasonable estimates for the missing values. Our work here is done.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-words&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Words&lt;/h1&gt;
&lt;p&gt;This topic is so vast that I couldn’t possibly cover everything there is to say about Bayesian Linear Mixed Models. We only scratched the surface on interpreting Bayesian posteriors. For example, &lt;em&gt;brms&lt;/em&gt; offers formal ways of comparing models and I highly recommend looking into these.
I’m indebted to Richard McElreath’s fantastic lectures series for all that I know on the topic. Honestly, if you can spare the time, they’re so so worth it - plus they’re freely available on YouTube &lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&amp;amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&#34;&gt;here&lt;/a&gt;. Watch these and then follow along with the code in this &lt;em&gt;brms&lt;/em&gt; guide &lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/&#34;&gt;here&lt;/a&gt; and you’re golden! Finally, if you want a short primer on Bayesian MLM and an excellent guide for writing STAN code (not &lt;em&gt;brms&lt;/em&gt;), I’d highly recommend this &lt;a href=&#34;https://www.tqmp.org/RegularArticles/vol12-3/p175/p175.pdf&#34;&gt;paper&lt;/a&gt; by Sorensen, Hohenstein, and Vasishth (2016). It helped immensely in my ongoing transition from Frequentist to Bayesian statistics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.psychstatistics.com/2014/12/27/d-lkj-priors/&#34;&gt;Baldwin, S. (2014). &lt;em&gt;Visualizing the LKJ Correlation Distribution.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jflournoy.github.io/assets/pdf/srcdmeth.pdf&#34;&gt;Curran, P. J. (1997). Comparing Three Modern Approaches to Longitudinal Data Analysis: An Examination of a Single Developmental Sample. &lt;em&gt;Symposium Conducted at the 1997 Biennial Meeting of the Society for Research in Child Development, Washington, D.C.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/&#34;&gt;Kurz, A. S. (2019). &lt;em&gt;Statistical Rethinking with brms, ggplot2, and the tidyverse.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.tqmp.org/RegularArticles/vol12-3/p175/&#34;&gt;Sorensen, T., Hohenstein, S., &amp;amp; Vasishth, S. (2016). Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists. &lt;em&gt;The Quantitative Methods for Psychology&lt;/em&gt;, 175-200. doi: 10.20982/tqmp.12.3.p175&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0732118X18300746?via%3Dihub&#34;&gt;van Zyl, C. J. J. (2018). Frequentist and Bayesian inference: A conceptual primer. &lt;em&gt;New Ideas in Psychology, 51&lt;/em&gt;, 44-49.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Modeling Motivation and Emotion using Feedback Loops</title>
      <link>/post/feedback_loops/feedback_loops/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/feedback_loops/feedback_loops/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/viz/viz.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/DiagrammeR-styles/styles.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/grViz-binding/grViz.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;If you’re anything like me, you probably set a lot of goals. Whether it’s to finish a paper by the end of the summer or to spend more time with friends and family, goals are what help motivate us to &lt;em&gt;do something&lt;/em&gt;. Goals are also intimately tied to our feelings. You may have had the experience of falling behind in your goals, which made you upset, but ultimately motivated you to step up your efforts. Conversely, you could be comfortably ahead of your goals and be tempted to coast for a while (much like the Hare in the Tortoise and the Hare who takes a nap when he is far ahead of the Tortoise).&lt;/p&gt;
&lt;p&gt;Charles Carver presented an elegant model of emotion and behavior as intertwined in nested &lt;em&gt;feedback loops&lt;/em&gt;. The idea behind a feedback loop is that you have a system whose output feeds into its input (the typical example being a thermostat). This idea isn’t new (see Powers, 1973), but Carver brilliantly formalized these ideas in this &lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/1754073915590616?journalCode=emra&#34;&gt;2015 paper&lt;/a&gt;, using process diagrams to illustrate his model. In honor of Dr. Carver who passed away this year, I’m bringing his model to life in the form of a simulation.&lt;/p&gt;
&lt;div id=&#34;simple-feedback-loop&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple Feedback Loop&lt;/h2&gt;
&lt;p&gt;Let’s start with a simple type of feedback loop called a &lt;em&gt;discrepancy-reducing loop&lt;/em&gt;. As we’ll see, the term implies that the object of the loop is to reduce a discrepancy between the system’s current state and an optimal, desired state. Imagine we have a goal of running a marathon and we’re tracking our progress toward that goal. Our desired outcome is to be 100% prepared for the marathon (note that we’re considering the act of running the marathon separate). Every day, we’ll compare our current progress to our desired outcome and this comparison will dictate how much effort we put into training that day. It’s easier to see with a process diagram:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\n\ndigraph {\ngraph [overlap = true, fontsize = 20]\nnode [shape = box,\n      fontname = Helvetica]\n\&#34;Desired Progress\&#34;; Behavior; \&#34;Current Progress\&#34;; \&#34;Environment\&#34;\n\nnode [shape = triangle,\n      fixedsize = true,\n      width = 1.8,\n      height = 1.2]\nComparison\n\&#34;Desired Progress\&#34; -&gt; Comparison -&gt; Behavior -&gt; \&#34;Environment\&#34; -&gt; \&#34;Current Progress\&#34; -&gt; Comparison\n}      \n&#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;This diagram describes a simple feedback loop whereby a Comparison between Current and Desired Progress affects behavior which, in turn, affects progress. We can emulate this in a simulation using R code.&lt;/p&gt;
&lt;p&gt;Let’s return to our example of wannabe marathon runners. We’ll simulate 10 runners whose marathon preparedness progress starts at 0 and maxes out at 1. The code consists of nested loops. In the first loop (denoted by &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;) we have each of the 10 runners. We initialize their desired condition, which is 1 because they desire to be 100% prepared for the marathon. We then initialize the current condition to 0 to reflect their progress before the simulation starts.&lt;/p&gt;
&lt;p&gt;In the second loop (denoted by &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;) we have each of the 100 days nested within each of the 10 runners. Each day, the runner compares their current progress with their desired condition. The larger this difference, the more &lt;em&gt;effort&lt;/em&gt; they’ll put into training, which increases their chances of making progress. The closer they are to their goal, the less effort they put in. If one’s effort is successful, they make 2.5% progress, if they are not successful, there is a 50/50 chance that progress will decline by 2.5% or stay at it’s current state.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;progress &amp;lt;- data.frame(matrix(nrow = 100, ncol = 10)) # initialize an empty dataframe in which to store progress

set.seed(1678) # for reproducibility

for(i in 1:10){
  desired_condition &amp;lt;- 1 # person desires to complete the task (100% completion)
  current_condition &amp;lt;- 0 # person starts with 0% completion
  for(j in 1:100){ 
    behavior &amp;lt;- desired_condition - current_condition # distance between desire and reality
    environment &amp;lt;- sample(x = c(-.025, 0, .025), size = 1, prob = c(abs(1 - behavior) * .50, abs(1 - behavior) * .50, abs(behavior)))
    current_condition &amp;lt;- current_condition + environment 
    progress[j, i] &amp;lt;- current_condition
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we track progress over each day, here’s what it looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(RColorBrewer)

progress &amp;lt;- progress %&amp;gt;%
  mutate(time = 1:100)

progress_gathered &amp;lt;- gather(progress, key = &amp;quot;person&amp;quot;, value = &amp;quot;progress&amp;quot;, -time)

progress_gathered %&amp;gt;%
  ggplot(aes(x = time, y = progress, group = person)) +
  geom_line(size = 1.25, color = &amp;quot;dodgerblue2&amp;quot;, alpha = .65) +
  labs(x = &amp;quot;Day&amp;quot;,
       y = &amp;quot;Progress toward Goal&amp;quot;,
       title = &amp;quot;Simple Feedback Loop&amp;quot;) +
  theme_minimal(base_size = 24) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/feedback_loops/Feedback_Loops_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each line represents a person in the simulation. We see a sharp spike in progress during the first 25 days, then it levels off as the runners approach their goal. Admittedly, the example might not perfectly apply to marathon runners, who may actually see less progress early on when they are in poorer physical shape. A more appropriate example might be weight loss, where people are initially highly motivated to lose weight, but then the person reaches an asymptote and less progress is made.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simple-feedback-with-an-outer-monitoring-loop&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple Feedback with an Outer Monitoring Loop&lt;/h2&gt;
&lt;p&gt;So, our first example doesn’t do a great job of simulating &lt;em&gt;human&lt;/em&gt; behavior. One major drawback is that progress is only affected by the distance between one’s current state and their desired state. People don’t always use such a wide perspective when contemplating their goals. Recent progress is perhaps more important. In the rhetoric of motivational speakers and self help books: take it one day at a time.&lt;/p&gt;
&lt;p&gt;So now we’ll incorporate an &lt;em&gt;outer loop&lt;/em&gt; that monitors activity in the &lt;em&gt;inner loop&lt;/em&gt; that we built earlier. Importantly though, this monitoring will affect stuff in the inner loop, so it’s not just a passive observer. We’re going to add a &lt;em&gt;rate&lt;/em&gt; parameter that measures the difference between one’s current progress and their progress of the previous day. This will be compared to their desired rate of progress, which we’ll choose to be 5% across the board. For greater clarity, here’s Carver’s take on how the outer loop functions:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“This second loop essentially checks on how well the first one is doing. Thus, the input for the second loop is a representation of the &lt;em&gt;rate of discrepancy reduction in the action system over time&lt;/em&gt;” (emphasis in original).&lt;/p&gt;
&lt;p&gt;— Carver (2015, p. 302)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We’ll use the word &lt;em&gt;approach&lt;/em&gt; to define behavior directed toward a goal. Carver proposed that approach is essentially a &lt;em&gt;discrepancy-reducing&lt;/em&gt; behavior (as we’ll see soon, there’s a counterpart to approach that is instrumental to understanding more complex behaviors). So the higher one’s approach, the greater chances are of success. Diagramatically, it looks like this:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;grViz html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;diagram&#34;:&#34;\n\ndigraph {\ngraph [overlap = true, fontsize = 20]\nnode [shape = box,\n      fontname = Helvetica]\n\&#34;Desired Progress\&#34;; Behavior; \&#34;Current Progress\&#34;; \&#34;Environment\&#34;\n\nnode [shape = oval,\n      width = .5]\n\&#34;Desired Rate\&#34;; \&#34;Current Rate\&#34;; \&#34;Change in Rate\&#34;\n\nnode [shape = triangle,\n      fixedsize = true,\n      width = 2.1,\n      height = 1.6]\n\&#34;Comparison A\&#34;; \&#34;Comparison B\&#34;\n\&#34;Desired Progress\&#34; -&gt; \&#34;Comparison A\&#34; -&gt; Behavior -&gt; \&#34;Environment\&#34; -&gt; \&#34;Current Progress\&#34; -&gt; \&#34;Comparison A\&#34;\nBehavior -&gt; \&#34;Current Rate\&#34; -&gt; \&#34;Comparison B\&#34;\n\&#34;Desired Rate\&#34; -&gt; \&#34;Comparison B\&#34;-&gt; \&#34;Change in Rate\&#34; -&gt; \&#34;Environment\&#34; -&gt; \&#34;Current Rate\&#34;\n}      \n&#34;,&#34;config&#34;:{&#34;engine&#34;:&#34;dot&#34;,&#34;options&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Adding another feedback loop complicates things, but what’s important to see is that the organism is comparing both the overall progress and the rate of progress. We can think of it as reflecting on how well it’s doing - not just how far it is from its goal. Now we can add these parameters to the simulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1272)

progress &amp;lt;- data.frame(matrix(nrow = 100, ncol = 10))

for(i in 1:10){
  desired_condition &amp;lt;- 1 
  desired_rate &amp;lt;- .025 # desirable rate of progress (2.5% per instance)
  current_condition &amp;lt;- 0 
  rate_comparison &amp;lt;- 1 # person starts with rate comparison of 1
  for(j in 1:100){ 
    # Inner Loop
    comparison &amp;lt;- desired_condition - current_condition 
    approach &amp;lt;- comparison * .50 + rate_comparison * .50
    environment &amp;lt;- sample(x = c(-.025, 0, .025), size = 1, prob = c(abs(1 - approach) * .50, abs(1 - approach) * .50, abs(approach)))
    current_condition &amp;lt;- current_condition + environment
    progress[j, i] &amp;lt;- current_condition

    # Outer Loop
    if(j == 1) { 
      current_rate &amp;lt;- 0
    } else {
      current_rate &amp;lt;- progress[j, i] - progress[j - 1, i]
    }
    rate_comparison &amp;lt;- desired_rate - current_rate
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, we can look at progress over time. From now on, I’ll use different colors for each person in the simulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;progress &amp;lt;- progress %&amp;gt;%
  mutate(time = 1:100)

progress_gathered &amp;lt;- gather(progress, key = &amp;quot;person&amp;quot;, value = &amp;quot;progress&amp;quot;, -time)

progress_gathered %&amp;gt;%
  ggplot(aes(x = time, y = progress, color = person)) +
  geom_line(size = 1.25, alpha = .65) +
  scale_color_brewer(palette = &amp;quot;Spectral&amp;quot;) +
  labs(x = &amp;quot;Day&amp;quot;,
       y = &amp;quot;Progress toward Goal&amp;quot;,
       title = &amp;quot;Feedback Loop with Monitoring&amp;quot;) +
  theme_minimal(base_size = 24) +
  theme(plot.title = element_text(hjust = .5),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/feedback_loops/Feedback_Loops_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Compared to before, the rate of progress is slower on average. There is also more variation between people, as some achieve 50% progress and others are not much farther from where they started. What’s happening is &lt;em&gt;approach&lt;/em&gt; behavior increases when the rate of progress is less than desirable. However, when the rate of progress is greater than desirable, approach behavior decreases. This leads to a fluctuating pattern of ups and downs, similar to what occurs in people who diet.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feedback-loops-and-mood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feedback Loops and Mood&lt;/h2&gt;
&lt;p&gt;In the last example, we used an &lt;em&gt;Outer Loop&lt;/em&gt; to monitor the rate of progress in the system. We can think of &lt;em&gt;mood&lt;/em&gt; as a byproduct of this outer loop - our subjective feeling about our progress. When we make progress toward our goals we feel happy and we relax. When we fall behind in our goals we feel upset and increase our efforts.&lt;/p&gt;
&lt;p&gt;Now we’ll add to the outer loop a variable called &lt;em&gt;affect&lt;/em&gt;, which is just the psychologist’s way of saying ‘mood’. Unlike my previous emotion simulations, we’ll stick to only one dimension of affect: valence (pleasant vs. unpleasant). High valence means you’re feeling happy, while low valence could mean sadness or anger.&lt;/p&gt;
&lt;p&gt;Affect will be neutral for the first 5 days. After that, affect will be a function of &lt;em&gt;autocorrelation&lt;/em&gt;, one’s current rate of progress, and their overall &lt;em&gt;progress slope&lt;/em&gt; (i.e., extant rate of progress). The progress slope forecasts one’s overall rate of progress based on how well they’ve done so far. In this way, if someone is progressing at a slow rate, they will feel more negative.&lt;/p&gt;
&lt;p&gt;We won’t bother with the diagrams from hereout because they become needlessly complicated. What’s important to know is that affect emerges as a byproduct of multiple comparisons between progress and desired progress. Here’s the revised R code for the simulation with affect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(12581)

progress &amp;lt;- data.frame(matrix(nrow = 100, ncol = 10))
affect &amp;lt;- data.frame(matrix(nrow = 100, ncol = 10))

for(i in 1:10){
  desired_condition &amp;lt;- 1
  desired_rate &amp;lt;- .025
  current_condition &amp;lt;- 0
  rate_comparison &amp;lt;- 1
  valence_attractor &amp;lt;- rnorm(n = 1, mean = 0.3, sd = .01) # average emotional state
  for(j in 1:100){ 
    # Inner Loop
    comparison &amp;lt;- desired_condition - current_condition
    approach &amp;lt;- comparison * .50 + rate_comparison * .50
    environment &amp;lt;- sample(x = c(-.025, 0, .025), size = 1, prob = c(abs(1 - approach) * .50, abs(1 - approach) * .50, abs(approach)))
    current_condition &amp;lt;- current_condition + environment
    progress[j, i] &amp;lt;- current_condition

    # Outer Loop
    if(j == 1) {
      current_rate &amp;lt;- 0
    } else {
      current_rate &amp;lt;- progress[j, i] - progress[j - 1, i]
    }
    rate_comparison &amp;lt;- desired_rate - current_rate
    progress_slope &amp;lt;- progress[j, i] - progress[1, i]/(j - 1)
    if(j &amp;lt;= 5) {
      affect[j, i] &amp;lt;- valence_attractor + rnorm(n = 1, mean = valence_attractor, sd = 1) * .01
    } else {
      affect[j, i] &amp;lt;- affect[j - 1, i] * .25 + progress_slope * .50 + (1 - rate_comparison) * .24 + rnorm(n = 1, mean = valence_attractor, sd = 1) * .01
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have progress and affect, let’s look at them together. We can add two plots together using the &lt;em&gt;ggpubr&lt;/em&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggpubr)

progress &amp;lt;- progress %&amp;gt;%
  mutate(time = 1:100)

progress_gathered &amp;lt;- gather(progress, key = &amp;quot;person&amp;quot;, value = &amp;quot;progress&amp;quot;, -time)

progress_plot &amp;lt;- progress_gathered %&amp;gt;%
  ggplot(aes(x = time, y = progress, color = person)) +
  geom_line(size = 1.25, alpha = .65) +
  scale_color_brewer(palette = &amp;quot;Spectral&amp;quot;) +
  labs(x = &amp;quot;Day&amp;quot;,
       y = &amp;quot;Progress toward Goal&amp;quot;,
       title = &amp;quot;Progress&amp;quot;) +
  theme_minimal(base_size = 20) +
  theme(legend.position = &amp;quot;none&amp;quot;,
        plot.title = element_text(hjust = .5),
        axis.title.y = element_text(size = 14)) 

affect &amp;lt;- affect %&amp;gt;%
  mutate(time = 1:100)

affect_gathered &amp;lt;- gather(affect, key = &amp;quot;person&amp;quot;, value = &amp;quot;affect&amp;quot;, -time)

affect_plot &amp;lt;- affect_gathered %&amp;gt;%
  ggplot(aes(x = time, y = affect, color = person)) +
  geom_line(size = 1.25, alpha = .65) +
  scale_color_brewer(palette = &amp;quot;Spectral&amp;quot;) +
  labs(x = &amp;quot;Day&amp;quot;,
       y = &amp;quot;Affect (higher = happier)&amp;quot;,
       title = &amp;quot;Affect&amp;quot;) +
  theme_minimal(base_size = 20) +
  theme(legend.position = &amp;quot;none&amp;quot;,
        plot.title = element_text(hjust = .5),
        axis.title.y = element_text(size = 14))

plots1 &amp;lt;- ggarrange(progress_plot, affect_plot, nrow = 2)
plots1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/feedback_loops/Feedback_Loops_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Affect is largely in-step with progress. Again, this is reflecting that people feel happier when they are making progress toward their goals. But we know that our mood is more complicated than this. Say we’re competing against others in a diet challenge to see who can lose the most weight over 100 days. If we see that we’re way behind in our progress, with no hope of catching up, we may throw in the towel and quit trying altogether. Indeed, humans are highly attuned to &lt;em&gt;social comparisons&lt;/em&gt;, especially in competitive environments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feedback-loops-mood-and-social-comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feedback Loops, Mood, and Social Comparison&lt;/h2&gt;
&lt;p&gt;Our last and most complex simulation will take into account how people’s behavior toward a goal changes when they receive social comparison feedback. If you have no chance winning the competition, why expend effort at all? Things get even more complicated from here, as we now have to create dependence among organisms in the simulation. Before, everyone’s performance was independent; now each new person who enters the simulation becomes aware of their performance relative to their peers’.&lt;/p&gt;
&lt;p&gt;We add two new parameters. The first, &lt;em&gt;average progress rate&lt;/em&gt;, is the average slope of prior runs in the simulation (akin to watching your competitor’s performance and then yourself performing). Indeed, these are not run in parallel, so the first simulated person has no insight into the performance of others, while the second person is only aware of how well the first person did, and so on. The second parameter is &lt;em&gt;relative standing&lt;/em&gt; and it is simply the difference between one’s current progress rate and the average progress rate. In general, as relative standing increases (as the distance becomes larger) avoidance behavior increases. This works in two ways: (1) performance exceeds that of others and effort goes down (e.g., coasting) or (2) performance pales relative to others and hopelessness ensues. To model this, at the halfway point, if one’s relative standing is 35% less than the average, that person will effectively give up, saving themselves from severe upset. Indeed, some have suggested that this is an adaptive response because it allows people to reprioritize their efforts (Simon, 1967).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1249)

progress &amp;lt;- data.frame(matrix(nrow = 100, ncol = 10))
affect &amp;lt;- data.frame(matrix(nrow = 100, ncol = 10))

for(i in 1:10){
  desired_condition &amp;lt;- 1
  desired_rate &amp;lt;- .050
  current_condition &amp;lt;- 0
  rate_comparison &amp;lt;- 1
  valence_attractor &amp;lt;- rnorm(n = 1, mean = 0.3, sd = .01)
  average_progress_rate &amp;lt;- 0
  relative_standing &amp;lt;- 1
  for(j in 1:100){ 
    # Inner Loop
    comparison &amp;lt;- desired_condition - current_condition
    approach &amp;lt;- (abs(comparison * .50 + rate_comparison * .50))/(abs(relative_standing) + (comparison * .75 + abs(rate_comparison) * .25))
    avoidance &amp;lt;- (abs(relative_standing))/(abs(relative_standing) + (comparison * .75 + abs(rate_comparison) * .25))
    environment &amp;lt;- sample(x = c(-.025, 0, .025), size = 1, prob = c(avoidance * .50, avoidance * .50, approach))
    current_condition &amp;lt;- current_condition + environment 
    progress[j, i] &amp;lt;- ifelse(current_condition &amp;lt;= 0, 0, current_condition)

    # Outer Loop
    if(j == 1) {
      current_rate &amp;lt;- 0
    } else {
      current_rate &amp;lt;- progress[j, i] - progress[j - 1, i]
      progress_slope &amp;lt;- progress[j, i] - progress[1, i]/(j - 1)
      relative_standing &amp;lt;- progress_slope - average_progress_rate
    }
    rate_comparison &amp;lt;- desired_rate - current_rate
    if(j &amp;lt;= 5) {
      affect[j, i] &amp;lt;- valence_attractor + rnorm(n = 1, mean = valence_attractor, sd = 1) * .01
    } else if (j &amp;gt;= 50 &amp;amp;&amp;amp; relative_standing &amp;lt;= .35) {
      desired_condition &amp;lt;- current_condition
      rate_comparison &amp;lt;- 0
      affect[j, i] &amp;lt;- affect[j - 1, i] * .99 + rnorm(n = 1, mean = valence_attractor, sd = 1) * .01
    } else {
      affect[j, i] &amp;lt;- affect[j - 1, i] * .25 + progress_slope * .50 + relative_standing * .24 + rnorm(n = 1, mean = valence_attractor, sd = 1) * .01
    }
  }
  if(i &amp;gt; 1){
    average_progress_rate &amp;lt;- mean(t(progress[100, (i - 1):1]))
  } else {
    average_progress_rate &amp;lt;- 0
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running the code above, we can enter it into the next code chunk to get the plots. Again, we’ll plot them together to compare progress and affect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;progress &amp;lt;- progress %&amp;gt;%
  mutate(time = 1:100)

progress_gathered &amp;lt;- gather(progress, key = &amp;quot;person&amp;quot;, value = &amp;quot;progress&amp;quot;, -time)

progress_plot2 &amp;lt;- progress_gathered %&amp;gt;%
  ggplot(aes(x = time, y = progress, color = person)) +
  geom_line(size = 1.25, alpha = .65) +
  scale_color_brewer(palette = &amp;quot;Spectral&amp;quot;) +
  labs(x = &amp;quot;Day&amp;quot;,
       y = &amp;quot;Progress toward Goal&amp;quot;,
       title = &amp;quot;Progress&amp;quot;) +
  theme_minimal(base_size = 20) +
  theme(legend.position = &amp;quot;none&amp;quot;,
        plot.title = element_text(hjust = .5),
        axis.title.y = element_text(size = 14)) 

affect &amp;lt;- affect %&amp;gt;%
  mutate(time = 1:100)

affect_gathered &amp;lt;- gather(affect, key = &amp;quot;person&amp;quot;, value = &amp;quot;affect&amp;quot;, -time)

affect_plot2 &amp;lt;- affect_gathered %&amp;gt;%
  ggplot(aes(x = time, y = affect, color = person)) +
  geom_line(size = 1.25, alpha = .65) +
  scale_color_brewer(palette = &amp;quot;Spectral&amp;quot;) +
    labs(x = &amp;quot;Day&amp;quot;,
       y = &amp;quot;Affect (higher = happier)&amp;quot;,
       title = &amp;quot;Affect&amp;quot;) +
  theme_minimal(base_size = 20) +
  theme(legend.position = &amp;quot;none&amp;quot;,
        plot.title = element_text(hjust = .5),
        axis.title.y = element_text(size = 14))

plots2 &amp;lt;- ggarrange(progress_plot2, affect_plot2, nrow = 2)
plots2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/feedback_loops/Feedback_Loops_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can clearly see where a few competitors bailed out and their progress dropped to zero. This is because their relative standing was weak and they relinquished. Importantly though, they didn’t suffer too heavy a blow in affect, and this is because they no longer valued progress toward this goal.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;We set out to show that motivation and emotion can be simulated using fairly simple feedback loops. Of course, human behavior is exceedingly more complicated, but we can learn a lot by simulating ersatz humans with only a handful of parameters. As I continue to work toward my PhD, my appreciation for Carver’s computational models only grows stronger. My goal is to see psychological research and computer simulations partake in their own form of feedback loop, where knowledge from the former feeds into our construction of the latter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/1754073915590616?journalCode=emra&#34;&gt;Carver, C. S. (2015). Control processes, priority management, and affective dynamics. &lt;em&gt;Emotion Review, 7&lt;/em&gt;, 301-307.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Powers, W. T. (1973). &lt;em&gt;Behavior: The control of perception.&lt;/em&gt; Chicago, IL: Aldine.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://psycnet.apa.org/record/1967-03946-001&#34;&gt;Simon, H. A. (1967). Motivational and emotional controls of cognition. &lt;em&gt;Psychology Review, 74&lt;/em&gt;, 29-39.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Model and Simulation of Emotion Dynamics</title>
      <link>/post/emotion-simulation1/emotion_sim/</link>
      <pubDate>Wed, 26 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/emotion-simulation1/emotion_sim/</guid>
      <description>


&lt;p&gt;Emotion dynamics is the study of how emotions change over time. Sometimes our feelings are quite stable, but other times capricious. Measuring and predicting these patterns for different people is somewhat of a Holy Grail for emotion researchers. In particular, some researchers are aspiring to discover mathematical laws that capture the complexity of our inner emotional experiences - much like physicists divining the laws that govern objects in the natural environment. These discoveries would revolutionize our understanding of our everyday feelings and when our emotions can go awry.&lt;/p&gt;
&lt;p&gt;This series of blog posts, which I kicked off earlier this month with a &lt;a href=&#34;https://willhipson.netlify.com/post/basketball_sim/basketball_sim/&#34;&gt;simulation of emotions during basketball games&lt;/a&gt;, is inspired by researchers like &lt;a href=&#34;https://ppw.kuleuven.be/okp/people/Peter_Kuppens/&#34;&gt;Peter Kuppens&lt;/a&gt; and &lt;a href=&#34;https://www.queensu.ca/psychology/people/faculty/tom-hollenstein&#34;&gt;Tom Hollenstein&lt;/a&gt; (to name a few) who have collected and analyzed reams of intensive self-reports on people’s feelings from one moment to the next. My approach is to reverse engineer these insights and generate models that &lt;em&gt;simulate&lt;/em&gt; emotions evolving over time - like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-simulation1/Emotion_Simulation_files/figure-html/unnamed-chunk-1-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;div id=&#34;affective-state-space&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Affective State Space&lt;/h2&gt;
&lt;p&gt;We start with the affective state space - the theoretical landscape on which our conscious feelings roam free. This space is represented as &lt;em&gt;two-dimensional&lt;/em&gt;, although we acknowledge that this fails to capture all aspects of conscious feeling. The first dimension, represented along the x-axis, is &lt;em&gt;valence&lt;/em&gt; and this refers to how unpleasant vs. pleasant we feel. The second dimension, represented along the y-axis, is &lt;em&gt;arousal&lt;/em&gt;. Somewhat less intuitive, arousal refers to how deactivated/sluggish/sleepy vs. activated/energized/awake we feel. At any time, our emotional state can be defined in terms of valence and arousal. So if you’re feeling stressed you would be low in valence and high in arousal. Let’s say you’re serene and calm, then you would be high in valence and low in arousal. Most of the time, we feel moderately high valence and moderate arousal (i.e., content), but if you’re the type of person who is chronically stressed, this would be different.&lt;/p&gt;
&lt;p&gt;This is all well and good when we think about how we’re feeling right now, but it’s also worth considering how our emotions are changing. On a regular day, our emotions undergo minor fluctuations - sometimes in response to minor hassles or victories, and sometimes for no discernible reason. In this small paragraph, I’ve laid out a number of parameters, all of which vary between different people:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Attractor&lt;/strong&gt;: Our typical emotional state. At any given moment, our feelings are pulled toward this state. Some people tend to be happier, whereas others are less happy.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stability&lt;/strong&gt;: How emotionally stable one is. Some people are more emotionally stable than others. Even in the face of adversity, an emotionally stable person keeps their cool.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dispersion&lt;/strong&gt;: The range of our emotional landscape. Some people experience intense highs and lows, whereas others persist somewhere in the middle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’ll keep all of this in mind for the simulation. We’ll start with a fairly simple simulation with 100 hypothetical people. We’ll need the following packages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)
library(tidyverse)
library(sn)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then we’ll create a function that performs the simulation. Note that each person &lt;em&gt;i&lt;/em&gt; has their own attractor, recovery rate, stability, and dispersion. For now we’ll just model random fluctuations in emotions, a sort of Brownian motion. You can imagine our little &lt;strong&gt;simulatons&lt;/strong&gt; (fun name for the hypothetical people in the simulation) sitting around on an average day doing nothing in particular.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simulate_affect &amp;lt;- function(n = 2, time = 250, negative_event_time = NULL) {
  dt &amp;lt;- data.frame(matrix(nrow = time, ncol = 1))
  colnames(dt) &amp;lt;- &amp;quot;time&amp;quot;
  dt$time &amp;lt;- 1:time
  
  valence &amp;lt;- data.frame(matrix(nrow = time, ncol = 0))
  arousal &amp;lt;- data.frame(matrix(nrow = time, ncol = 0))
  
  for(i in 1:n) {
    attractor_v &amp;lt;- rnorm(1, mean = 3.35, sd = .75)
    instability_v &amp;lt;- sample(3:12, 1, replace = TRUE, prob = c(.18, .22, .18, .15, .8, .6, .5, .4, .2, .1))
    dispersion_v &amp;lt;- abs(rsn(1, xi = .15, omega = .02, alpha = -6) * instability_v) #rsn simulates a skewed distribution.
    if(!is.null(negative_event_time)) {
      recovery_rate &amp;lt;- sample(1:50, 1, replace = TRUE) + negative_event_time
      negative_event &amp;lt;- (dt$time %in% negative_event_time:recovery_rate) * seq.int(50, 1, -1)
    }
    else {
      negative_event &amp;lt;- 0
    }
    valence[[i]] &amp;lt;- ksmooth(x = dt$time,
                            y = (negative_event * -.10) + arima.sim(list(order = c(1, 0, 0),
                                               ar = .50),
                                          n = time),
                            bandwidth = time/instability_v, kernel = &amp;quot;normal&amp;quot;)$y * dispersion_v + attractor_v 

#instability is modelled in the bandwidth term of ksmooth, such that higher instability results in higher bandwidth (greater fluctuation). 
#dispersion scales the white noise (arima) parameter, such that there are higher peaks and troughs at higher dispersion.
    
    attractor_a &amp;lt;- rnorm(1, mean = .50, sd = .75) + sqrt(instability_v) #arousal attractor is dependent on instability. This is because high instability is associated with higher arousal states.
    instability_a &amp;lt;- instability_v + sample(-1:1, 1, replace = TRUE)
    dispersion_a &amp;lt;- abs(rsn(1, xi = .15, omega = .02, alpha = -6) * instability_a)
    arousal[[i]] &amp;lt;- ksmooth(x = dt$time,
                            y = (negative_event * .075) + arima.sim(list(order = c(1, 0, 0),
                                               ar = .50),
                                          n = time),
                            bandwidth = time/instability_a, kernel = &amp;quot;normal&amp;quot;)$y * dispersion_a + attractor_a
  }
  
  valence[valence &amp;gt; 6] &amp;lt;- 6
  valence[valence &amp;lt; 0] &amp;lt;- 0
  arousal[arousal &amp;gt; 6] &amp;lt;- 6
  arousal[arousal &amp;lt; 0] &amp;lt;- 0
  
  colnames(valence) &amp;lt;- paste0(&amp;quot;valence_&amp;quot;, 1:n)
  colnames(arousal) &amp;lt;- paste0(&amp;quot;arousal_&amp;quot;, 1:n)
  
  dt &amp;lt;- cbind(dt, valence, arousal)
  
  return(dt)
}

set.seed(190625)

emotions &amp;lt;- simulate_affect(n = 100, time = 300)

emotions %&amp;gt;%
  select(valence_1, arousal_1) %&amp;gt;%
  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   valence_1 arousal_1
## 1  1.328024  5.380643
## 2  1.365657  5.385633
## 3  1.401849  5.390470
## 4  1.436284  5.395051
## 5  1.468765  5.399162
## 6  1.499062  5.402752&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So we see the first six rows for participant 1’s valence and arousal. But if we want to plot these across multiple simulatons, we need to wrangle the data into long form. We’ll also compute some measures of within-person deviation. The Root Mean Square Successive Difference (RMSSD) takes into account gradual shifts in the mean. Those who are more emotionally unstable will have a higher RMSSD. For two dimensions (valence and arousal) we’ll just compute the mean RMSSD.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_long &amp;lt;- emotions %&amp;gt;%
  gather(key, value, -time) %&amp;gt;%
  separate(key, into = c(&amp;quot;dimension&amp;quot;, &amp;quot;person&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  spread(dimension, value) %&amp;gt;%
  group_by(person) %&amp;gt;%
  mutate(rmssd_v = rmssd(valence),
         rmssd_a = rmssd(arousal),
         rmssd_total = mean(rmssd_v + rmssd_a)) %&amp;gt;%
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see what this looks like for valence and arousal individually.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_long %&amp;gt;%
  ggplot(aes(x = time, y = valence, group = person, color = rmssd_v)) +
  geom_line(size = .75, alpha = .75) +
  scale_color_gradient2(low = &amp;quot;black&amp;quot;, mid = &amp;quot;grey&amp;quot;, high = &amp;quot;red&amp;quot;, midpoint = median(emotions_long$rmssd_v)) +
  labs(x = &amp;quot;Time&amp;quot;,
       y = &amp;quot;Valence&amp;quot;,
       color = &amp;quot;Instability&amp;quot;,
       title = &amp;quot;Simulated Valence Scores over Time for 100 People&amp;quot;) +
  theme_minimal(base_size = 16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-simulation1/Emotion_Simulation_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_long %&amp;gt;%
  ggplot(aes(x = time, y = arousal, group = person, color = rmssd_a)) +
  geom_line(size = .75, alpha = .75) +
  scale_color_gradient2(low = &amp;quot;black&amp;quot;, mid = &amp;quot;grey&amp;quot;, high = &amp;quot;red&amp;quot;, midpoint = median(emotions_long$rmssd_a)) +
  labs(x = &amp;quot;Time&amp;quot;,
       y = &amp;quot;Arousal&amp;quot;,
       color = &amp;quot;Instability&amp;quot;,
       title = &amp;quot;Simulated Arousal Scores over Time for 100 People&amp;quot;) +
  theme_minimal(base_size = 16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-simulation1/Emotion_Simulation_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that some lines are fairly flat and others fluctuate more widely. More importantly, most people are somewhere in the middle.&lt;/p&gt;
&lt;p&gt;We can get a sense of one simulated person’s affective state space as well. The goal here is to mimic the kinds of models shown in &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/20853980&#34;&gt;Kuppens, Oravecz, and Tuerlinckx (2010)&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_long %&amp;gt;%
  filter(person %in% sample(1:100, 6, replace = FALSE)) %&amp;gt;%
  ggplot(aes(x = valence, y = arousal, group = person)) +
  geom_path(size = .75) + 
  scale_x_continuous(limits = c(0, 6)) +
  scale_y_continuous(limits = c(0, 6)) +
  labs(x = &amp;quot;Valence&amp;quot;,
       y = &amp;quot;Arousal&amp;quot;,
       title = &amp;quot;Affective State Space for Six Randomly Simulated People&amp;quot;) +
  facet_wrap(~person) +
  theme_minimal(base_size = 18) +
  theme(plot.title = element_text(size = 18, hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-simulation1/Emotion_Simulation_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;animating-the-affective-state-space&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Animating the Affective State Space&lt;/h2&gt;
&lt;p&gt;To really appreciate what’s going on, we need to animate this over time. I’ll add some labels to the affective state space so that it’s easier to interpret what one might be feeling at that time. I’ll also add color to show which individuals are more unstable according to RMSSD.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gganimate)

p &amp;lt;- emotions_long %&amp;gt;%
  ggplot(aes(x = valence, y = arousal, color = rmssd_total)) +
  annotate(&amp;quot;text&amp;quot;, x = c(1.5, 4.5, 1.5, 4.5), y = c(1.5, 1.5, 4.5, 4.5), label = c(&amp;quot;Gloomy&amp;quot;, &amp;quot;Calm&amp;quot;, &amp;quot;Anxious&amp;quot;, &amp;quot;Happy&amp;quot;),
           size = 10, alpha = .50) + 
  annotate(&amp;quot;rect&amp;quot;, xmin = 0, xmax = 3, ymin = 0, ymax = 3, alpha = 0.25, color = &amp;quot;black&amp;quot;, fill = &amp;quot;white&amp;quot;) +
  annotate(&amp;quot;rect&amp;quot;, xmin = 3, xmax = 6, ymin = 0, ymax = 3, alpha = 0.25, color = &amp;quot;black&amp;quot;, fill = &amp;quot;white&amp;quot;) +
  annotate(&amp;quot;rect&amp;quot;, xmin = 0, xmax = 3, ymin = 3, ymax = 6, alpha = 0.25, color = &amp;quot;black&amp;quot;, fill = &amp;quot;white&amp;quot;) +
  annotate(&amp;quot;rect&amp;quot;, xmin = 3, xmax = 6, ymin = 3, ymax = 6, alpha = 0.25, color = &amp;quot;black&amp;quot;, fill = &amp;quot;white&amp;quot;) +
  geom_point(size = 3.5) +
  scale_color_gradient2(low = &amp;quot;black&amp;quot;, mid = &amp;quot;grey&amp;quot;, high = &amp;quot;red&amp;quot;, midpoint = median(emotions_long$rmssd_total)) +
  scale_x_continuous(limits = c(0, 6)) +
  scale_y_continuous(limits = c(0, 6)) +
  labs(x = &amp;quot;Valence&amp;quot;,
       y = &amp;quot;Arousal&amp;quot;,
       color = &amp;quot;Instability&amp;quot;,
       title = &amp;#39;Time: {round(frame_time)}&amp;#39;) +
  transition_time(time) +
  theme_minimal(base_size = 18)

ani_p &amp;lt;- animate(p, nframes = 320, end_pause = 20, fps = 16, width = 550, height = 500)

ani_p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-simulation1/Emotion_Simulation_files/figure-html/unnamed-chunk-7-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theres-a-storm-coming&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;There’s a Storm Coming…&lt;/h2&gt;
&lt;p&gt;Our simulation does a pretty good job at emulating the natural ebb and flow of emotions, but we know that emotions can be far more volatile. Let’s subject our simulation to a negative event. Perhaps all 100 &lt;strong&gt;simulatons&lt;/strong&gt; co-authored a paper that just got rejected. In the function &lt;em&gt;simulate_affect&lt;/em&gt;, there’s an optional argument &lt;em&gt;negative_event_time&lt;/em&gt; that causes a negative event to occur at the specified time. For this, we need to consider one more emotion dynamics parameter:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Recovery rate&lt;/strong&gt;: How quickly one recovers from an emotional event. If something bad happens, how long does it take to return to the attractor. You can see how I’ve modelled this parameter in the function above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So we’ll run the simulation with a negative event arising at &lt;em&gt;t&lt;/em&gt; = 150. The negative event will cause a downward spike in valence and an upward spike in arousal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_event &amp;lt;- simulate_affect(n = 100, time = 300, negative_event_time = 150)

emotions_event_long &amp;lt;- emotions_event %&amp;gt;%
  gather(key, value, -time) %&amp;gt;%
  separate(key, into = c(&amp;quot;dimension&amp;quot;, &amp;quot;person&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  spread(dimension, value) %&amp;gt;%
  group_by(person) %&amp;gt;%
  mutate(rmssd_v = rmssd(valence),
         rmssd_a = rmssd(arousal),
         rmssd_total = mean(rmssd_v + rmssd_a)) %&amp;gt;%
  ungroup()

emotions_event_long %&amp;gt;%
  ggplot(aes(x = time, y = valence, group = person, color = rmssd_v)) +
  geom_line(size = .75, alpha = .75) +
  scale_color_gradient2(low = &amp;quot;black&amp;quot;, mid = &amp;quot;grey&amp;quot;, high = &amp;quot;red&amp;quot;, midpoint = median(emotions_event_long$rmssd_v)) +
  labs(x = &amp;quot;Time&amp;quot;,
       y = &amp;quot;Valence&amp;quot;,
       color = &amp;quot;Instability&amp;quot;,
       title = &amp;quot;Simulated Valence Scores over Time for 100 People&amp;quot;) +
  theme_minimal(base_size = 16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-simulation1/Emotion_Simulation_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_event_long %&amp;gt;%
  ggplot(aes(x = time, y = arousal, group = person, color = rmssd_a)) +
  geom_line(size = .75, alpha = .75) +
  scale_color_gradient2(low = &amp;quot;black&amp;quot;, mid = &amp;quot;grey&amp;quot;, high = &amp;quot;red&amp;quot;, midpoint = median(emotions_event_long$rmssd_a)) +
  labs(x = &amp;quot;Time&amp;quot;,
       y = &amp;quot;Arousal&amp;quot;,
       color = &amp;quot;Instability&amp;quot;,
       title = &amp;quot;Simulated Arousal Scores over Time for 100 People&amp;quot;) +
  theme_minimal(base_size = 16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-simulation1/Emotion_Simulation_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s pretty clear that something bad happened. Of course, some of our &lt;strong&gt;simulatons&lt;/strong&gt; are unflappable, but most experienced a drop in valence and spike in arousal that we might identify as anxiety. Again, let’s visualize this evolving over time. Pay close attention to when the timer hits 150.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2 &amp;lt;- emotions_event_long %&amp;gt;%
  ggplot(aes(x = valence, y = arousal, color = rmssd_total)) +
  annotate(&amp;quot;text&amp;quot;, x = c(1.5, 4.5, 1.5, 4.5), y = c(1.5, 1.5, 4.5, 4.5), label = c(&amp;quot;Gloomy&amp;quot;, &amp;quot;Calm&amp;quot;, &amp;quot;Anxious&amp;quot;, &amp;quot;Happy&amp;quot;),
           size = 10, alpha = .50) + 
  annotate(&amp;quot;rect&amp;quot;, xmin = 0, xmax = 3, ymin = 0, ymax = 3, alpha = 0.25, color = &amp;quot;black&amp;quot;, fill = &amp;quot;white&amp;quot;) +
  annotate(&amp;quot;rect&amp;quot;, xmin = 3, xmax = 6, ymin = 0, ymax = 3, alpha = 0.25, color = &amp;quot;black&amp;quot;, fill = &amp;quot;white&amp;quot;) +
  annotate(&amp;quot;rect&amp;quot;, xmin = 0, xmax = 3, ymin = 3, ymax = 6, alpha = 0.25, color = &amp;quot;black&amp;quot;, fill = &amp;quot;white&amp;quot;) +
  annotate(&amp;quot;rect&amp;quot;, xmin = 3, xmax = 6, ymin = 3, ymax = 6, alpha = 0.25, color = &amp;quot;black&amp;quot;, fill = &amp;quot;white&amp;quot;) +
  geom_point(size = 3.5) +
  scale_color_gradient2(low = &amp;quot;black&amp;quot;, mid = &amp;quot;grey&amp;quot;, high = &amp;quot;red&amp;quot;, midpoint = median(emotions_event_long$rmssd_total)) +
  scale_x_continuous(limits = c(0, 6)) +
  scale_y_continuous(limits = c(0, 6)) +
  labs(x = &amp;quot;Valence&amp;quot;,
       y = &amp;quot;Arousal&amp;quot;,
       color = &amp;quot;Instability&amp;quot;,
       title = &amp;#39;Time: {round(frame_time)}&amp;#39;) +
  transition_time(time) +
  theme_minimal(base_size = 18)

ani_p2 &amp;lt;- animate(p2, nframes = 320, end_pause = 20, fps = 16, width = 550, height = 500)

ani_p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-simulation1/Emotion_Simulation_files/figure-html/unnamed-chunk-9-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The overall picture is that some are more emotionally resilient than others. As of now, all the &lt;strong&gt;simulatons&lt;/strong&gt; return to their baseline attractor, but we would realistically expect some to stay stressed or gloomy following bad news. In the coming months I’ll be looking into how to incorporate emotion regulation into the simulation. For example, maybe some of the &lt;strong&gt;simulatons&lt;/strong&gt; use better coping strategies than others? I’m also interested in incorporating &lt;em&gt;appraisal&lt;/em&gt; mechanisms that allow for different reactions depending on the type of emotional stimulus.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/20853980&#34;&gt;Kuppens, P., Oravecz, Z., &amp;amp; Tuerlinckx, F. (2010). Feelings change: Accounting for individual differences in the temporal dynamics of affect. &lt;em&gt;Journal of Personality and Social Psychology, 99&lt;/em&gt;, 1042-1060&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Emotions during a Basketball Game - Just a Feeling in the Crowd</title>
      <link>/post/basketball_sim/basketball_sim/</link>
      <pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/basketball_sim/basketball_sim/</guid>
      <description>


&lt;p&gt;Sporting events host witness to a wide range of human emotion. The emotional ups and downs are especially clear among invested fans. Fans experience the joy and excitement of a triumphant comeback, or the anxiety and disappointment of a loss. It is particularly interesting to see how emotions differ from two opposing fan groups watching the same match.&lt;/p&gt;
&lt;p&gt;I decided to perform some simulations on how a crowd of fans would react during a basketball game. Why basketball? Two reasons: First is the frequency of scoring (more baskets = more &lt;em&gt;reactive&lt;/em&gt; simulation), and second is that the NBA finals are in swing at time of writing.&lt;/p&gt;
&lt;p&gt;Below I’ve described the parameters of the simulation in detail and provided reproducible code for all examples, but here’s the quick and dirty. I’m simulating fans’ happiness and nerves (valence and arousal). Happiness reflects baskets scored (up for team basket and down for enemy basket) and overall team performance. Nerves reflects baskets scored, time remaining in game, and the difference between score (higher for closer game). There are other parameters involved that are described in more detail below.&lt;/p&gt;
&lt;p&gt;Let’s run the simulation! I’m simulating a short game that’s about 16 minutes long, but the simulation is sped up. There are 25 fans per team with each dot representing a fan. Blue dots root for the blue team and red dots for red. In this game, blue starts ahead, but red makes a rallying comeback.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/basketball_sim/Basketball_Simulation_files/figure-html/unnamed-chunk-1-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;What are we seeing here? As the game progresses, fans move into different emotional states. I’ve used the terms &lt;em&gt;content&lt;/em&gt;, &lt;em&gt;disappointed&lt;/em&gt;, &lt;em&gt;nervous&lt;/em&gt;, and &lt;em&gt;excited&lt;/em&gt; as helpful placeholders, but we should think of these in terms of varying along dimensions of &lt;em&gt;valence&lt;/em&gt; (e.g., happiness) and &lt;em&gt;arousal&lt;/em&gt; (e.g., nerves, excitedness). For example, when their team is doing well, fans are happier, but they’re only excited when the score is close. For each basket, there’s a brief spike in arousal and happiness. As the game draws closer to an end, fans get more excited/anxious, especially if the score is close.&lt;/p&gt;
&lt;p&gt;So how would this play out if one team decimated the other? Let’s give the red team a slight edge over blue:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/basketball_sim/Basketball_Simulation_files/figure-html/unnamed-chunk-2-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Notice how ‘excited’ becomes ‘content’ and ‘nervous’ turns to ‘disappointment’ because the difference becomes insurmountable for the blue team. We can plug in any numbers for this function to produce different results. However, the algorithm is highly sensitive to small changes in scoring chance because this is calculated for every &lt;em&gt;second&lt;/em&gt; of the game.&lt;/p&gt;
&lt;div id=&#34;going-behind-the-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Going Behind the Simulation&lt;/h2&gt;
&lt;p&gt;Skip this if you have no interest in the psychology of emotions. What I’m striving to simulate are the laws of emotion dynamics (Kuppens &amp;amp; Verduyn, 2017). Emotions change from moment to moment, but there’s also some stability from one moment to the next. Apart from when a basket is scored, most fans cluster around a particular state (this is called an &lt;em&gt;attractor state&lt;/em&gt;). Any change is attributable to random fluctuations (e.g., one fan spills some of their beer, maybe another fan sees an amusing picture of a cat on their phone). When a basket &lt;em&gt;is&lt;/em&gt; scored, this causes a temporary fluctuation away from the attractor state, after which people resort back to their attractor. More gradual situational factors result in small changes in attractors too. This is why arousal tends to increase with closer games and when the game is approaching the end.&lt;/p&gt;
&lt;div id=&#34;detailed-aspects-of-simulation-and-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Detailed Aspects of Simulation and Code&lt;/h3&gt;
&lt;p&gt;There are two parts to this simulation. Part 1 simulates a basketball game for a specified duration of time (&lt;em&gt;n_seconds&lt;/em&gt;) and specified probabilities of scoring. The simulation is also designed with a small post game period - sort of like a cool down after the game. During the post game, fans’ arousal returns to baseline.&lt;/p&gt;
&lt;p&gt;The second and more complicated part of the simulation is the emotional part. Each fan has a score on valence and arousal at each second. These can be further broken down into fixed effects and random effects. The fixed effect for valence is described as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Valence = 3 + \beta_{1overall.difference}(.20) + \beta_{2recent.score(1.5 + \frac{current.time}{total.time})} - \beta_{3recent.enemy.score}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The fixed effect for arousal looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Arousal = 3 + \beta_{1 (\frac{current.time}{total.time})(.75)(post.game)} + \beta_{2(1 - \sqrt{|difference|})} + \beta_{3|recent.basket|}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The exact values for these coefficients are somewhat arbitrary. They resulted from a lot of trial and error to identify which best simulated emotion dynamics.&lt;/p&gt;
&lt;p&gt;Each fan’s valence and arousal is calculated using the above equations + variability. The variability is added by applying random variation to the first constant in the above equations and white noise with a simulated ARIMA model. On top of this, I used a kernel regression smoother to smooth out each fan’s trajectory. To simulate delayed reaction time to baskets, I added some &lt;em&gt;lags&lt;/em&gt; at random intervals - otherwise the simulated fans appear to react quicker than the score board. Here’s what all of this looks like for one fan’s valence over time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

game_long %&amp;gt;%
  filter(fan == 1,
         team == &amp;quot;team1&amp;quot;) %&amp;gt;%
  ggplot(aes(x = second, y = valence)) +
  geom_line(color = &amp;quot;red&amp;quot;, size = 1) +
  theme_minimal(base_size = 16) +
  labs(title = &amp;quot;Fan One&amp;#39;s Valence over Time&amp;quot;,
       x = &amp;quot;Time&amp;quot;,
       y = &amp;quot;Valence&amp;quot;) +
  theme(legend.position = &amp;quot;none&amp;quot;,
        axis.title.x = element_text(size = 18),
        axis.title.y = element_text(size = 18),
        plot.title = element_text(size = 18, hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/basketball_sim/Basketball_Simulation_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see this across all fans as well:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;game_long %&amp;gt;%
  ggplot(aes(x = instance, color = team)) +
  geom_line(aes(y = valence), alpha = .65) +
  labs(title = &amp;quot;Valence over Time&amp;quot;,
       x = &amp;quot;Time&amp;quot;,
       y = &amp;quot;Valence&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;,
        axis.title.x = element_text(size = 18),
        axis.title.y = element_text(size = 18),
        plot.title = element_text(size = 18, hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/basketball_sim/Basketball_Simulation_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And for arousal…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;game_long %&amp;gt;%
  ggplot(aes(x = instance, color = team)) +
  geom_line(aes(y = arousal), alpha = .65) +
  labs(title = &amp;quot;Arousal over Time&amp;quot;,
       x = &amp;quot;Time&amp;quot;,
       y = &amp;quot;Arousal&amp;quot;) +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;,
        axis.title.x = element_text(size = 18),
        axis.title.y = element_text(size = 18),
        plot.title = element_text(size = 18, hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/basketball_sim/Basketball_Simulation_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For those interested, here’s the full code for the simulation and first animation. Have fun with it! I’m also looking for ways to improve it. Ironically, I don’t watch a ton of sports, so if you think I’m open to input on how to make it more realistic!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(forecast)
library(gganimate)
library(extrafont)

game_simulation &amp;lt;- function(n_seconds = 1000, n_fans = 25, team1_prob = 0.012, team2_prob = 0.012) {
  
  score_board &amp;lt;- data.frame(matrix(nrow = n_seconds, ncol = 5))
  colnames(score_board) &amp;lt;- c(&amp;quot;second&amp;quot;, &amp;quot;team1_score&amp;quot;, &amp;quot;team2_score&amp;quot;, &amp;quot;difference&amp;quot;, &amp;quot;end_game&amp;quot;)
  score_board$second &amp;lt;- c(1:n_seconds)
  
  score_board$team1_score &amp;lt;- cumsum(sample(c(0, 1, 1.5), n_seconds, replace = TRUE, prob = c(1 - team1_prob, team1_prob * .90, team1_prob * .10)))
  
  score_board$team2_score &amp;lt;- cumsum(sample(c(0, 1, 1.5), n_seconds, replace = TRUE, prob = c(1 - team2_prob, team2_prob * .90, team2_prob * .10)))
  
  score_board$difference &amp;lt;- score_board$team1_score - score_board$team2_score
  
  score_board$end_game = 1
  
  total_time &amp;lt;- n_seconds + n_seconds * .05
  
  end_time &amp;lt;- data.frame(max(score_board$second + 1):(total_time))
  colnames(end_time) &amp;lt;- &amp;quot;second&amp;quot;
  end_state &amp;lt;- score_board[nrow(score_board), 2:5]
  post_game &amp;lt;- data.frame(matrix(nrow = n_seconds * .05, ncol = 0),
                          &amp;quot;team1_score&amp;quot; = end_state$team1_score,
                          &amp;quot;team2_score&amp;quot; = end_state$team2_score,
                          &amp;quot;difference&amp;quot; = end_state$difference,
                          &amp;quot;end_game&amp;quot; = 0)
  
  post_game &amp;lt;- cbind(end_time, post_game)
  
  score_board &amp;lt;- rbind(score_board, post_game)
  
  valence &amp;lt;- function(time, overall_difference, recent_team_score, recent_enemy_score) (
    (overall_difference * .20) + (recent_team_score * (1.5 + time/length(time))) - (recent_enemy_score)
  )
  
  arousal &amp;lt;- function(time, difference, recent_score, end_game) (
    ((time/n_seconds) * .75 * end_game) + (1 - sqrt(abs(difference))) + abs(recent_score * 1.5)
  )
  
  team1_valence &amp;lt;- data.frame(matrix(nrow = total_time, ncol = 0))
  team1_arousal &amp;lt;- data.frame(matrix(nrow = total_time, ncol = 0))
  team2_valence &amp;lt;- data.frame(matrix(nrow = total_time, ncol = 0))
  team2_arousal &amp;lt;- data.frame(matrix(nrow = total_time, ncol = 0))
  
  for(i in 1:n_fans) {

    team1_valence[[i]] &amp;lt;- ksmooth(x = score_board$second,
                                  y = rnorm(n = 1, mean = 3, sd = .25) + (arima.sim(list(order = c(1, 0, 1), ma = -.1, ar = c(.9)), n = total_time) * .25) + valence(time = score_board$second,
                                  overall_difference = score_board$difference,
                                  recent_team_score = lag(score_board$team1_score, n = sample(c(1:5), 1), default = 0) - lag(score_board$team1_score, n = sample(c(6:8), 1), default = 0),
                                  recent_enemy_score = lag(score_board$team2_score, n = sample(c(1:5), 1), default = 0) - lag(score_board$team2_score, n = sample(c(6:8), 1), default = 0)),
                                  bandwidth = total_time/100, kernel = &amp;quot;normal&amp;quot;)$y
    
  }
  
  colnames(team1_valence) &amp;lt;- paste0(&amp;quot;team1_&amp;quot;, &amp;quot;valence_&amp;quot;, 1:n_fans)
  team1_valence[team1_valence &amp;gt; 6] &amp;lt;- 6
  team1_valence[team1_valence &amp;lt; 0] &amp;lt;- 0
  
  for(i in 1:n_fans) {
    team1_arousal[[i]] &amp;lt;- ksmooth(x = score_board$second,
                                  y = rnorm(n = 1, mean = 3, sd = .25) + (arima.sim(list(order = c(1, 0, 1), ma = -.1, ar = c(.9)), n = total_time) * .25) + arousal(time = score_board$second,
                                  difference = score_board$difference,
                                  recent_score = lag(score_board$difference, n = sample(c(1:3), 1), default = 0) - lag(score_board$difference, n = sample(c(4:6), 1), default = 0),
                                  end_game = score_board$end_game),
                                  bandwidth = total_time/100, kernel = &amp;quot;normal&amp;quot;)$y
  }
  
  colnames(team1_arousal) &amp;lt;- paste0(&amp;quot;team1_&amp;quot;, &amp;quot;arousal_&amp;quot;, 1:n_fans)
  
  team1_arousal[team1_arousal &amp;gt; 6] &amp;lt;- 6
  team1_arousal[team1_arousal &amp;lt; 0] &amp;lt;- 0
   
  for(i in 1:n_fans) {
    team2_valence[[i]] &amp;lt;- ksmooth(x = score_board$second,
                                  y = rnorm(n = 1, mean = 3, sd = .25) + (arima.sim(list(order = c(1, 0, 1), ma = -.1, ar = c(.9)), n = total_time) * .25) + valence(time = score_board$second,
                                  overall_difference = -score_board$difference,
                                  recent_team_score = lag(score_board$team2_score, n = sample(c(1:5), 1), default = 0) - lag(score_board$team2_score, n = sample(c(6:8), 1), default = 0),
                                  recent_enemy_score = lag(score_board$team1_score, n = sample(c(1:5), 1), default = 0) - lag(score_board$team1_score, n = sample(c(6:8), 1), default = 0)),
                                  bandwidth = total_time/100, kernel = &amp;quot;normal&amp;quot;)$y
  }
  
  colnames(team2_valence) &amp;lt;- paste0(&amp;quot;team2_&amp;quot;, &amp;quot;valence_&amp;quot;, 1:n_fans)
  
  team2_valence[team2_valence &amp;gt; 6] &amp;lt;- 6
  team2_valence[team2_valence &amp;lt; 0] &amp;lt;- 0
  
  for (i in 1:n_fans) {
    team2_arousal[[i]] &amp;lt;- ksmooth(x = score_board$second,
                                  y = rnorm(n = 1, mean = 3, sd = .25) + (arima.sim(list(order = c(1, 0, 1), ma = -.1, ar = c(.9)), n = total_time) * .25) + arousal(time = score_board$second,
                                  difference = score_board$difference,
                                  recent_score = lag(score_board$difference, n = sample(c(1:3), 1), default = 0) - lag(score_board$difference, n = sample(c(4:6), 1), default = 0),
                                  end_game = score_board$end_game),
                                  bandwidth = total_time/100, kernel = &amp;quot;normal&amp;quot;)$y
  }
  
  colnames(team2_arousal) &amp;lt;- paste0(&amp;quot;team2_&amp;quot;, &amp;quot;arousal_&amp;quot;, 1:n_fans)
  
  team2_arousal[team2_arousal &amp;gt; 6] &amp;lt;- 6
  team2_arousal[team2_arousal &amp;lt; 0] &amp;lt;- 0
  
  game_affect &amp;lt;- cbind(score_board, team1_valence, team1_arousal, team2_valence, team2_arousal)
  
  return(game_affect)
}

set.seed(060519)

game &amp;lt;- game_simulation(n_seconds = 1000, n_fans = 25)

game_long &amp;lt;- game %&amp;gt;%
  mutate(instance = row_number(),
         team1_score = team1_score * 2,
         team2_score = team2_score * 2) %&amp;gt;%
  select(instance, everything()) %&amp;gt;%
  select(-end_game) %&amp;gt;%
  gather(key = &amp;quot;team&amp;quot;, value = &amp;quot;score&amp;quot;, c(team1_valence_1:length(game))) %&amp;gt;%
  separate(col = team, into = c(&amp;quot;team&amp;quot;, &amp;quot;dimension&amp;quot;, &amp;quot;fan&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  spread(dimension, score)
  
p &amp;lt;- game_long %&amp;gt;%
  ggplot() +
  annotate(&amp;quot;rect&amp;quot;, xmin = 0, xmax = 3, ymin = 0, ymax = 3, alpha = 0.5, fill = &amp;quot;lightblue&amp;quot;) +
  annotate(&amp;quot;rect&amp;quot;, xmin = 3, xmax = 6, ymin = 0, ymax = 3, alpha = 0.5, fill = &amp;quot;lightgreen&amp;quot;) +
  annotate(&amp;quot;rect&amp;quot;, xmin = 0, xmax = 3, ymin = 3, ymax = 6, alpha = 0.2, fill = &amp;quot;red&amp;quot;) +
  annotate(&amp;quot;rect&amp;quot;, xmin = 3, xmax = 6, ymin = 3, ymax = 6, alpha = 0.2, fill = &amp;quot;yellow&amp;quot;) +
  annotate(&amp;quot;text&amp;quot;, x = c(1.5, 4.5, 1.5, 4.5), y = c(1.5, 1.5, 4.5, 4.5), label = c(&amp;quot;Disappointed&amp;quot;, &amp;quot;Content&amp;quot;, &amp;quot;Nervous&amp;quot;, &amp;quot;Excited&amp;quot;),
           size = 10, alpha = .50, family = &amp;quot;Verdana&amp;quot;) +
  geom_text(aes(x = 1.5, y = 5.5, label = paste(&amp;quot;Red:&amp;quot;, team1_score)), size = 8, color = &amp;quot;red&amp;quot;, family = &amp;quot;Verdana&amp;quot;) +
  geom_text(aes(x = 4.5, y = 5.5, label = paste(&amp;quot;Blue:&amp;quot;, team2_score)), size = 8, color = &amp;quot;blue&amp;quot;, family = &amp;quot;Verdana&amp;quot;) +
  geom_point(aes(x = valence, y = arousal, color = team), alpha = .70, size = 4) +
  scale_x_continuous(limits = c(0, 6)) +
  scale_y_continuous(limits = c(0, 6)) +
  scale_color_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;)) +
  labs(title = &amp;#39;Time: {round((max(game_long$second) - frame_time)/60, 2)}&amp;#39;,
       x = &amp;quot;Happiness&amp;quot;,
       y = &amp;quot;Nerves&amp;quot;) +
  theme_minimal(base_size = 16) +
  transition_time(second) +
  theme(legend.position = &amp;quot;none&amp;quot;,
        axis.title.x = element_text(size = 18),
        axis.title.y = element_text(size = 18),
        plot.title = element_text(size = 18, hjust = .5),
        text = element_text(family = &amp;quot;Verdana&amp;quot;))

game1 &amp;lt;- animate(p, nframes = 1062, width = 500, height = 500, fps = 14, end_pause = 12)

game1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Kuppens P, Verduyn P. (2017). Emotion dynamics. &lt;em&gt;Current Opinion in Psychology. 17&lt;/em&gt;, 22–26. doi: 10.1016/j.copsyc.2017.06.004.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Seeking more solitude: Conceptualization, assessment, and implications of aloneliness</title>
      <link>/publication/coplan-et-al.-2019/</link>
      <pubDate>Sun, 26 May 2019 00:00:00 -0400</pubDate>
      
      <guid>/publication/coplan-et-al.-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quick Example of Latent Profile Analysis in R</title>
      <link>/post/latent-profile/latent-profile/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/latent-profile/latent-profile/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/crosstalk/css/crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/plotly-main/plotly-latest.min.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Latent Profile Analysis (LPA) tries to identify clusters of individuals (i.e., latent profiles) based on responses to a series of continuous variables (i.e., indicators). LPA assumes that there are unobserved latent profiles that generate patterns of responses on indicator items.&lt;/p&gt;
&lt;p&gt;Here, I will go through a quick example of LPA to identify groups of people based on their interests/hobbies. The data comes from the &lt;a href=&#34;https://www.kaggle.com/miroslavsabo/young-people-survey&#34;&gt;Young People Survey&lt;/a&gt;, available freely on Kaggle.com.&lt;/p&gt;
&lt;p&gt;Here’s a sneak peek at what we’re going for:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/latent-profile/Latent_Profile_Interests_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Terminology note: People use the terms &lt;em&gt;clusters&lt;/em&gt;, &lt;em&gt;profiles&lt;/em&gt;, &lt;em&gt;classes&lt;/em&gt;, and &lt;em&gt;groups&lt;/em&gt; interchangeably, but there are subtle differences. I’ll mostly stick to &lt;em&gt;profile&lt;/em&gt; to refer to a grouping of cases, in keeping with LPA terminology. We should note that LPA is a branch of &lt;em&gt;Gaussian Finite Mixture Modeling&lt;/em&gt;, which includes Latent Class Analysis (LCA). The difference between LPA and LCA is conceptual, not computational: LPA uses continuous indicators and LCA uses binary indicators. LPA is a probabilistic model, which means that it models the probability of case belonging to a profile. This is superior to an approach like &lt;em&gt;K-means&lt;/em&gt; that uses distance algorithms.&lt;/p&gt;
&lt;p&gt;With that aside, let’s load in the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;survey &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/whipson/tidytuesday/master/young_people.csv&amp;quot;) %&amp;gt;%
  select(History:Pets)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is on 32 interests/hobbies. Each item is ranked 1 (not interested) to 5 (very interested).&lt;/p&gt;
&lt;p&gt;The description on Kaggle suggests there may be careless responding (e.g., participants who selected the same value over and over). We can use the &lt;em&gt;careless&lt;/em&gt; package to identify “string responding”. Let’s also look for multivariate outliers with Mahalanobis Distance (see my &lt;a href=&#34;https://willhipson.netlify.com/post/outliers/outliers/&#34;&gt;previous post&lt;/a&gt; on Mahalanobis for identifying outliers).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(careless)
library(psych)

interests &amp;lt;- survey %&amp;gt;%
  mutate(string = longstring(.)) %&amp;gt;%
  mutate(md = outlier(., plot = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll cap string responding to a maximum of 10 and use a Mahalanobis D cutoff of alpha = .001.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cutoff &amp;lt;- (qchisq(p = 1 - .001, df = ncol(interests)))

interests_clean &amp;lt;- interests %&amp;gt;%
  filter(string &amp;lt;= 10,
         md &amp;lt; cutoff) %&amp;gt;%
  select(-string, -md)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The package &lt;em&gt;mclust&lt;/em&gt; performs various types of model-based clustering and dimension reduction. Plus, it’s really intuitive to use. It requires complete data (no missing), so for this example we’ll remove cases with NAs. This is not the preferred approach; we’d be better off imputing. But for illustrative purposes, this works fine. I’m also going to standardize all of the indicators so when we plot the profiles it’s clearer to see the differences between clusters. Running this code will take a few minutes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mclust)

interests_clustering &amp;lt;- interests_clean %&amp;gt;%
  na.omit() %&amp;gt;%
  mutate_all(list(scale))

BIC &amp;lt;- mclustBIC(interests_clustering)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll start by plotting Bayesian Information Criteria for all the models with profiles ranging from 1 to 9.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(BIC)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/latent-profile/Latent_Profile_Interests_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s not immediately clear which model is the best since the y-axis is so large and many of the models score close together. summary(BIC) shows the top three models based on BIC.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(BIC)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Best BIC values:
##             VVE,3       VEE,3      EVE,3
## BIC      -75042.7 -75165.1484 -75179.165
## BIC diff      0.0   -122.4442   -136.461&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The highest BIC comes from VVE, 3. This says there are 3 clusters with variable volume, variable shape, equal orientation, and ellipsodial distribution (see Figure 2 from &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5096736/&#34;&gt;this paper&lt;/a&gt; for a visual). However, VEE, 3 is not far behind and actually may be a more theoretically useful model since it constrains the shape of the distribution to be equal. For this reason, we’ll go with VEE, 3.&lt;/p&gt;
&lt;p&gt;If we want to look at this model more closely, we save it as an object and inspect it with &lt;em&gt;summary()&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- Mclust(interests_clustering, modelNames = &amp;quot;VEE&amp;quot;, G = 3, x = BIC)

summary(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust VEE (ellipsoidal, equal shape and orientation) model with 3
## components: 
## 
##  log-likelihood   n  df       BIC       ICL
##       -35455.83 874 628 -75165.15 -75216.14
## 
## Clustering table:
##   1   2   3 
## 137 527 210&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output describes the geometric characteristics of the profiles and the number of cases classified into each of the three clusters.&lt;/p&gt;
&lt;p&gt;BIC is one of the best fit indices, but it’s always recommended to look for more evidence that the solution we’ve chosen is the correct one. We can also compare values of the Integrated Completed Likelikood (ICL) criterion. See &lt;a href=&#34;https://arxiv.org/pdf/1411.4257.pdf&#34;&gt;this paper&lt;/a&gt; for more details. ICL isn’t much different from BIC, except that it adds a penalty on solutions with greater &lt;em&gt;entropy&lt;/em&gt; or classification uncertainty.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ICL &amp;lt;- mclustICL(interests_clustering)

plot(ICL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/latent-profile/Latent_Profile_Interests_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(ICL)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Best ICL values:
##              VVE,3        VEE,3      EVE,3
## ICL      -75134.69 -75216.13551 -75272.891
## ICL diff      0.00    -81.44795   -138.203&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see similar results. ICL suggests that model VEE, 3 fits quite well. Finally, we’ll perform the Bootstrap Likelihood Ratio Test (BLRT) which compares model fit between &lt;em&gt;k&lt;/em&gt;-1 and &lt;em&gt;k&lt;/em&gt; cluster models. In other words, it looks to see if an increase in profiles increases fit. Based on simulations by &lt;a href=&#34;https://www.statmodel.com/download/LCA_tech11_nylund_v83.pdf&#34;&gt;Nylund, Asparouhov, and Muthén (2007)&lt;/a&gt; BIC and BLRT are the best indicators for how many profiles there are. This line of code will take a long time to run, so if you’re just following along I suggest skipping it unless you want to step out for a coffee break.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mclustBootstrapLRT(interests_clustering, modelName = &amp;quot;VEE&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ------------------------------------------------------------- 
## Bootstrap sequential LRT for the number of mixture components 
## ------------------------------------------------------------- 
## Model        = VEE 
## Replications = 999 
##               LRTS bootstrap p-value
## 1 vs 2    197.0384             0.001
## 2 vs 3    684.8743             0.001
## 3 vs 4   -124.1935             1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;BLRT also suggests that a 3-profile solution is ideal.&lt;/p&gt;
&lt;p&gt;##Visualizing LPA&lt;/p&gt;
&lt;p&gt;Now that we’re confident in our choice of a 3-profile solution, let’s plot the results. Specifically, we want to see how the profiles differ on the indicators, that is, the items that made up the profiles. If the solution is theoretically meaningful, we should see differences that make sense.&lt;/p&gt;
&lt;p&gt;First, we’ll extract the means for each profile (remember, we chose these to be standardized). Then, we &lt;em&gt;melt&lt;/em&gt; this into long form. Note that I’m trimming values exceeding +1 SD, otherwise we run into plotting issues.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)

means &amp;lt;- data.frame(mod1$parameters$mean, stringsAsFactors = FALSE) %&amp;gt;%
  rownames_to_column() %&amp;gt;%
  rename(Interest = rowname) %&amp;gt;%
  melt(id.vars = &amp;quot;Interest&amp;quot;, variable.name = &amp;quot;Profile&amp;quot;, value.name = &amp;quot;Mean&amp;quot;) %&amp;gt;%
  mutate(Mean = round(Mean, 2),
         Mean = ifelse(Mean &amp;gt; 1, 1, Mean))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the code for the plot. I’m reordering the indicators so that similar activities are close together.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;means %&amp;gt;%
  ggplot(aes(Interest, Mean, group = Profile, color = Profile)) +
  geom_point(size = 2.25) +
  geom_line(size = 1.25) +
  scale_x_discrete(limits = c(&amp;quot;Active sport&amp;quot;, &amp;quot;Adrenaline sports&amp;quot;, &amp;quot;Passive sport&amp;quot;,
                              &amp;quot;Countryside, outdoors&amp;quot;, &amp;quot;Gardening&amp;quot;, &amp;quot;Cars&amp;quot;,
                              &amp;quot;Art exhibitions&amp;quot;, &amp;quot;Dancing&amp;quot;, &amp;quot;Musical instruments&amp;quot;, &amp;quot;Theatre&amp;quot;, &amp;quot;Writing&amp;quot;, &amp;quot;Reading&amp;quot;,
                              &amp;quot;Geography&amp;quot;, &amp;quot;History&amp;quot;, &amp;quot;Law&amp;quot;, &amp;quot;Politics&amp;quot;, &amp;quot;Psychology&amp;quot;, &amp;quot;Religion&amp;quot;, &amp;quot;Foreign languages&amp;quot;,
                              &amp;quot;Biology&amp;quot;, &amp;quot;Chemistry&amp;quot;, &amp;quot;Mathematics&amp;quot;, &amp;quot;Medicine&amp;quot;, &amp;quot;Physics&amp;quot;, &amp;quot;Science and technology&amp;quot;,
                              &amp;quot;Internet&amp;quot;, &amp;quot;PC&amp;quot;,
                              &amp;quot;Celebrities&amp;quot;, &amp;quot;Economy Management&amp;quot;, &amp;quot;Fun with friends&amp;quot;, &amp;quot;Shopping&amp;quot;, &amp;quot;Pets&amp;quot;)) +
  labs(x = NULL, y = &amp;quot;Standardized mean interest&amp;quot;) +
  theme_bw(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/latent-profile/Latent_Profile_Interests_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have a lot of indicators (more than typical for LPA), but we see some interesting differences. Clearly the red group is interested in science and the blue group shows greater interest in arts and humanities. The green group seems disinterested in both science and art, but moderately interested in other things.&lt;/p&gt;
&lt;p&gt;We can make this plot more informative by plugging in profile names and proportions. I’m also going to save this plot as an object so that we can do something really cool with it!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- means %&amp;gt;%
  mutate(Profile = recode(Profile, 
                          X1 = &amp;quot;Science: 16%&amp;quot;,
                          X2 = &amp;quot;Disinterest: 60%&amp;quot;,
                          X3 = &amp;quot;Arts &amp;amp; Humanities: 24%&amp;quot;)) %&amp;gt;%
  ggplot(aes(Interest, Mean, group = Profile, color = Profile)) +
  geom_point(size = 2.25) +
  geom_line(size = 1.25) +
  scale_x_discrete(limits = c(&amp;quot;Active sport&amp;quot;, &amp;quot;Adrenaline sports&amp;quot;, &amp;quot;Passive sport&amp;quot;,
                              &amp;quot;Countryside, outdoors&amp;quot;, &amp;quot;Gardening&amp;quot;, &amp;quot;Cars&amp;quot;,
                              &amp;quot;Art exhibitions&amp;quot;, &amp;quot;Dancing&amp;quot;, &amp;quot;Musical instruments&amp;quot;, &amp;quot;Theatre&amp;quot;, &amp;quot;Writing&amp;quot;, &amp;quot;Reading&amp;quot;,
                              &amp;quot;Geography&amp;quot;, &amp;quot;History&amp;quot;, &amp;quot;Law&amp;quot;, &amp;quot;Politics&amp;quot;, &amp;quot;Psychology&amp;quot;, &amp;quot;Religion&amp;quot;, &amp;quot;Foreign languages&amp;quot;,
                              &amp;quot;Biology&amp;quot;, &amp;quot;Chemistry&amp;quot;, &amp;quot;Mathematics&amp;quot;, &amp;quot;Medicine&amp;quot;, &amp;quot;Physics&amp;quot;, &amp;quot;Science and technology&amp;quot;,
                              &amp;quot;Internet&amp;quot;, &amp;quot;PC&amp;quot;,
                              &amp;quot;Celebrities&amp;quot;, &amp;quot;Economy Management&amp;quot;, &amp;quot;Fun with friends&amp;quot;, &amp;quot;Shopping&amp;quot;, &amp;quot;Pets&amp;quot;)) +
  labs(x = NULL, y = &amp;quot;Standardized mean interest&amp;quot;) +
  theme_bw(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &amp;quot;top&amp;quot;)

p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/latent-profile/Latent_Profile_Interests_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The something &lt;em&gt;really cool&lt;/em&gt; that I want to do is make an interactive plot. Why would I want to do this? Well, one of the problems with the static plot is that with so many indicators it’s tough to read the values for each indicator. An interactive plot lets the reader narrow in on specific indicators or profiles of interest. We’ll use &lt;em&gt;plotly&lt;/em&gt; to turn our static plot into an interactive one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plotly)

ggplotly(p, tooltip = c(&amp;quot;Interest&amp;quot;, &amp;quot;Mean&amp;quot;)) %&amp;gt;%
  layout(legend = list(orientation = &amp;quot;h&amp;quot;, y = 1.2))&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;data&#34;:[{&#34;x&#34;:[14,17,16,22,24,26,27,29,20,21,12,13,19,23,15,6,7,18,4,8,9,11,3,1,5,28,31,25,10,30,2,32],&#34;y&#34;:[-0.11,-0.13,-0.21,0.03,0.25,-0.14,-0.18,-0.48,1,1,0.18,-0.1,-0.1,1,-0.13,-0.09,-0.12,0.05,0.06,0.16,0.01,-0.41,-0.02,-0.01,0.28,-0.03,0.02,0.26,0.02,0.12,-0.06,0.28],&#34;text&#34;:[&#34;Interest: History&lt;br /&gt;Mean: -0.11&#34;,&#34;Interest: Psychology&lt;br /&gt;Mean: -0.13&#34;,&#34;Interest: Politics&lt;br /&gt;Mean: -0.21&#34;,&#34;Interest: Mathematics&lt;br /&gt;Mean:  0.03&#34;,&#34;Interest: Physics&lt;br /&gt;Mean:  0.25&#34;,&#34;Interest: Internet&lt;br /&gt;Mean: -0.14&#34;,&#34;Interest: PC&lt;br /&gt;Mean: -0.18&#34;,&#34;Interest: Economy Management&lt;br /&gt;Mean: -0.48&#34;,&#34;Interest: Biology&lt;br /&gt;Mean:  1.00&#34;,&#34;Interest: Chemistry&lt;br /&gt;Mean:  1.00&#34;,&#34;Interest: Reading&lt;br /&gt;Mean:  0.18&#34;,&#34;Interest: Geography&lt;br /&gt;Mean: -0.10&#34;,&#34;Interest: Foreign languages&lt;br /&gt;Mean: -0.10&#34;,&#34;Interest: Medicine&lt;br /&gt;Mean:  1.00&#34;,&#34;Interest: Law&lt;br /&gt;Mean: -0.13&#34;,&#34;Interest: Cars&lt;br /&gt;Mean: -0.09&#34;,&#34;Interest: Art exhibitions&lt;br /&gt;Mean: -0.12&#34;,&#34;Interest: Religion&lt;br /&gt;Mean:  0.05&#34;,&#34;Interest: Countryside, outdoors&lt;br /&gt;Mean:  0.06&#34;,&#34;Interest: Dancing&lt;br /&gt;Mean:  0.16&#34;,&#34;Interest: Musical instruments&lt;br /&gt;Mean:  0.01&#34;,&#34;Interest: Writing&lt;br /&gt;Mean: -0.41&#34;,&#34;Interest: Passive sport&lt;br /&gt;Mean: -0.02&#34;,&#34;Interest: Active sport&lt;br /&gt;Mean: -0.01&#34;,&#34;Interest: Gardening&lt;br /&gt;Mean:  0.28&#34;,&#34;Interest: Celebrities&lt;br /&gt;Mean: -0.03&#34;,&#34;Interest: Shopping&lt;br /&gt;Mean:  0.02&#34;,&#34;Interest: Science and technology&lt;br /&gt;Mean:  0.26&#34;,&#34;Interest: Theatre&lt;br /&gt;Mean:  0.02&#34;,&#34;Interest: Fun with friends&lt;br /&gt;Mean:  0.12&#34;,&#34;Interest: Adrenaline sports&lt;br /&gt;Mean: -0.06&#34;,&#34;Interest: Pets&lt;br /&gt;Mean:  0.28&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;markers&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(248,118,109,1)&#34;,&#34;opacity&#34;:1,&#34;size&#34;:8.50393700787402,&#34;symbol&#34;:&#34;circle&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(248,118,109,1)&#34;}},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Science: 16%&#34;,&#34;legendgroup&#34;:&#34;Science: 16%&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[14,17,16,22,24,26,27,29,20,21,12,13,19,23,15,6,7,18,4,8,9,11,3,1,5,28,31,25,10,30,2,32],&#34;y&#34;:[-0.08,-0.13,-0.01,0,-0.1,0.08,0.08,0.15,-0.35,-0.42,-0.23,-0.06,-0.06,-0.37,-0.02,0.14,-0.18,-0.11,-0.03,-0.12,-0.18,-0.52,0.07,0,-0.19,0.02,-0.01,-0.07,-0.15,0.02,0,-0.08],&#34;text&#34;:[&#34;Interest: History&lt;br /&gt;Mean: -0.08&#34;,&#34;Interest: Psychology&lt;br /&gt;Mean: -0.13&#34;,&#34;Interest: Politics&lt;br /&gt;Mean: -0.01&#34;,&#34;Interest: Mathematics&lt;br /&gt;Mean:  0.00&#34;,&#34;Interest: Physics&lt;br /&gt;Mean: -0.10&#34;,&#34;Interest: Internet&lt;br /&gt;Mean:  0.08&#34;,&#34;Interest: PC&lt;br /&gt;Mean:  0.08&#34;,&#34;Interest: Economy Management&lt;br /&gt;Mean:  0.15&#34;,&#34;Interest: Biology&lt;br /&gt;Mean: -0.35&#34;,&#34;Interest: Chemistry&lt;br /&gt;Mean: -0.42&#34;,&#34;Interest: Reading&lt;br /&gt;Mean: -0.23&#34;,&#34;Interest: Geography&lt;br /&gt;Mean: -0.06&#34;,&#34;Interest: Foreign languages&lt;br /&gt;Mean: -0.06&#34;,&#34;Interest: Medicine&lt;br /&gt;Mean: -0.37&#34;,&#34;Interest: Law&lt;br /&gt;Mean: -0.02&#34;,&#34;Interest: Cars&lt;br /&gt;Mean:  0.14&#34;,&#34;Interest: Art exhibitions&lt;br /&gt;Mean: -0.18&#34;,&#34;Interest: Religion&lt;br /&gt;Mean: -0.11&#34;,&#34;Interest: Countryside, outdoors&lt;br /&gt;Mean: -0.03&#34;,&#34;Interest: Dancing&lt;br /&gt;Mean: -0.12&#34;,&#34;Interest: Musical instruments&lt;br /&gt;Mean: -0.18&#34;,&#34;Interest: Writing&lt;br /&gt;Mean: -0.52&#34;,&#34;Interest: Passive sport&lt;br /&gt;Mean:  0.07&#34;,&#34;Interest: Active sport&lt;br /&gt;Mean:  0.00&#34;,&#34;Interest: Gardening&lt;br /&gt;Mean: -0.19&#34;,&#34;Interest: Celebrities&lt;br /&gt;Mean:  0.02&#34;,&#34;Interest: Shopping&lt;br /&gt;Mean: -0.01&#34;,&#34;Interest: Science and technology&lt;br /&gt;Mean: -0.07&#34;,&#34;Interest: Theatre&lt;br /&gt;Mean: -0.15&#34;,&#34;Interest: Fun with friends&lt;br /&gt;Mean:  0.02&#34;,&#34;Interest: Adrenaline sports&lt;br /&gt;Mean:  0.00&#34;,&#34;Interest: Pets&lt;br /&gt;Mean: -0.08&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;markers&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(0,186,56,1)&#34;,&#34;opacity&#34;:1,&#34;size&#34;:8.50393700787402,&#34;symbol&#34;:&#34;circle&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(0,186,56,1)&#34;}},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Disinterest: 60%&#34;,&#34;legendgroup&#34;:&#34;Disinterest: 60%&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[14,17,16,22,24,26,27,29,20,21,12,13,19,23,15,6,7,18,4,8,9,11,3,1,5,28,31,25,10,30,2,32],&#34;y&#34;:[0.28,0.43,0.15,-0.03,0.08,-0.11,-0.09,-0.06,0.01,-0.08,0.47,0.23,0.23,0.06,0.14,-0.3,0.53,0.25,0.03,0.21,0.47,1,-0.18,0,0.29,-0.03,0.01,0,0.36,-0.13,0.04,0.01],&#34;text&#34;:[&#34;Interest: History&lt;br /&gt;Mean:  0.28&#34;,&#34;Interest: Psychology&lt;br /&gt;Mean:  0.43&#34;,&#34;Interest: Politics&lt;br /&gt;Mean:  0.15&#34;,&#34;Interest: Mathematics&lt;br /&gt;Mean: -0.03&#34;,&#34;Interest: Physics&lt;br /&gt;Mean:  0.08&#34;,&#34;Interest: Internet&lt;br /&gt;Mean: -0.11&#34;,&#34;Interest: PC&lt;br /&gt;Mean: -0.09&#34;,&#34;Interest: Economy Management&lt;br /&gt;Mean: -0.06&#34;,&#34;Interest: Biology&lt;br /&gt;Mean:  0.01&#34;,&#34;Interest: Chemistry&lt;br /&gt;Mean: -0.08&#34;,&#34;Interest: Reading&lt;br /&gt;Mean:  0.47&#34;,&#34;Interest: Geography&lt;br /&gt;Mean:  0.23&#34;,&#34;Interest: Foreign languages&lt;br /&gt;Mean:  0.23&#34;,&#34;Interest: Medicine&lt;br /&gt;Mean:  0.06&#34;,&#34;Interest: Law&lt;br /&gt;Mean:  0.14&#34;,&#34;Interest: Cars&lt;br /&gt;Mean: -0.30&#34;,&#34;Interest: Art exhibitions&lt;br /&gt;Mean:  0.53&#34;,&#34;Interest: Religion&lt;br /&gt;Mean:  0.25&#34;,&#34;Interest: Countryside, outdoors&lt;br /&gt;Mean:  0.03&#34;,&#34;Interest: Dancing&lt;br /&gt;Mean:  0.21&#34;,&#34;Interest: Musical instruments&lt;br /&gt;Mean:  0.47&#34;,&#34;Interest: Writing&lt;br /&gt;Mean:  1.00&#34;,&#34;Interest: Passive sport&lt;br /&gt;Mean: -0.18&#34;,&#34;Interest: Active sport&lt;br /&gt;Mean:  0.00&#34;,&#34;Interest: Gardening&lt;br /&gt;Mean:  0.29&#34;,&#34;Interest: Celebrities&lt;br /&gt;Mean: -0.03&#34;,&#34;Interest: Shopping&lt;br /&gt;Mean:  0.01&#34;,&#34;Interest: Science and technology&lt;br /&gt;Mean:  0.00&#34;,&#34;Interest: Theatre&lt;br /&gt;Mean:  0.36&#34;,&#34;Interest: Fun with friends&lt;br /&gt;Mean: -0.13&#34;,&#34;Interest: Adrenaline sports&lt;br /&gt;Mean:  0.04&#34;,&#34;Interest: Pets&lt;br /&gt;Mean:  0.01&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;markers&#34;,&#34;marker&#34;:{&#34;autocolorscale&#34;:false,&#34;color&#34;:&#34;rgba(97,156,255,1)&#34;,&#34;opacity&#34;:1,&#34;size&#34;:8.50393700787402,&#34;symbol&#34;:&#34;circle&#34;,&#34;line&#34;:{&#34;width&#34;:1.88976377952756,&#34;color&#34;:&#34;rgba(97,156,255,1)&#34;}},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Arts &amp; Humanities: 24%&#34;,&#34;legendgroup&#34;:&#34;Arts &amp; Humanities: 24%&#34;,&#34;showlegend&#34;:true,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],&#34;y&#34;:[-0.01,-0.06,-0.02,0.06,0.28,-0.09,-0.12,0.16,0.01,0.02,-0.41,0.18,-0.1,-0.11,-0.13,-0.21,-0.13,0.05,-0.1,1,1,0.03,1,0.25,0.26,-0.14,-0.18,-0.03,-0.48,0.12,0.02,0.28],&#34;text&#34;:[&#34;Interest: Active sport&lt;br /&gt;Mean: -0.01&#34;,&#34;Interest: Adrenaline sports&lt;br /&gt;Mean: -0.06&#34;,&#34;Interest: Passive sport&lt;br /&gt;Mean: -0.02&#34;,&#34;Interest: Countryside, outdoors&lt;br /&gt;Mean:  0.06&#34;,&#34;Interest: Gardening&lt;br /&gt;Mean:  0.28&#34;,&#34;Interest: Cars&lt;br /&gt;Mean: -0.09&#34;,&#34;Interest: Art exhibitions&lt;br /&gt;Mean: -0.12&#34;,&#34;Interest: Dancing&lt;br /&gt;Mean:  0.16&#34;,&#34;Interest: Musical instruments&lt;br /&gt;Mean:  0.01&#34;,&#34;Interest: Theatre&lt;br /&gt;Mean:  0.02&#34;,&#34;Interest: Writing&lt;br /&gt;Mean: -0.41&#34;,&#34;Interest: Reading&lt;br /&gt;Mean:  0.18&#34;,&#34;Interest: Geography&lt;br /&gt;Mean: -0.10&#34;,&#34;Interest: History&lt;br /&gt;Mean: -0.11&#34;,&#34;Interest: Law&lt;br /&gt;Mean: -0.13&#34;,&#34;Interest: Politics&lt;br /&gt;Mean: -0.21&#34;,&#34;Interest: Psychology&lt;br /&gt;Mean: -0.13&#34;,&#34;Interest: Religion&lt;br /&gt;Mean:  0.05&#34;,&#34;Interest: Foreign languages&lt;br /&gt;Mean: -0.10&#34;,&#34;Interest: Biology&lt;br /&gt;Mean:  1.00&#34;,&#34;Interest: Chemistry&lt;br /&gt;Mean:  1.00&#34;,&#34;Interest: Mathematics&lt;br /&gt;Mean:  0.03&#34;,&#34;Interest: Medicine&lt;br /&gt;Mean:  1.00&#34;,&#34;Interest: Physics&lt;br /&gt;Mean:  0.25&#34;,&#34;Interest: Science and technology&lt;br /&gt;Mean:  0.26&#34;,&#34;Interest: Internet&lt;br /&gt;Mean: -0.14&#34;,&#34;Interest: PC&lt;br /&gt;Mean: -0.18&#34;,&#34;Interest: Celebrities&lt;br /&gt;Mean: -0.03&#34;,&#34;Interest: Economy Management&lt;br /&gt;Mean: -0.48&#34;,&#34;Interest: Fun with friends&lt;br /&gt;Mean:  0.12&#34;,&#34;Interest: Shopping&lt;br /&gt;Mean:  0.02&#34;,&#34;Interest: Pets&lt;br /&gt;Mean:  0.28&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:4.7244094488189,&#34;color&#34;:&#34;rgba(248,118,109,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Science: 16%&#34;,&#34;legendgroup&#34;:&#34;Science: 16%&#34;,&#34;showlegend&#34;:false,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],&#34;y&#34;:[0,0,0.07,-0.03,-0.19,0.14,-0.18,-0.12,-0.18,-0.15,-0.52,-0.23,-0.06,-0.08,-0.02,-0.01,-0.13,-0.11,-0.06,-0.35,-0.42,0,-0.37,-0.1,-0.07,0.08,0.08,0.02,0.15,0.02,-0.01,-0.08],&#34;text&#34;:[&#34;Interest: Active sport&lt;br /&gt;Mean:  0.00&#34;,&#34;Interest: Adrenaline sports&lt;br /&gt;Mean:  0.00&#34;,&#34;Interest: Passive sport&lt;br /&gt;Mean:  0.07&#34;,&#34;Interest: Countryside, outdoors&lt;br /&gt;Mean: -0.03&#34;,&#34;Interest: Gardening&lt;br /&gt;Mean: -0.19&#34;,&#34;Interest: Cars&lt;br /&gt;Mean:  0.14&#34;,&#34;Interest: Art exhibitions&lt;br /&gt;Mean: -0.18&#34;,&#34;Interest: Dancing&lt;br /&gt;Mean: -0.12&#34;,&#34;Interest: Musical instruments&lt;br /&gt;Mean: -0.18&#34;,&#34;Interest: Theatre&lt;br /&gt;Mean: -0.15&#34;,&#34;Interest: Writing&lt;br /&gt;Mean: -0.52&#34;,&#34;Interest: Reading&lt;br /&gt;Mean: -0.23&#34;,&#34;Interest: Geography&lt;br /&gt;Mean: -0.06&#34;,&#34;Interest: History&lt;br /&gt;Mean: -0.08&#34;,&#34;Interest: Law&lt;br /&gt;Mean: -0.02&#34;,&#34;Interest: Politics&lt;br /&gt;Mean: -0.01&#34;,&#34;Interest: Psychology&lt;br /&gt;Mean: -0.13&#34;,&#34;Interest: Religion&lt;br /&gt;Mean: -0.11&#34;,&#34;Interest: Foreign languages&lt;br /&gt;Mean: -0.06&#34;,&#34;Interest: Biology&lt;br /&gt;Mean: -0.35&#34;,&#34;Interest: Chemistry&lt;br /&gt;Mean: -0.42&#34;,&#34;Interest: Mathematics&lt;br /&gt;Mean:  0.00&#34;,&#34;Interest: Medicine&lt;br /&gt;Mean: -0.37&#34;,&#34;Interest: Physics&lt;br /&gt;Mean: -0.10&#34;,&#34;Interest: Science and technology&lt;br /&gt;Mean: -0.07&#34;,&#34;Interest: Internet&lt;br /&gt;Mean:  0.08&#34;,&#34;Interest: PC&lt;br /&gt;Mean:  0.08&#34;,&#34;Interest: Celebrities&lt;br /&gt;Mean:  0.02&#34;,&#34;Interest: Economy Management&lt;br /&gt;Mean:  0.15&#34;,&#34;Interest: Fun with friends&lt;br /&gt;Mean:  0.02&#34;,&#34;Interest: Shopping&lt;br /&gt;Mean: -0.01&#34;,&#34;Interest: Pets&lt;br /&gt;Mean: -0.08&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:4.7244094488189,&#34;color&#34;:&#34;rgba(0,186,56,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Disinterest: 60%&#34;,&#34;legendgroup&#34;:&#34;Disinterest: 60%&#34;,&#34;showlegend&#34;:false,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null},{&#34;x&#34;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],&#34;y&#34;:[0,0.04,-0.18,0.03,0.29,-0.3,0.53,0.21,0.47,0.36,1,0.47,0.23,0.28,0.14,0.15,0.43,0.25,0.23,0.01,-0.08,-0.03,0.06,0.08,0,-0.11,-0.09,-0.03,-0.06,-0.13,0.01,0.01],&#34;text&#34;:[&#34;Interest: Active sport&lt;br /&gt;Mean:  0.00&#34;,&#34;Interest: Adrenaline sports&lt;br /&gt;Mean:  0.04&#34;,&#34;Interest: Passive sport&lt;br /&gt;Mean: -0.18&#34;,&#34;Interest: Countryside, outdoors&lt;br /&gt;Mean:  0.03&#34;,&#34;Interest: Gardening&lt;br /&gt;Mean:  0.29&#34;,&#34;Interest: Cars&lt;br /&gt;Mean: -0.30&#34;,&#34;Interest: Art exhibitions&lt;br /&gt;Mean:  0.53&#34;,&#34;Interest: Dancing&lt;br /&gt;Mean:  0.21&#34;,&#34;Interest: Musical instruments&lt;br /&gt;Mean:  0.47&#34;,&#34;Interest: Theatre&lt;br /&gt;Mean:  0.36&#34;,&#34;Interest: Writing&lt;br /&gt;Mean:  1.00&#34;,&#34;Interest: Reading&lt;br /&gt;Mean:  0.47&#34;,&#34;Interest: Geography&lt;br /&gt;Mean:  0.23&#34;,&#34;Interest: History&lt;br /&gt;Mean:  0.28&#34;,&#34;Interest: Law&lt;br /&gt;Mean:  0.14&#34;,&#34;Interest: Politics&lt;br /&gt;Mean:  0.15&#34;,&#34;Interest: Psychology&lt;br /&gt;Mean:  0.43&#34;,&#34;Interest: Religion&lt;br /&gt;Mean:  0.25&#34;,&#34;Interest: Foreign languages&lt;br /&gt;Mean:  0.23&#34;,&#34;Interest: Biology&lt;br /&gt;Mean:  0.01&#34;,&#34;Interest: Chemistry&lt;br /&gt;Mean: -0.08&#34;,&#34;Interest: Mathematics&lt;br /&gt;Mean: -0.03&#34;,&#34;Interest: Medicine&lt;br /&gt;Mean:  0.06&#34;,&#34;Interest: Physics&lt;br /&gt;Mean:  0.08&#34;,&#34;Interest: Science and technology&lt;br /&gt;Mean:  0.00&#34;,&#34;Interest: Internet&lt;br /&gt;Mean: -0.11&#34;,&#34;Interest: PC&lt;br /&gt;Mean: -0.09&#34;,&#34;Interest: Celebrities&lt;br /&gt;Mean: -0.03&#34;,&#34;Interest: Economy Management&lt;br /&gt;Mean: -0.06&#34;,&#34;Interest: Fun with friends&lt;br /&gt;Mean: -0.13&#34;,&#34;Interest: Shopping&lt;br /&gt;Mean:  0.01&#34;,&#34;Interest: Pets&lt;br /&gt;Mean:  0.01&#34;],&#34;type&#34;:&#34;scatter&#34;,&#34;mode&#34;:&#34;lines&#34;,&#34;line&#34;:{&#34;width&#34;:4.7244094488189,&#34;color&#34;:&#34;rgba(97,156,255,1)&#34;,&#34;dash&#34;:&#34;solid&#34;},&#34;hoveron&#34;:&#34;points&#34;,&#34;name&#34;:&#34;Arts &amp; Humanities: 24%&#34;,&#34;legendgroup&#34;:&#34;Arts &amp; Humanities: 24%&#34;,&#34;showlegend&#34;:false,&#34;xaxis&#34;:&#34;x&#34;,&#34;yaxis&#34;:&#34;y&#34;,&#34;hoverinfo&#34;:&#34;text&#34;,&#34;frame&#34;:null}],&#34;layout&#34;:{&#34;margin&#34;:{&#34;t&#34;:29.0178497301785,&#34;r&#34;:9.29846409298464,&#34;b&#34;:103.992103462279,&#34;l&#34;:62.2997094229971},&#34;plot_bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;paper_bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:18.5969281859693},&#34;xaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[0.4,32.6],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;Active sport&#34;,&#34;Adrenaline sports&#34;,&#34;Passive sport&#34;,&#34;Countryside, outdoors&#34;,&#34;Gardening&#34;,&#34;Cars&#34;,&#34;Art exhibitions&#34;,&#34;Dancing&#34;,&#34;Musical instruments&#34;,&#34;Theatre&#34;,&#34;Writing&#34;,&#34;Reading&#34;,&#34;Geography&#34;,&#34;History&#34;,&#34;Law&#34;,&#34;Politics&#34;,&#34;Psychology&#34;,&#34;Religion&#34;,&#34;Foreign languages&#34;,&#34;Biology&#34;,&#34;Chemistry&#34;,&#34;Mathematics&#34;,&#34;Medicine&#34;,&#34;Physics&#34;,&#34;Science and technology&#34;,&#34;Internet&#34;,&#34;PC&#34;,&#34;Celebrities&#34;,&#34;Economy Management&#34;,&#34;Fun with friends&#34;,&#34;Shopping&#34;,&#34;Pets&#34;],&#34;tickvals&#34;:[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;Active sport&#34;,&#34;Adrenaline sports&#34;,&#34;Passive sport&#34;,&#34;Countryside, outdoors&#34;,&#34;Gardening&#34;,&#34;Cars&#34;,&#34;Art exhibitions&#34;,&#34;Dancing&#34;,&#34;Musical instruments&#34;,&#34;Theatre&#34;,&#34;Writing&#34;,&#34;Reading&#34;,&#34;Geography&#34;,&#34;History&#34;,&#34;Law&#34;,&#34;Politics&#34;,&#34;Psychology&#34;,&#34;Religion&#34;,&#34;Foreign languages&#34;,&#34;Biology&#34;,&#34;Chemistry&#34;,&#34;Mathematics&#34;,&#34;Medicine&#34;,&#34;Physics&#34;,&#34;Science and technology&#34;,&#34;Internet&#34;,&#34;PC&#34;,&#34;Celebrities&#34;,&#34;Economy Management&#34;,&#34;Fun with friends&#34;,&#34;Shopping&#34;,&#34;Pets&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;outside&#34;,&#34;tickcolor&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;ticklen&#34;:4.64923204649232,&#34;tickwidth&#34;:0.845314917544058,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.8775425487754},&#34;tickangle&#34;:-45,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:&#34;rgba(235,235,235,1)&#34;,&#34;gridwidth&#34;:0.845314917544058,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;y&#34;,&#34;title&#34;:{&#34;text&#34;:&#34;&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:18.5969281859693}},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;yaxis&#34;:{&#34;domain&#34;:[0,1],&#34;automargin&#34;:true,&#34;type&#34;:&#34;linear&#34;,&#34;autorange&#34;:false,&#34;range&#34;:[-0.596,1.076],&#34;tickmode&#34;:&#34;array&#34;,&#34;ticktext&#34;:[&#34;-0.5&#34;,&#34;0.0&#34;,&#34;0.5&#34;,&#34;1.0&#34;],&#34;tickvals&#34;:[-0.5,0,0.5,1],&#34;categoryorder&#34;:&#34;array&#34;,&#34;categoryarray&#34;:[&#34;-0.5&#34;,&#34;0.0&#34;,&#34;0.5&#34;,&#34;1.0&#34;],&#34;nticks&#34;:null,&#34;ticks&#34;:&#34;outside&#34;,&#34;tickcolor&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;ticklen&#34;:4.64923204649232,&#34;tickwidth&#34;:0.845314917544058,&#34;showticklabels&#34;:true,&#34;tickfont&#34;:{&#34;color&#34;:&#34;rgba(77,77,77,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.8775425487754},&#34;tickangle&#34;:-0,&#34;showline&#34;:false,&#34;linecolor&#34;:null,&#34;linewidth&#34;:0,&#34;showgrid&#34;:true,&#34;gridcolor&#34;:&#34;rgba(235,235,235,1)&#34;,&#34;gridwidth&#34;:0.845314917544058,&#34;zeroline&#34;:false,&#34;anchor&#34;:&#34;x&#34;,&#34;title&#34;:{&#34;text&#34;:&#34;Standardized mean interest&#34;,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:18.5969281859693}},&#34;hoverformat&#34;:&#34;.2f&#34;},&#34;shapes&#34;:[{&#34;type&#34;:&#34;rect&#34;,&#34;fillcolor&#34;:&#34;transparent&#34;,&#34;line&#34;:{&#34;color&#34;:&#34;rgba(51,51,51,1)&#34;,&#34;width&#34;:0.845314917544058,&#34;linetype&#34;:&#34;solid&#34;},&#34;yref&#34;:&#34;paper&#34;,&#34;xref&#34;:&#34;paper&#34;,&#34;x0&#34;:0,&#34;x1&#34;:1,&#34;y0&#34;:0,&#34;y1&#34;:1}],&#34;showlegend&#34;:true,&#34;legend&#34;:{&#34;bgcolor&#34;:&#34;rgba(255,255,255,1)&#34;,&#34;bordercolor&#34;:&#34;transparent&#34;,&#34;borderwidth&#34;:2.40515390121689,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:14.8775425487754},&#34;y&#34;:1.2,&#34;orientation&#34;:&#34;h&#34;},&#34;annotations&#34;:[{&#34;text&#34;:&#34;Profile&#34;,&#34;x&#34;:1.02,&#34;y&#34;:1,&#34;showarrow&#34;:false,&#34;ax&#34;:0,&#34;ay&#34;:0,&#34;font&#34;:{&#34;color&#34;:&#34;rgba(0,0,0,1)&#34;,&#34;family&#34;:&#34;&#34;,&#34;size&#34;:18.5969281859693},&#34;xref&#34;:&#34;paper&#34;,&#34;yref&#34;:&#34;paper&#34;,&#34;textangle&#34;:-0,&#34;xanchor&#34;:&#34;left&#34;,&#34;yanchor&#34;:&#34;bottom&#34;,&#34;legendTitle&#34;:true}],&#34;hovermode&#34;:&#34;closest&#34;,&#34;barmode&#34;:&#34;relative&#34;},&#34;config&#34;:{&#34;doubleClick&#34;:&#34;reset&#34;,&#34;showSendToCloud&#34;:false},&#34;source&#34;:&#34;A&#34;,&#34;attrs&#34;:{&#34;4bf4bb81ddf&#34;:{&#34;x&#34;:{},&#34;y&#34;:{},&#34;colour&#34;:{},&#34;type&#34;:&#34;scatter&#34;},&#34;4bf44e0b7ed0&#34;:{&#34;x&#34;:{},&#34;y&#34;:{},&#34;colour&#34;:{}}},&#34;cur_data&#34;:&#34;4bf4bb81ddf&#34;,&#34;visdat&#34;:{&#34;4bf4bb81ddf&#34;:[&#34;function (y) &#34;,&#34;x&#34;],&#34;4bf44e0b7ed0&#34;:[&#34;function (y) &#34;,&#34;x&#34;]},&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;There’s a quick example of LPA. Overall, I think LPA is great tool for Exploratory Analysis, although I question its reproducibility. What’s important is that the statistician considers both fit indices and theory when deciding on the number of profiles.&lt;/p&gt;
&lt;p&gt;###References &amp;amp; Resources&lt;/p&gt;
&lt;p&gt;Bertoletti, M., Friel, N., &amp;amp; Rastelli, R. (2015). Choosing the number of clusters in a finite mixture model using an exact Integrated Completed Likelihood criterion. &lt;a href=&#34;https://arxiv.org/pdf/1411.4257.pdf&#34; class=&#34;uri&#34;&gt;https://arxiv.org/pdf/1411.4257.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Nylund, K. L., Asparouhov, T., &amp;amp; Muthén, B. O. (2007). Deciding on the Number of Classes in Latent Class Analysis and Growth Mixture Modeling: A Monte Carlo Simulation Study. &lt;em&gt;Structural Equation Modeling, 14&lt;/em&gt;, 535-569.&lt;/p&gt;
&lt;p&gt;Scrucca, L., Fop, M., Murphy, T. B., &amp;amp; Raftery, A. E. (2016). mclust5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models. &lt;em&gt;The R Journal, 8&lt;/em&gt;, 289-317.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Detecting Developmental Trends and Gender Differences in Emotion Expression in Children&#39;s Poetry Using Sentiment Analysis</title>
      <link>/talk/srcd2019/</link>
      <pubDate>Sat, 23 Mar 2019 00:00:00 -0400</pubDate>
      
      <guid>/talk/srcd2019/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.&lt;/p&gt;

&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Network Analysis of Emotions</title>
      <link>/post/emotion-network/emotion-network/</link>
      <pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/emotion-network/emotion-network/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/d3/d3.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/forceNetwork-binding/forceNetwork.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this month’s post, I set out to create a visual network of emotions. Emotion Dynamics tells us that different emotions are highly interconnected, such that one emotion morphs into another and so on. I’ll be using a large dataset from an original study published in PLOS ONE by &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0145450&#34;&gt;Trampe, Quoidbach, and Taquet (2015)&lt;/a&gt;. Thanks to &lt;a href=&#34;https://toolbox.google.com/datasetsearch&#34;&gt;Google Dataset Search&lt;/a&gt;, I was able to locate this data. The data is collected from 11,000 participants who completed daily questionnaires on the emotions they felt at a given moment. The original paper is fascinating and I highly encourage checking it out - not to mention that the author’s analysis is the inspiration for this post. The raw data can be freely accessed from the author’s OSF page (link in online article) - props to them for publishing the data!&lt;/p&gt;
&lt;p&gt;What is a network? In a sentence, a network is a complex set of interrelations between variables. Some terminology: &lt;em&gt;nodes&lt;/em&gt; are the variables (in this case, emotions), and &lt;em&gt;edges&lt;/em&gt; are the relationships between the variables. Networks can be &lt;em&gt;directed&lt;/em&gt;, which means that variables are linked in a sequence (e.g, from emotion A to emotion B), or &lt;em&gt;undirected&lt;/em&gt;, which just shows the relationships. Trampe et al. (2015) created an undirected network in their paper, but the data also allows for a directed network - and this is what I’m going to make for this post.&lt;/p&gt;
&lt;p&gt;First, I’ll read in the data and fix up a few spelling errors from the original dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

emotion_raw &amp;lt;- read_csv(&amp;quot;https://osf.io/e7uab/download&amp;quot;) %&amp;gt;%
  rename(Offense = Ofense,
         Embarrassment = Embarassment)

emotion_raw&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 69,544 x 21
##       id Hours   Day Pride  Love  Hope Gratitude   Joy Satisfaction   Awe
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1     1     1     3     0     0     0         0     0            0     0
##  2     1    14     1     0     0     0         0     0            0     0
##  3     1    14     2     0     0     0         0     0            0     0
##  4     1    14     4     0     0     0         0     0            0     0
##  5     1    15     3     0     0     0         0     0            0     0
##  6     1    15     3     0     0     0         0     0            0     0
##  7     1    19     1     0     0     0         0     0            0     0
##  8     8     8     5     0     0     0         0     1            0     0
##  9     8     9     2     0     0     0         0     1            0     0
## 10     8    10     7     0     0     0         0     0            0     0
## # ... with 69,534 more rows, and 11 more variables: Amusement &amp;lt;dbl&amp;gt;,
## #   Alertness &amp;lt;dbl&amp;gt;, Anxiety &amp;lt;dbl&amp;gt;, Disdain &amp;lt;dbl&amp;gt;, Offense &amp;lt;dbl&amp;gt;,
## #   Guilt &amp;lt;dbl&amp;gt;, Disgust &amp;lt;dbl&amp;gt;, Fear &amp;lt;dbl&amp;gt;, Embarrassment &amp;lt;dbl&amp;gt;,
## #   Sadness &amp;lt;dbl&amp;gt;, Anger &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is formatted as a sparse matrix (lots of zeros). We have participant id, Day, and Hour that the emotion was reported. To make this data network compatible, I need to wrangle it into a dataframe of &lt;em&gt;edges&lt;/em&gt; - that is a &lt;em&gt;from&lt;/em&gt; column and a &lt;em&gt;to&lt;/em&gt; column. This will become more apparent shortly.&lt;/p&gt;
&lt;p&gt;I can use the function &lt;em&gt;gather&lt;/em&gt; to turn the data into long format. By filtering for values of 1, I remove all the zeros from the sparse matrix and I’m left with a column that includes the emotion that was experienced at the time of reporting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotion_long &amp;lt;- emotion_raw %&amp;gt;%
  gather(emotion_type, value, Pride:Anger) %&amp;gt;%
  arrange(id, Day) %&amp;gt;%
  filter(value == 1) %&amp;gt;%
  select(-value)

emotion_long&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 187,426 x 4
##       id Hours   Day emotion_type 
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;        
##  1     1    19     1 Offense      
##  2     1    19     1 Sadness      
##  3     1    14     2 Disgust      
##  4     1    15     3 Alertness    
##  5     1    15     3 Anxiety      
##  6     1    15     3 Embarrassment
##  7     1    15     3 Sadness      
##  8     1    14     4 Alertness    
##  9     1    14     4 Embarrassment
## 10     8     9     2 Joy          
## # ... with 187,416 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Still, there are no edges here - no link between one emotion and the next. Because the data is arranged so that each subsequent row is the next emotion, I can create a new variable, second_emotion, that is the &lt;em&gt;lead&lt;/em&gt; of the emotion in that row. Then, I make sure to remove the last row from each participant id (otherwise there would be a relationship between Participant #1’s last emotion and Participant #2’s first emotion).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotion_edges &amp;lt;- emotion_long %&amp;gt;%
  mutate(second_emotion = lead(emotion_type)) %&amp;gt;%
  rename(first_emotion = emotion_type) %&amp;gt;%
  select(id, Day, Hours, first_emotion, second_emotion) %&amp;gt;%
  group_by(id) %&amp;gt;%
  slice(-length(id))

emotion_edges&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 175,480 x 5
## # Groups:   id [11,332]
##       id   Day Hours first_emotion second_emotion
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;         
##  1     1     1    19 Offense       Sadness       
##  2     1     1    19 Sadness       Disgust       
##  3     1     2    14 Disgust       Alertness     
##  4     1     3    15 Alertness     Anxiety       
##  5     1     3    15 Anxiety       Embarrassment 
##  6     1     3    15 Embarrassment Sadness       
##  7     1     3    15 Sadness       Alertness     
##  8     1     4    14 Alertness     Embarrassment 
##  9     8     2     9 Joy           Alertness     
## 10     8     2     9 Alertness     Alertness     
## # ... with 175,470 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how first and second emotion form a sort of chain - Offense to Sadness, Sadness to Disgust, Disgust to Alertness, etc.&lt;/p&gt;
&lt;p&gt;We’re ignoring the fact that people are experiencing multiple emotions at once and in those instances we don’t know which emotion was experienced first.&lt;/p&gt;
&lt;p&gt;Now that we have our edges, we need to create an object containing the nodes. This is pretty simple, but I’ll add some information indicating the valence and frequency (n) of each emotion, which will help with the visualizations that follow.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotion_nodes &amp;lt;- emotion_long %&amp;gt;%
  count(emotion_type) %&amp;gt;%
  rowid_to_column(&amp;quot;id&amp;quot;) %&amp;gt;%
  rename(label = emotion_type) %&amp;gt;%
  mutate(valence = ifelse(label %in% c(&amp;quot;Awe&amp;quot;, &amp;quot;Amusement&amp;quot;, &amp;quot;Joy&amp;quot;, &amp;quot;Alertness&amp;quot;,
                                              &amp;quot;Hope&amp;quot;, &amp;quot;Love&amp;quot;, &amp;quot;Gratitude&amp;quot;, &amp;quot;Pride&amp;quot;,
                                              &amp;quot;Satisfaction&amp;quot;), &amp;quot;positive&amp;quot;, &amp;quot;negative&amp;quot;))

emotion_nodes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 18 x 4
##       id label             n valence 
##    &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;   
##  1     1 Alertness     17932 positive
##  2     2 Amusement     11136 positive
##  3     3 Anger          6394 negative
##  4     4 Anxiety       19115 negative
##  5     5 Awe            3830 positive
##  6     6 Disdain         682 negative
##  7     7 Disgust        7412 negative
##  8     8 Embarrassment  3000 negative
##  9     9 Fear           3457 negative
## 10    10 Gratitude      7379 positive
## 11    11 Guilt          3500 negative
## 12    12 Hope          16665 positive
## 13    13 Joy           23465 positive
## 14    14 Love          18625 positive
## 15    15 Offense        3160 negative
## 16    16 Pride          9214 positive
## 17    17 Sadness       13460 negative
## 18    18 Satisfaction  19000 positive&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have an object containing our nodes and an object containing our edges. Now it’s a matter of weighting (counting) the relationships between the emotions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotion_network &amp;lt;- emotion_edges %&amp;gt;%
  group_by(first_emotion, second_emotion) %&amp;gt;%
  summarize(weight = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  select(first_emotion, second_emotion, weight)

emotion_network&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 315 x 3
##    first_emotion second_emotion weight
##    &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;           &amp;lt;int&amp;gt;
##  1 Alertness     Alertness        6191
##  2 Alertness     Amusement          97
##  3 Alertness     Anger             214
##  4 Alertness     Anxiety          4784
##  5 Alertness     Awe                14
##  6 Alertness     Disdain            82
##  7 Alertness     Disgust           544
##  8 Alertness     Embarrassment     208
##  9 Alertness     Fear              109
## 10 Alertness     Gratitude          70
## # ... with 305 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A few more modifications are needed to make it ready for visualization. I’m trimming some of the really high values using &lt;em&gt;ifelse&lt;/em&gt;, just so that they don’t overwhelm the plotting screen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edges &amp;lt;- emotion_network %&amp;gt;%
  left_join(emotion_nodes, by = c(&amp;quot;first_emotion&amp;quot; = &amp;quot;label&amp;quot;)) %&amp;gt;%
  rename(from = id)

edges &amp;lt;- edges %&amp;gt;%
  left_join(emotion_nodes, by = c(&amp;quot;second_emotion&amp;quot; = &amp;quot;label&amp;quot;)) %&amp;gt;%
  rename(to = id) %&amp;gt;%
  select(from, to, weight) %&amp;gt;%
  mutate(weight = ifelse(weight &amp;gt; 4500, 4500, weight))

edges&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 315 x 3
##     from    to weight
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;
##  1     1     1   4500
##  2     1     2     97
##  3     1     3    214
##  4     1     4   4500
##  5     1     5     14
##  6     1     6     82
##  7     1     7    544
##  8     1     8    208
##  9     1     9    109
## 10     1    10     70
## # ... with 305 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need the &lt;em&gt;tidygraph&lt;/em&gt; and &lt;em&gt;ggraph&lt;/em&gt; packages for the visualization. I’ll note that there are a number of packages for visualizing networks, but &lt;em&gt;ggraph&lt;/em&gt; seems to be preferred because it is compatible with ggplot terminology. The function &lt;em&gt;tbl_graph&lt;/em&gt; will take the nodes and edges and make them ggraph ready.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidygraph)
library(ggraph)

network &amp;lt;- tbl_graph(emotion_nodes, edges, directed = TRUE)

set.seed(190318)

ggraph(network, layout = &amp;quot;graphopt&amp;quot;) +
  geom_edge_link(aes(width = weight, color = scale(weight), alpha = weight), check_overlap = TRUE) +
  scale_edge_color_gradient2(low = &amp;quot;darkgrey&amp;quot;, mid = &amp;quot;#00BFFF&amp;quot;, midpoint = 1.5, high = &amp;quot;dodgerblue2&amp;quot;) +
  scale_edge_width(range = c(.2, 1.75)) +
  geom_node_label(aes(label = label, fill = valence), size = 4) +
  scale_fill_manual(values = c(&amp;quot;#FF6A6A&amp;quot;, &amp;quot;#43CD80&amp;quot;)) +
  theme_graph() +
  theme(legend.position = &amp;quot;none&amp;quot;, plot.background = element_rect(fill = &amp;quot;black&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-network/Emotion_Network_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Stronger relationships show up as thicker lines. Positive emotions seem to be more pronounced and interconnected, which is what was found in the original article. Unfortunately, we don’t get a good sense of temporality (adding directional arrows creates more of a mess than anything). An interactive plot might be more informative, so let’s try that using &lt;em&gt;networkD3&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(networkD3)

nodes_d3 &amp;lt;- emotion_nodes %&amp;gt;%
  mutate(id = id - 1,
         n = (scale(n) + 3)^3)

edges_d3 &amp;lt;- edges %&amp;gt;%
  mutate(from = from - 1, to = to - 1,
         weight = ifelse(weight &amp;lt; 600, 0, log(weight)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s VERY important to transform the values to base 0, which is why I’m using mutate -1. networkD3 won’t work on base 1 values.&lt;/p&gt;
&lt;p&gt;Again, I’ve made a few adjustments for visualization purposes. Namely, I’m removing relationships that occur less than 600 times and scaling the values somewhat arbitrarily. Of course, this is exploratory analysis, but caution should be taken when interpreting these results. The function &lt;em&gt;forceNetwork&lt;/em&gt; takes the nodes and edges specified above and turns them into something beautiful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;forceNetwork(Links = edges_d3, Nodes = nodes_d3, Source = &amp;quot;from&amp;quot;, Nodesize = &amp;quot;n&amp;quot;,
             Target = &amp;quot;to&amp;quot;, NodeID = &amp;quot;label&amp;quot;, Group = &amp;quot;valence&amp;quot;, Value = &amp;quot;weight&amp;quot;, fontFamily = &amp;quot;sans-serif&amp;quot;,
             colourScale = JS(&amp;#39;d3.scaleOrdinal().domain([&amp;quot;negative&amp;quot;, &amp;quot;positive&amp;quot;]).range([&amp;quot;#FF6A6A&amp;quot;, &amp;quot;#43CD80&amp;quot;])&amp;#39;),
             opacity = 1, fontSize = 24, linkDistance = 300, linkColour = c(&amp;quot;#8DB6CD&amp;quot;),
             arrows = TRUE, zoom = TRUE, bounded = TRUE, legend = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Links is a tbl_df. Converting to a plain data frame.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Nodes is a tbl_df. Converting to a plain data frame.&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;forceNetwork html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;links&#34;:{&#34;source&#34;:[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17],&#34;target&#34;:[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,9,10,11,12,13,14,15,16,17,0,1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,6,7,9,10,11,12,13,14,15,16,17,0,1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],&#34;value&#34;:[8.41183267575841,0,0,8.41183267575841,0,0,0,0,0,0,0,0,0,6.55393340402581,0,6.78219205600679,6.52795791762255,0,8.33591109419694,7.97108575350561,0,7.26262860097424,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7.13169851046691,0,0,0,0,0,0,0,0,0,0,6.80903930604298,0,6.60934924316738,0,0,0,0,0,8.41183267575841,0,0,7.38770923908104,0,6.77308037565554,0,7.16317239084664,0,0,6.68959926917897,7.24921505711439,6.44571981938558,7.46336304552002,0,6.4425401664682,7.38523092306657,0,0,6.89365635460264,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7.46508273639955,0,6.99209642741589,0,0,0,0,0,0,0,7.78030308790837,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7.11151211649616,0,0,0,0,0,0,0,6.49978704065585,6.61606518513282,0,0,0,0,0,0,0,7.06561336359772,0,0,0,0,0,0,0,0,7.93092537248339,0,0,8.15679704667565,0,0,0,0,0,0,0,0,0,0,7.03790596344718,0,0,0,6.54821910276237,0,0,0,0,0,0,0,6.4377516497364,0,0,6.58063913728495,0,0,0,0,0,8.02092771898158,0,8.41183267575841,8.3513747067213,0,0,0,0,6.63463335786169,7.13009851012558,7.40488757561612,0,6.82219739062049,0,0,0,0,0,0,0,0,8.41183267575841,0,0,0,0,8.41183267575841,0,0,0,6.69456205852109,0,0,0,0,0,0,0,8.41183267575841,7.99361999482774,8.41183267575841,0,0,0,0,0,0,0,0,6.94119005506837,0,0,0,6.50876913697168,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7.27309259599952,6.55393340402581,8.17723488551019,0,7.92334821193015,0,0,0,0,8.10379671298179,0,0,0,0,0,0,0,0,0,0,6.96129604591017,0,6.60123011872888,8.33471162182092,0,7.87853419614036,8.12058871174027,0,7.23128700432762,7.57250298502038,0,0,0,0,0,0,0,0,0,0,0,0,8.41183267575841],&#34;colour&#34;:[&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;,&#34;#8DB6CD&#34;]},&#34;nodes&#34;:{&#34;name&#34;:[&#34;Alertness&#34;,&#34;Amusement&#34;,&#34;Anger&#34;,&#34;Anxiety&#34;,&#34;Awe&#34;,&#34;Disdain&#34;,&#34;Disgust&#34;,&#34;Embarrassment&#34;,&#34;Fear&#34;,&#34;Gratitude&#34;,&#34;Guilt&#34;,&#34;Hope&#34;,&#34;Joy&#34;,&#34;Love&#34;,&#34;Offense&#34;,&#34;Pride&#34;,&#34;Sadness&#34;,&#34;Satisfaction&#34;],&#34;group&#34;:[&#34;positive&#34;,&#34;positive&#34;,&#34;negative&#34;,&#34;negative&#34;,&#34;positive&#34;,&#34;negative&#34;,&#34;negative&#34;,&#34;negative&#34;,&#34;negative&#34;,&#34;positive&#34;,&#34;negative&#34;,&#34;positive&#34;,&#34;positive&#34;,&#34;positive&#34;,&#34;negative&#34;,&#34;positive&#34;,&#34;negative&#34;,&#34;positive&#34;],&#34;nodesize&#34;:[66.4777260049541,29.823109822341,14.4971585620182,74.9545346567671,8.99863483398379,4.41089086229159,17.1859473592041,7.57540966372343,8.33882236853888,17.0939636079722,8.41318408245376,58.1352372636087,112.27983133578,71.3610761571251,7.83713818129178,22.7248729842467,40.2101604728943,74.1005872923404]},&#34;options&#34;:{&#34;NodeID&#34;:&#34;label&#34;,&#34;Group&#34;:&#34;valence&#34;,&#34;colourScale&#34;:&#34;d3.scaleOrdinal().domain([\&#34;negative\&#34;, \&#34;positive\&#34;]).range([\&#34;#FF6A6A\&#34;, \&#34;#43CD80\&#34;])&#34;,&#34;fontSize&#34;:24,&#34;fontFamily&#34;:&#34;sans-serif&#34;,&#34;clickTextSize&#34;:60,&#34;linkDistance&#34;:300,&#34;linkWidth&#34;:&#34;function(d) { return Math.sqrt(d.value); }&#34;,&#34;charge&#34;:-30,&#34;opacity&#34;:1,&#34;zoom&#34;:true,&#34;legend&#34;:true,&#34;arrows&#34;:true,&#34;nodesize&#34;:true,&#34;radiusCalculation&#34;:&#34; Math.sqrt(d.nodesize)+6&#34;,&#34;bounded&#34;:true,&#34;opacityNoHover&#34;:0,&#34;clickAction&#34;:null}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Hovering over the nodes shows the emotion label and its relationships with other emotions. The arrows indicate directionality in time. It’s a good enough graph, although I would like for the labels to show up at all times. I still have lots to learn about network analysis.&lt;/p&gt;
&lt;p&gt;As a final note, I’ll mention that I neglected to adjust for the nested structure of the data - emotions nested within hours, days, and participants. This is crucial when conducting formal statistical tests, but should also be accounted for in visualizations.&lt;/p&gt;
&lt;div id=&#34;references-resources&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References &amp;amp; Resources&lt;/h3&gt;
&lt;p&gt;This &lt;a href=&#34;https://www.jessesadler.com/post/network-analysis-with-r/&#34;&gt;blog post&lt;/a&gt; by Jesse Sadler really helped in the initial stages of my learning on network analysis.&lt;/p&gt;
&lt;p&gt;Trampe, D., Quoidbach, J., Taquet, M. (2015). Emotions in everyday life. &lt;em&gt;PLOS ONE&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1371/journal.pone.0145450&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1371/journal.pone.0145450&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Using sentiment analysis to detect affect in children’s and adolescents’ poetry</title>
      <link>/publication/hipson-2019/</link>
      <pubDate>Sat, 23 Feb 2019 00:00:00 -0500</pubDate>
      
      <guid>/publication/hipson-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Active emotion regulation mediates links between shyness and social adjustment in preschool</title>
      <link>/publication/hipson-et-al.-2019/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 -0500</pubDate>
      
      <guid>/publication/hipson-et-al.-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Face of (Dis)Agreement - Intraclass Correlations</title>
      <link>/post/emotion-expression-icc/emotion-expression/</link>
      <pubDate>Mon, 04 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/emotion-expression-icc/emotion-expression/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I was recently introduced to &lt;a href=&#34;https://toolbox.google.com/datasetsearch&#34;&gt;Google Dataset Search&lt;/a&gt;, an extension that searches for open access datasets. There I stumbled upon this dataset on childrens’ and adult’s ratings of facial expressions. The data comes from a published article by &lt;a href=&#34;https://www.tandfonline.com/doi/full/10.1080/17405629.2017.1287073&#34;&gt;Vesker et al. (2018)&lt;/a&gt;. Briefly, this study involved having adults and 9-year-old children rate a series of 48 faces on two dimensions of emotion, valence (positive vs. negative) and arousal (activated vs. deactivated) (see my previous &lt;a href=&#34;https://willhipson.netlify.com/post/circumplex/circumplex/&#34;&gt;post&lt;/a&gt; for more info on valence and arousal). The authors made some interesting observations about differences in childrens’ and adult’s ratings of these facial expressions.&lt;/p&gt;
&lt;p&gt;However, absent from the writeup was a discussion about how reliable these ratings are. We might wonder about the extent to which people agree on the valence or arousal of a face and whether this varies between children and adults. Here, I tackle the issue of intraclass correlation (ICC) using the dataset published by Vesker et al. (2018). The data itself is openly accessible &lt;a href=&#34;https://zenodo.org/record/293008#.XFXlr1xKhPZ&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, I’ll load up the &lt;em&gt;tidyverse&lt;/em&gt; and &lt;em&gt;readxl&lt;/em&gt; packages, which will help with the data cleaning.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)
library(tidyverse)

options(digits = 3, scipen = -2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upon downloading the data, we’re immediately presented with an issue: it’s an xlsx document (Excel) containing multiple sheets, with each sheet representing a “condition” (Child vs. Adult) and (Valence vs. Arousal). On Stack Overflow, I found a useful function for reading in multiple sheets (&lt;a href=&#34;https://stackoverflow.com/questions/12945687/read-all-worksheets-in-an-excel-workbook-into-an-r-list-with-data-frames&#34;&gt;see here for original post&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_excel_allsheets &amp;lt;- function(filename, tibble = TRUE) {
  sheets &amp;lt;- readxl::excel_sheets(filename)
  x &amp;lt;- lapply(sheets, function(x) readxl::read_excel(filename, sheet = x))
  if(!tibble) x &amp;lt;- lapply(x, as.data.frame)
  names(x) &amp;lt;- sheets
  x
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I’ll read in the data and extract each sheet. I’m using VA to refer to valence and AR for arousal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;faces &amp;lt;- read_excel_allsheets(&amp;quot;C:/Users/wille/Downloads/adults and 9yo all AR and VAL face ratings zenodo.xlsx&amp;quot;)

VA_adult_raw &amp;lt;- faces$`VAL adult faces`
VA_child_raw &amp;lt;- faces$`Val 9yo faces`
AR_adult_raw &amp;lt;- faces$`AR adult faces`
AR_child_raw &amp;lt;- faces$`AR 9yo faces`&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s get a look at one of these datasets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(VA_adult_raw[, 1:20]) #Limiting preview to n = 20&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 20
##   ..1       ..2   ..3   ..4   ..5   ..6   ..7   ..8   ..9  ..10  ..11  ..12
##   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 dl sa ~     4     4     3     4     2     3     4     3     3     5     3
## 2 dl sa ~     4     4     3     3     2     3     3     3     3     4     4
## 3 lp sa ~     2     3     4     3     3     3     2     3     3     3     2
## 4 lp sa ~     4     3     2     3     3     3     4     2     2     4     2
## 5 ma sa ~     1     1     1     2     1     2     1     1     2     3     2
## 6 md sa ~     2     1     1     2     2     3     1     1     2     3     2
## # ... with 8 more variables: ..13 &amp;lt;dbl&amp;gt;, ..14 &amp;lt;dbl&amp;gt;, ..15 &amp;lt;dbl&amp;gt;,
## #   ..16 &amp;lt;dbl&amp;gt;, ..17 &amp;lt;dbl&amp;gt;, ..18 &amp;lt;dbl&amp;gt;, ..19 &amp;lt;dbl&amp;gt;, ..20 &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s not immediately apparent what is being displayed here because the columns aren’t labeled. The article tells us that participants rated 48 faces, so based on the dimensions we can assume that each row is a face and each column is a participant who rated that face. Admittedly, it’s a less intuitive way of representing the data, but its actually ideal for computing ICCs.&lt;/p&gt;
&lt;p&gt;Still, there’s a lot of data cleaning and wrangling to be done here. First, we have some rows and columns that aren’t relevant, so we’ll get rid of those. Of note, &lt;em&gt;dplyr&lt;/em&gt;’s lesser known &lt;em&gt;slice&lt;/em&gt; function is helpful for identifying which rows we want to keep.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VA_adult &amp;lt;- VA_adult_raw %&amp;gt;%
  slice(1:48) %&amp;gt;%
  select(-1, -mean, -SD, -`M mean`, -`F mean`, -`dist from 4`, -`0`, -aa, -Valence) %&amp;gt;%
  mutate(face = row_number()) %&amp;gt;%
  select(face, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, to make the columns more intuitive, we’ll label them properly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colnames(VA_adult)[2:161] &amp;lt;- paste(&amp;quot;rater&amp;quot;, 1:160)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;plotting-the-scores&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting the Scores&lt;/h2&gt;
&lt;p&gt;We may want to plot the data to get a sense of the variability around raters’ labeling of valence across the 48 faces. Our data is currently in wide form, but we need to set it up such that all of the ratings are in one column. This is where &lt;em&gt;reshape2&lt;/em&gt; comes into action.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;em&gt;melt&lt;/em&gt; will take our wide dataset and make it long. We supply it with an &lt;em&gt;id.vars&lt;/em&gt; which tells it which of our original columns we want to stay as a column. Then it takes all of the other columns and condenses them into one variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VA_adult_melt &amp;lt;- VA_adult %&amp;gt;%
  melt(id.vars = &amp;quot;face&amp;quot;, value.name = &amp;quot;valence&amp;quot;, variable.name = &amp;quot;rater&amp;quot;)

head(VA_adult_melt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   face   rater valence
## 1    1 rater 1       4
## 2    2 rater 1       4
## 3    3 rater 1       2
## 4    4 rater 1       4
## 5    5 rater 1       1
## 6    6 rater 1       2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ll turn this into a line graph with each line representing an individual rater’s valence ratings for each of the 48 faces. It will be crowded, but that’s OK. We just want to see if the lines cluster around each other or not.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VA_adult_melt %&amp;gt;%
  ggplot(aes(face, valence, group = rater, color = rater)) +
  geom_line(size = .8, alpha = .5) +
  scale_x_discrete(limits = 1:48) +
  geom_vline(xintercept = 24.5, size = 1.5, color = &amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;Face&amp;quot;, y = &amp;quot;Valence (higher = more positive)&amp;quot;,
       title = &amp;quot;Adult Valence Ratings for 48 Faces&amp;quot;,
       subtitle = &amp;quot;Red line indicates where faces become positive&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-expression-icc/Face-Recognition_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s a clear division between the positive and negative faces. It seems that there’s strong agreement among adults as to what constitutes a positive or negative face.&lt;/p&gt;
&lt;p&gt;What if we looked at the distributions of the ratings for each face?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VA_adult_melt %&amp;gt;%
  group_by(face) %&amp;gt;%
  summarize(mean = mean(valence),
            sd = sd(valence)) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(face, mean)) +
  geom_errorbar(aes(ymin = mean - 1.96*(sd/sqrt(ncol(VA_adult))),
                    ymax = mean + 1.96*(sd/sqrt(ncol(VA_adult))))) +
  geom_point(size = 2) +
  scale_x_discrete(limits = 1:48) +
  geom_vline(xintercept = 24.5, color = &amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;Face&amp;quot;, y = &amp;quot;Valence (higher = more positive)&amp;quot;,
       title = &amp;quot;Adult - Average Valence Ratings for 48 Faces&amp;quot;,
       subtitle = &amp;quot;Red line indicates where faces become positive; error bars = 95% CI&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-expression-icc/Face-Recognition_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It tells the same story, but it’s a more polished figure. Notice how the error bars for the 95% confidence interval around the mean are quite small.&lt;/p&gt;
&lt;div id=&#34;cleaning-the-remaining-datasets&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cleaning the Remaining Datasets&lt;/h3&gt;
&lt;p&gt;First, we’ll look at the dataset for 9-year-olds’ ratings of valence. Note that there are a few modifications to the script due to idiosyncracies with the original datasets.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VA_child &amp;lt;- VA_child_raw %&amp;gt;%
  slice(2:49) %&amp;gt;%
  select(-1, -`average child ratings`, -33, -code,
         -`pic name`, -emotion, -`Child M`, -`Child F`, -sex, -Valence) %&amp;gt;%
  mutate(face = row_number()) %&amp;gt;%
  select(face, everything())

colnames(VA_child)[2:31] &amp;lt;- paste(&amp;quot;rater&amp;quot;, 1:30)

VA_child &amp;lt;- tbl_df(lapply(VA_child, function(x){ #Need to use a function to convert to numeric
  as.numeric(as.character(x)) 
}))

VA_child_melt &amp;lt;- VA_child %&amp;gt;%
  melt(id.vars = &amp;quot;face&amp;quot;, value.name = &amp;quot;valence&amp;quot;, variable.name = &amp;quot;rater&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VA_child_melt %&amp;gt;%
  ggplot(aes(face, valence, group = rater, color = rater)) +
  geom_line(size = .8, alpha = .5) +
  scale_x_discrete(limits = 1:48) +
  geom_vline(xintercept = 24.5, size = 1.5, color = &amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;Face&amp;quot;, y = &amp;quot;Valence (higher = more positive)&amp;quot;,
       title = &amp;quot;Child Valence Ratings for 48 Faces&amp;quot;,
       subtitle = &amp;quot;Red line indicates where faces become positive&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-expression-icc/Face-Recognition_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VA_child_melt %&amp;gt;%
  group_by(face) %&amp;gt;%
  summarize(mean = mean(valence, na.rm = TRUE),
            sd = sd(valence, na.rm = TRUE)) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(face, mean)) +
  geom_errorbar(aes(ymin = mean - 1.96*(sd/sqrt(ncol(VA_child))),
                    ymax = mean + 1.96*(sd/sqrt(ncol(VA_child))))) +
  geom_point(size = 2) +
  scale_x_discrete(limits = 1:48) +
  geom_vline(xintercept = 24.5, color = &amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;Face&amp;quot;, y = &amp;quot;Valence (higher = more positive)&amp;quot;,
       title = &amp;quot;Child - Average Valence Ratings for 48 Faces&amp;quot;,
       subtitle = &amp;quot;Red line indicates where faces become positive; error bars = 95% CI&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-expression-icc/Face-Recognition_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results are similar to those from the adults. We can’t trust the wider confidence intervals to tell us about reliability though, because there are far fewer child raters than adult raters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;repeating-the-procedure-for-ratings-of-arousal&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Repeating the Procedure for Ratings of Arousal&lt;/h3&gt;
&lt;p&gt;Finally, we repeat the analysis for measures of arousal, starting with adults then with children.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AR_adult &amp;lt;- AR_adult_raw %&amp;gt;%
  slice(1:48) %&amp;gt;%
  select(-1, -43, -SD, -`M mean`, -`F mean`, -mean, -`0`, -Valence) %&amp;gt;%
  mutate(face = row_number()) %&amp;gt;%
  select(face, everything())

colnames(AR_adult)[2:42] &amp;lt;- paste(&amp;quot;rater&amp;quot;, 1:41)

AR_adult_melt &amp;lt;- AR_adult %&amp;gt;%
  melt(id.vars = &amp;quot;face&amp;quot;, value.name = &amp;quot;arousal&amp;quot;, variable.name = &amp;quot;rater&amp;quot;)

AR_adult_melt %&amp;gt;%
  ggplot(aes(face, arousal, group = rater, color = rater)) +
  geom_line(size = .8, alpha = .5) +
  scale_x_discrete(limits = 1:48) +
  geom_vline(xintercept = 24.5, size = 1.5, color = &amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;Face&amp;quot;, y = &amp;quot;Arousal (higher = more activated)&amp;quot;,
       title = &amp;quot;Adult Arousal Ratings for 48 Faces&amp;quot;,
       subtitle = &amp;quot;Red line indicates where faces become positive&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-expression-icc/Face-Recognition_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There seems to be much less consensus for ratings of arousal. We do notice that there is no differentiation between positive and negative faces - this is good because the theory suggests that arousal is independent of valence. Someone can be positively aroused (e.g., excited) or negatively aroused (e.g., stressed). However, if there was high consensus we would still see the lines converging. Instead, they’re all over the place.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AR_adult_melt %&amp;gt;%
  group_by(face) %&amp;gt;%
  summarize(mean = mean(arousal),
            sd = sd(arousal)) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(face, mean)) +
  geom_errorbar(aes(ymin = mean - 1.96*(sd/sqrt(ncol(AR_adult))),
                    ymax = mean + 1.96*(sd/sqrt(ncol(AR_adult))))) +
  geom_point(size = 2) +
  scale_x_discrete(limits = 1:48) +
  geom_vline(xintercept = 24.5, color = &amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;Face&amp;quot;, y = &amp;quot;Arousal (higher = more activated)&amp;quot;,
       title = &amp;quot;Adult - Average Arousal Ratings for 48 Faces&amp;quot;,
       subtitle = &amp;quot;Red line indicates where faces become positive; error bars = 95% CI&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-expression-icc/Face-Recognition_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Confidence intervals are much wider too, but again, we have a smaller sample size so that adds some uncertainty. Still, it seems like adults have difficulty agreeing on ratings of arousal compared to ratings of valence. Let’s go back to 9-year-olds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AR_child &amp;lt;- AR_child_raw %&amp;gt;%
  slice(2:49) %&amp;gt;%
  select(-`photo ID`, -child, -`adult ratings PELL`, -`Photo ID`,
         -30, -image, -emotion, -`m child`, -`f child`, -Valence) %&amp;gt;%
  mutate(face = row_number()) %&amp;gt;%
  select(face, everything())

colnames(AR_child)[2:31] &amp;lt;- paste(&amp;quot;rater&amp;quot;, 1:30)

AR_child &amp;lt;- tbl_df(lapply(AR_child, function(x){ #Need to use a function to convert to numeric
  as.numeric(as.character(x)) #Note: There is one missing value from original dataset
}))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in FUN(X[[i]], ...): NAs introduced by coercion&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AR_child_melt &amp;lt;- AR_child %&amp;gt;%
  melt(id.vars = &amp;quot;face&amp;quot;, value.name = &amp;quot;arousal&amp;quot;, variable.name = &amp;quot;rater&amp;quot;)

AR_child_melt %&amp;gt;%
  ggplot(aes(face, arousal, group = rater, color = rater)) +
  geom_line(size = .8, alpha = .5) +
  scale_x_discrete(limits = 1:48) +
  geom_vline(xintercept = 24.5, size = 1.5, color = &amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;Face&amp;quot;, y = &amp;quot;Arousal (higher = more activated)&amp;quot;,
       title = &amp;quot;Child Arousal Ratings for 48 Faces&amp;quot;,
       subtitle = &amp;quot;Red line indicates where faces become positive&amp;quot;) +
  theme_bw() +
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 48 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-expression-icc/Face-Recognition_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AR_child_melt %&amp;gt;%
  group_by(face) %&amp;gt;%
  summarize(mean = mean(arousal, na.rm = TRUE),
            sd = sd(arousal, na.rm = TRUE)) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot(aes(face, mean)) +
  geom_errorbar(aes(ymin = mean - 1.96*(sd/sqrt(ncol(AR_child))),
                    ymax = mean + 1.96*(sd/sqrt(ncol(AR_child))))) +
  geom_point(size = 2) +
  scale_x_discrete(limits = 1:48) +
  geom_vline(xintercept = 24.5, color = &amp;quot;red&amp;quot;) +
  labs(x = &amp;quot;Face&amp;quot;, y = &amp;quot;Arousal (higher = more activated)&amp;quot;,
       title = &amp;quot;Child - Average Arousal Ratings for 48 Faces&amp;quot;,
       subtitle = &amp;quot;Red line indicates where faces become positive; error bars = 95% CI&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/emotion-expression-icc/Face-Recognition_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Results look similar for children. I won’t spend much time discussing mean differences in valence and arousal between children and adults - the original article expands on this. However, I am interested in the variability in ratings of arousal vs. valence.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;quantifying-interrater-agreement&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quantifying Interrater Agreement&lt;/h2&gt;
&lt;p&gt;So far, we’ve created a series of plots showing the variability in childrens’ and adult’s ratings of emotional facial expressions. We get a sense that both children and adults reliably label faces as positive or negative, but they struggle to agree on arousal. Although this is apparent from the plots, we may want to test this more formally. This is actually very important because our estimates of variability (e.g., 95% CI) are sensitive to sample size, which varies by adults and children in this dataset.&lt;/p&gt;
&lt;div id=&#34;intra-correlation-coefficient-icc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Intra-correlation coefficient (ICC)&lt;/h3&gt;
&lt;p&gt;The ICC is an index of reliability or agreement for continuous ratings. ICCs range from 0 (no agreement) to 1 (perfect agreement). We will use ICC to quantify agreement on ratings of emotional facial expressions, but ICC is applicable to other situations, such as quantifying heritability or assessing items in a test bank. Here, we will calculate four ICCs: (1) Adult ratings of Valence, (2) Child ratings of Valence, (3) Adults ratings of Arousal, and (4) Child ratings of Arousal.&lt;/p&gt;
&lt;p&gt;Shrout and Fleiss (1979), and later McGraw and Wong (1996), describe several different calculations for ICC that depend on the characteristics of the sample. In our case, we will use a two-way random model for single measurements to quantify absolute agreement, also known as ICC2.&lt;/p&gt;
&lt;p&gt;Two way random, single measures, absolute (ICC2):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho = \frac{\sigma^2_r}{\sigma^2_r + \sigma^2_c + \sigma^2_e}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is the population parameter for the ICC, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_r\)&lt;/span&gt; is the row variability (variability between raters), &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_c\)&lt;/span&gt; is the column variability (variability between faces), and &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_e\)&lt;/span&gt; is the error.&lt;/p&gt;
&lt;p&gt;We’re using a two way random model because we expect variability between subjects, but also within (faces have different underlying valence and arousal). Also note that the ‘single measures’ part is referring to the fact that each rating is a single score, not an average of scores.&lt;/p&gt;
&lt;p&gt;We’ll use the &lt;em&gt;ICC&lt;/em&gt; function from the &lt;em&gt;psych&lt;/em&gt; package to compute the ICCs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)

VA_adult_icc &amp;lt;- VA_adult %&amp;gt;%
  select(-face) %&amp;gt;%
  ICC()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VA_child_icc &amp;lt;- VA_child %&amp;gt;%
  select(-face) %&amp;gt;%
  ICC()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AR_adult_icc &amp;lt;- AR_adult %&amp;gt;%
  select(-face) %&amp;gt;%
  ICC()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;AR_child_icc &amp;lt;- AR_child %&amp;gt;%
  select(-face) %&amp;gt;%
  ICC()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we’ll use the &lt;em&gt;kableExtra&lt;/em&gt; package to generate a table of the results. Note that I’m extracting the 2nd value for the ICC results because it is the ICC2. If we expected no column variability then we might use ICC1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(kableExtra)

kable(data.frame(matrix(c(VA_adult_icc$results$ICC[2], VA_child_icc$results$ICC[2],
                          AR_adult_icc$results$ICC[2], AR_child_icc$results$ICC[2]),
                   nrow = 2, ncol = 2),
                   row.names = c(&amp;quot;Adult&amp;quot;, &amp;quot;Child&amp;quot;)),
      col.names = c(&amp;quot;ICC Valence&amp;quot;, &amp;quot;ICC Arousal&amp;quot;)) %&amp;gt;%
  kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
ICC Valence
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
ICC Arousal
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Adult
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.806
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.194
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Child
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.795
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.220
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Clearly and not surprisingly, the ICCs for arousal (~.20) are much lower than those for valence (~.80). Using Cicchetti’s (1994) guidelines, we would interpret the valence ICCs as indicating excellent agreement and the arousal ICCs as poor agreement. It is also worth noting that adults and children seem equally (un)reliable in their reporting.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The findings suggest that we should give pause before attempting to interpret differences between children and adults in their overall ratings of arousal in facial expressions. Such disagreement is actually expected according to dimensional theories of emotion (Russell, 2003) because emotions are not viewed as prototypical things, and there can be wide variability in facial expressions even across similar situations. In other words, there’s no universal facial expression for high arousal (in fact, there’s little reason to believe in universality for any emotional expression).&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Cicchetti, D. V. (1994). Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology. &lt;em&gt;Psychological Assessment, 6&lt;/em&gt;, 284-290.&lt;/p&gt;
&lt;p&gt;McGraw, K. O., &amp;amp; Wong, S. P. (1996). Forming inferences about some intraclass correlation coefficients. &lt;em&gt;Psychological Methods, 1&lt;/em&gt;, 30-46.&lt;/p&gt;
&lt;p&gt;Russell, J. A. (2003). Core affect and the psychological construction of emotion. &lt;em&gt;Psychological Review, 110&lt;/em&gt;, 145-172.&lt;/p&gt;
&lt;p&gt;Shrout, P. E., &amp;amp; Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing reliability. &lt;em&gt;Psychological Bulletin, 86&lt;/em&gt;, 420-428.&lt;/p&gt;
&lt;p&gt;Vesker, M., Bahn, D., Dege, F., Kauschke, C., &amp;amp; Gudrun, S. (2018). Perceiving arousal and valence in facial expressions: Differences between children and adults. &lt;em&gt;European Journal of Developmental Psychology, 15&lt;/em&gt;, 411-425.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Plotting the Affect Circumplex in R</title>
      <link>/post/circumplex/circumplex/</link>
      <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/circumplex/circumplex/</guid>
      <description>


&lt;p&gt;I’m a strong adherent to the &lt;em&gt;circumplex&lt;/em&gt; model of emotions introduced by James Russell in the late 1980s. Russell argued that all emotional experience can be boiled down to two dimensions: valence and arousal, with valence being how positive or negative you feel and arousal being how sluggish or emotionally activated you feel. The emotions we commonly label as anger, sadness, joy, etc. can be mapped within this &lt;em&gt;affective&lt;/em&gt; two-dimensional space, such that joy is a high valence, high arousal emotion, whereas boredom is a moderately low valence and low arousal emotion.&lt;/p&gt;
&lt;p&gt;This kind of model is great in the field of emotion dynamics, where we are interested in how emotions change over time. It’s great because we don’t have to get bogged down in philosophical debates about whether someone is in a state of sadness or not, but can instead focus on quantifying and mapping changes in valence and arousal. For my doctoral dissertation, I’m using the circumplex model of emotions to explore how emotions change over time in instances when people are alone or with others. Briefly, I’m interested in whether being alone reduces arousal (i.e., makes you more calm). Some evidence in support of this is offered in a recent paper by Nguyen, Ryan, &amp;amp; Deci (2018), although they didn’t use a circumplex approach to emotions.&lt;/p&gt;
&lt;p&gt;The circumplex model shines in another way: because it models emotional states in two dimensions, it can be presented visually. This is what I’m attempting to do for my own research. So for now, I’ll simulate some data akin to what I’ll be analyzing in my dissertation, starting simply with just two time points and one condition. The data will represent participants’ valence and arousal (Likert scale of 1-7) at baseline and the same measurements one hour later. I’ll use the &lt;em&gt;simstudy&lt;/em&gt; package to generate the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(simstudy)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the code for simulating the data and binding it all together. I’m using the function &lt;em&gt;genCorGen&lt;/em&gt; to simulate and generate correlated data for valence and arousal, respectively. In this call, the params refer to the mean and standard deviations, while rho is the correlation coefficient.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(190113)

dx &amp;lt;- genCorGen(600, nvars = 2, params1 = c(4.79, 4.02), params2 = c(1.3, .9), dist = &amp;quot;normal&amp;quot;,
                rho = .67, corstr = &amp;quot;cs&amp;quot;, wide = TRUE,
                cnames = c(&amp;quot;valence1&amp;quot;, &amp;quot;valence2&amp;quot;))

dv &amp;lt;- genCorGen(600, nvars = 2, params1 = c(4.12, 3.97), params2 = c(.80, 1.2), dist = &amp;quot;normal&amp;quot;,
                rho = .43, corstr = &amp;quot;cs&amp;quot;, wide = TRUE,
                cnames = c(&amp;quot;arousal1&amp;quot;, &amp;quot;arousal2&amp;quot;))

core &amp;lt;- data.frame(round(dx), round(dv[, c(2, 3)]))

core$valence1[core$valence1 &amp;gt; 7] &amp;lt;- 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After generating the data for valence and arousal, I binded the two variables, rounded them to the nearest integer, and trimmed cases that exceeded the 7-point cut-off.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)

describe(core)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          vars   n   mean     sd median trimmed    mad min max range  skew
## id          1 600 300.50 173.35  300.5  300.50 222.39   1 600   599  0.00
## valence1    2 600   4.81   1.10    5.0    4.84   1.48   1   7     6 -0.16
## valence2    3 600   4.06   0.94    4.0    4.05   1.48   1   7     6 -0.01
## arousal1    4 600   4.14   0.92    4.0    4.15   1.48   2   7     5 -0.10
## arousal2    5 600   4.08   1.14    4.0    4.09   1.48   1   7     6 -0.05
##          kurtosis   se
## id          -1.21 7.08
## valence1    -0.12 0.04
## valence2     0.17 0.04
## arousal1    -0.26 0.04
## arousal2    -0.05 0.05&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The summary statistics check out. So now it’s time to plot the data. The function I’m quaintly calling &lt;em&gt;circumplexi&lt;/em&gt; takes four vectors as inputs (time 1 valence, time 2 valence, time 1 arousal, time 2 arousal) and returns a circumplex plot. As it stands, it’s not the most intuitive function, but it produces a decent looking plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;circumplexi &amp;lt;- function(valence_time1, valence_time2, arousal_time1, arousal_time2) {
  v1 &amp;lt;- valence_time1
  v2 &amp;lt;- valence_time2
  a1 &amp;lt;- arousal_time1
  a2 &amp;lt;- arousal_time2
  
  v1mean &amp;lt;- mean(valence_time1, na.rm = TRUE)
  v2mean &amp;lt;- mean(valence_time2, na.rm = TRUE)
  a1mean &amp;lt;- mean(arousal_time1, na.rm = TRUE)
  a2mean &amp;lt;- mean(arousal_time2, na.rm = TRUE)
  
  ggplot() +
    geom_segment(aes(x = (min(v1) + max(v1))/2, y = min(v1), xend = (min(v1) + max(v1))/2, yend = max(v1)), color = &amp;quot;gray60&amp;quot;, size = 1) +
    geom_segment(aes(x = min(v1), y = (min(v1) + max(v1))/2, xend = max(v1), yend = (min(v1) + max(v1))/2), color = &amp;quot;gray60&amp;quot;, size = 1) +
    geom_point(aes(x = a1mean, y = v1mean, size = 5, color = &amp;quot;Time 1&amp;quot;)) +
    geom_point(aes(x = a2mean, y = v2mean, size = 5, color = &amp;quot;Time 2&amp;quot;)) +
    scale_x_discrete(name = &amp;quot;arousal&amp;quot;, limits = c(min(v1):max(v1)), expand = c(0, 0)) +
    scale_y_discrete(name = &amp;quot;valence&amp;quot;, limits = c(min(v1):max(v1)), expand = c(0, 0)) +
    geom_segment(aes(x = a1mean,
                     y = v1mean, 
                     xend = a2mean,
                     yend = v2mean),
                 arrow = arrow(type = &amp;quot;closed&amp;quot;, length = unit(.125, &amp;quot;inches&amp;quot;))) +
    coord_fixed() + 
    theme_light() +
    labs(title = &amp;quot;Change in Affect from Time 1 to Time 2&amp;quot;,
         subtitle = &amp;quot;Red dot is affect at Time 1. Blue dot is affect at Time 2&amp;quot;) +
    theme(legend.position = &amp;quot;none&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;circumplexi(core$valence1, core$valence2, core$arousal1, core$arousal2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/circumplex/Affect_Circumplex_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, the plot shows that affect becomes more neutral (i.e, returns to baseline) following Time 1. In my own research, I’ll be using circumplex plots to depict this change between multiple groups as well. For now, this is a good start.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A New Way to Handle Multivariate Outliers</title>
      <link>/post/outliers/outliers/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/outliers/outliers/</guid>
      <description>


&lt;p&gt;Psychologists often have a standoffish attitude toward outliers. Developmental psychologists, in particular, seem uncomfortable with removing cases because of the challenges inherent in obtaining data in the first place. However, the process of identifying and (sometimes) removing outliers is not a witch hunt to &lt;em&gt;cleanse&lt;/em&gt; datasets of “weird” cases; rather, dealing with outliers is an important step toward solid, reproducible science. As I’ll demonstrate in this simulated example, a few outliers can completely reverse the conclusions derived from statistical analyses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)
library(tidyverse)
library(simstudy)
library(jtools)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;a-hypothetical-case&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Hypothetical Case&lt;/h2&gt;
&lt;p&gt;I’ll pretend that I have data on participants’ self-reported &lt;em&gt;affinity for aloneness&lt;/em&gt; (i.e., how much time they like being alone), &lt;em&gt;time alone&lt;/em&gt; (i.e., number of hours typically spent alone per week), and &lt;em&gt;loneliness&lt;/em&gt;. We might expect that people who spend more time alone feel more loneliness. However, if you’re the kind of person who enjoys being alone, maybe being by yourself isn’t so bad. In other words, I’m interested in the &lt;em&gt;moderating&lt;/em&gt; effect of time alone on the association between affinity for aloneness and loneliness.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generating-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generating the Data&lt;/h2&gt;
&lt;p&gt;I’ll simulate 600 cases using the &lt;em&gt;simstudy&lt;/em&gt; package. Because I want the variables correlated, I’ll specify a correlation matrix that makes theoretical sense.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c &amp;lt;- matrix(c(1, .43, .28, .43, 1, .12, .28, .12, 1), nrow = 3)
c&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,] 1.00 0.43 0.28
## [2,] 0.43 1.00 0.12
## [3,] 0.28 0.12 1.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I can use the correlation matrix when I generate the data. In the function &lt;em&gt;genCorData&lt;/em&gt;, &lt;strong&gt;mu&lt;/strong&gt; refers to the sample means and &lt;strong&gt;sigma&lt;/strong&gt; refers to their respective standard deviations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(206134)

data &amp;lt;- genCorData(600, mu = c(2.65, 3.56, 2.21), sigma = c(.56, 1.12, .70), corMatrix = c)

data &amp;lt;- data %&amp;gt;%
  select(-id) %&amp;gt;%
  rename(alone_affinity = V1, time_alone = V2, loneliness = V3)

data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      alone_affinity time_alone loneliness
##   1:       2.053861   2.880370   1.750774
##   2:       2.782888   5.131749   1.646151
##   3:       2.429589   1.488717   2.333513
##   4:       2.289647   3.711900   2.780851
##   5:       3.177230   3.629568   2.694580
##  ---                                     
## 596:       2.660343   4.055748   1.811799
## 597:       1.564866   2.921037   1.842257
## 598:       2.742394   4.205703   2.598651
## 599:       1.439261   2.065413   1.547111
## 600:       3.137692   4.936879   2.580417&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the data generated, I can take a look at the univariate and multivariate distributions in one fell swoop using the function &lt;em&gt;pairs.panels&lt;/em&gt; from the &lt;em&gt;psych&lt;/em&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairs.panels(data, stars = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/outliers/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Everything looks normal and the correlations are pretty close to the ones that I chose.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bring-in-the-outliers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bring in the Outliers!&lt;/h2&gt;
&lt;p&gt;To make this example more pathological, I’ll introduce some multivariate outliers. I won’t show the code for this, but all I’ve done is manually change 20 cases.&lt;/p&gt;
&lt;p&gt;Looking at the data again, it’s clear that the outliers have an effect. The sample correlations are still significant, but quite off the mark.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairs.panels(data_outlier, stars = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/outliers/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;model-1-all-data---including-outliers&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model 1: All Data - Including Outliers&lt;/h3&gt;
&lt;p&gt;What if we ran a linear regression on these variables? Here, I’ll run a hierarchical linear regression with the first step predicting loneliness from affinity for aloneness and time alone. The second step adds an interaction (this is the moderation I mentioned earlier).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model1 &amp;lt;- lm(loneliness ~ .*time_alone, data = data_outlier)
summary(model1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = loneliness ~ . * time_alone, data = data_outlier)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.99016 -0.48682  0.01538  0.46143  2.48231 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)                0.66138    0.36935   1.791   0.0739 .  
## alone_affinity             0.58212    0.13974   4.166 3.56e-05 ***
## time_alone                 0.24982    0.10581   2.361   0.0185 *  
## alone_affinity:time_alone -0.08935    0.03772  -2.369   0.0182 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.718 on 596 degrees of freedom
## Multiple R-squared:  0.06105,    Adjusted R-squared:  0.05632 
## F-statistic: 12.92 on 3 and 596 DF,  p-value: 3.478e-08&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Overall, affinity for aloneness and time alone both uniquely positively predict loneliness. More importantly though, the interaction is statistically significant with a &lt;em&gt;p&lt;/em&gt;-value at .018. We can visualize this more clearly with simple slopes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model1_int &amp;lt;- lm(loneliness ~ time_alone * alone_affinity, data = data_outlier)

interact_plot(model1_int, pred = &amp;quot;time_alone&amp;quot;, modx = &amp;quot;alone_affinity&amp;quot;) +
  theme_apa()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/outliers/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A pristine looking interaction plot! Our simulated data shows that at higher affinity for aloneness the association between time alone and loneliness becomes more negative. This is what was expected.&lt;/p&gt;
&lt;p&gt;If this were real data, these results are potentially publishable. What is not immediately clear though is that outliers have a severe impact on this finding. Let’s look at the simple slopes a bit differently:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;interact_plot(model1_int, pred = &amp;quot;time_alone&amp;quot;, modx = &amp;quot;alone_affinity&amp;quot;, linearity.check = TRUE) +
  theme_apa()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/outliers/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Oh dear… The assumption of linearity for these subsamples is clearly not met. It looks like some cases are skewing the associations among the high and low affinity groups.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-2---mahalanobis-distance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model 2 - Mahalanobis Distance&lt;/h3&gt;
&lt;p&gt;A popular way to identify and deal with multivariate outliers is to use Mahalanobis Distance (MD). MD calculates the distance of each case from the central mean. Larger values indicate that a case is farther from where most of the points cluster. The &lt;em&gt;psych&lt;/em&gt; package contains a function that quickly calculates and plots MDs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;outlier(data_outlier)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/outliers/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Wow, one case is way out there, you can hardly see it! Otherwise, most of the points appear to follow in line. We might prefer a more formal test of outliers by using a cut-off score for MD. Here, I’ll recalcuate the MDs using the &lt;em&gt;mahalanobis&lt;/em&gt; function and identify those that fall above the cut-off score for a chi-square with &lt;em&gt;k&lt;/em&gt; degrees of freedom (3 for 3 variables, but I’ll use &lt;em&gt;ncol&lt;/em&gt; in case I want to add or remove variables later):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;md &amp;lt;- mahalanobis(data, center = colMeans(data_outlier), cov = cov(data_outlier))

alpha &amp;lt;- .001

cutoff &amp;lt;- (qchisq(p = 1 - alpha, df = ncol(data_outlier)))

names_outliers_MH &amp;lt;- which(md &amp;gt; cutoff)

excluded_mh &amp;lt;- names_outliers_MH

data_clean_mh &amp;lt;- data_outlier[-excluded_mh, ]

data[excluded_mh, ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    alone_affinity time_alone loneliness
## 1:            4.6        1.4        4.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using this cut-off, only one outlier was identified. Not surprisingly, it’s the case with a huge MD relative to the others. Probing this simulated case closely, we see that this hypothetical individual really likes being alone, spent little time alone, and reported feeling very lonely.&lt;/p&gt;
&lt;p&gt;Now we can rerun the model with this outlier omitted:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model2 &amp;lt;- lm(loneliness ~ .*time_alone, data = data_clean_mh)
summary(model2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = loneliness ~ . * time_alone, data = data_clean_mh)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.98403 -0.48734  0.01331  0.45859  2.48196 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)                0.79882    0.37897   2.108 0.035461 *  
## alone_affinity             0.52131    0.14476   3.601 0.000343 ***
## time_alone                 0.21964    0.10738   2.045 0.041259 *  
## alone_affinity:time_alone -0.07595    0.03861  -1.967 0.049624 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.7171 on 595 degrees of freedom
## Multiple R-squared:  0.05384,    Adjusted R-squared:  0.04907 
## F-statistic: 11.29 on 3 and 595 DF,  p-value: 3.289e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The interaction is still significant, but just barely, with a &lt;em&gt;p&lt;/em&gt;-value of .049.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-3---minimum-covariance-determinant&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model 3 - Minimum Covariance Determinant&lt;/h3&gt;
&lt;p&gt;Is this enough to conclude that the data supports the model? Many would probably be content to stop here, but we haven’t adequately dealt with the outlier infestation. This demonstrates the fallability of MD, which Leys et al. (2018) argue is not a robust way to determine outliers. The problem lies with the fact that MD uses the means and covariances of all the data - including the outliers - and bases the individual difference scores from these values. If we’re really interested in identifying cases that stray from the pack, it makes more sense to base the criteria for removal using &lt;em&gt;a subset of the data that is the most central&lt;/em&gt;. This is the idea behind &lt;strong&gt;Minimum Covariance Determinant&lt;/strong&gt;, which calculates the mean and covariance matrix based on the most central subset of the data.&lt;/p&gt;
&lt;p&gt;We’ll use this to calculate new distance scores from a 75% subset of the data that is highly central. For this, we need the &lt;em&gt;MASS&lt;/em&gt; package. The approach for calculating the distance scores is similar, and we can use the same cut-off score as before.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MASS)

output75 &amp;lt;- cov.mcd(data_outlier, quantile.used = nrow(data_outlier)*.75)

mhmcd75 &amp;lt;- mahalanobis(data_outlier, output75$center, output75$cov)

names_outlier_MCD75 &amp;lt;- which(mhmcd75 &amp;gt; cutoff)

excluded_mcd75 &amp;lt;- names_outlier_MCD75

data_clean_mcd &amp;lt;- data_outlier[-excluded_mcd75, ]

data_outlier[excluded_mcd75, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach identified 9 outliers, as opposed to the 1 identified with the traditional MD. Let’s see whether removing these cases changes the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model3 &amp;lt;- lm(loneliness ~ .*time_alone, data = data_clean_mcd)
summary(model3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = loneliness ~ . * time_alone, data = data_clean_mcd)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.9695 -0.4725  0.0168  0.4519  2.5129 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)                1.18154    0.39213   3.013   0.0027 **
## alone_affinity             0.36392    0.15217   2.391   0.0171 * 
## time_alone                 0.08494    0.11128   0.763   0.4456   
## alone_affinity:time_alone -0.02316    0.04057  -0.571   0.5683   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.7064 on 587 degrees of freedom
## Multiple R-squared:  0.05802,    Adjusted R-squared:  0.0532 
## F-statistic: 12.05 on 3 and 587 DF,  p-value: 1.153e-07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow. Removing 9 data points was enough to decimate the significance of the interaction - the &lt;em&gt;p&lt;/em&gt;-value is now .568. This is clearly demonstrated in the simple slopes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model3_int &amp;lt;- lm(loneliness ~ time_alone * alone_affinity, data = data_clean_mcd)

interact_plot(model3_int, pred = &amp;quot;time_alone&amp;quot;, modx = &amp;quot;alone_affinity&amp;quot;) +
  theme_apa()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/outliers/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Of course, this would be a disappointing realization for any researcher. We do see, however, that the correlations are better estimated now that these outliers are removed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pairs.panels(data_clean_mcd, stars = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/outliers/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This simulation was a pathological (but realistic) example of how outliers can dramatically skew results, even with reasonably large samples. The Minimum Covariance Determinant version of MD is a more robust method of identifying and removing outliers that would otherwise go unnoticed with traditional MD.&lt;/p&gt;
&lt;p&gt;Many researchers in psychology are uncomfortable with removing outliers because they worry about losing statistical power. Others feel that removing outliers is in some way dissociating their data from reality because “in the real world, there are outliers - people are different!”. Although true, the argument shouldn’t be about whether outliers exist or not, but how much they impact the conclusions we draw from our data. In this simulation, we saw that a difference of 8 cases out of 600 was enough to turn a non-significant result significant. If our goal is to generalize our findings to a larger population, it would be foolish to do so on the basis of 8 outlying cases.&lt;/p&gt;
&lt;p&gt;The article by Leys et al. (2018) offers suggestions about how to approach outliers. Ideally, a researcher should pre-register their plan for handling outliers. In a post-hoc situation, they advise publishing results with and without outliers. At the very least, we should be acknowledging outliers, rather than pretending the don’t exist.&lt;/p&gt;
&lt;p&gt;As a final note, I highly recommend reading the article by Leys et al. (2018). It provides a better theoeretical grasp of MD and MCD. Some of the code used in this example (specifically, the codes for calculating MD and MCD) was used from their article. See below for the full reference.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Leys, C., Klein, O., Dominicy, Y., &amp;amp; Ley, C. (2018). Detecting multivariate outliers: Use a robust variant of Mahalanobis distance. &lt;em&gt;Journal of Experimental Social Psychology, 74&lt;/em&gt;, 150-156.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Emotion Dynamics of Solitude</title>
      <link>/project/emotion-dynamics/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>/project/emotion-dynamics/</guid>
      <description>&lt;p&gt;We often think of emotions as being there one moment and gone the next. However, we are always experiencing some sort of emotion - whether it&amp;rsquo;s boiling anger or vapid listlessness. The study of emotion dynamics is all about how emotions change over time. Emotions can change from one moment to the next - intensifying or deintensifying. I&amp;rsquo;m interested in exploring what predicts these complex changes. In particular, I&amp;rsquo;m interested in whether solitude impacts our emotional state.&lt;/p&gt;

&lt;p&gt;Extending this further, we can describe change in emotion over &lt;em&gt;developmental&lt;/em&gt; time as well. For instance, we know that adolescents often experience more intense emotions compared to adults. My work attempts to explore emotion dynamics across multiple time scales.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Emotion Regulation</title>
      <link>/project/emotion-regulation/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>/project/emotion-regulation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>R</title>
      <link>/project/r/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>/project/r/</guid>
      <description>&lt;p&gt;R is a powerful tool for data science, which is why more and more researchers are turning to R for their analyses. I&amp;rsquo;m interested in using R for a variety of purposes, including (but not limited to) text analysis, linear mixed models, Monte Carlo simulations, and data visualization. I blog relatively frequently about using R to solve analytical problems in psychology.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment Analysis</title>
      <link>/project/sentiment-analysis/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>/project/sentiment-analysis/</guid>
      <description>&lt;p&gt;In the era of &lt;em&gt;Big Data&lt;/em&gt;, we are awash in online textual data, such as Tweets, Google Books, and countless blogs. These sources represent a wealth of data. Sentiment analysis is a technique that automatically assesses the emotional content of text. In partnership with &lt;a href=&#34;http://saifmohammad.com/&#34; target=&#34;_blank&#34;&gt;Dr. Saif Mohammad&lt;/a&gt; of the National Research Council Canada, I&amp;rsquo;m exploring developmental trends in emotions from thousands of online poetry submitted by children and adolescents. The goal of this project is to provide insight into how emotions change over &lt;em&gt;developmental&lt;/em&gt; time. The &lt;em&gt;Children&amp;rsquo;s Poems&lt;/em&gt; (Hipson &amp;amp; Mohammad, 2019) corpus is freely available on my &lt;a href=&#34;https://github.com/whipson?tab=repositories&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Social Development in Different Cultures</title>
      <link>/project/social-development-in-different-cultures/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>/project/social-development-in-different-cultures/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Social Withdrawal and Socio-emotional Development</title>
      <link>/project/social-withdrawal-and-socio-emotional-development/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 -0500</pubDate>
      
      <guid>/project/social-withdrawal-and-socio-emotional-development/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-regulation, learning problems, and maternal authoritarian parenting in Chinese children: A developmental cascades model</title>
      <link>/publication/liu-et-al.-2018/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 -0400</pubDate>
      
      <guid>/publication/liu-et-al.-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 -0400</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linking shyness with social and school adjustment in early childhood: The moderating role of inhibitory control</title>
      <link>/publication/sette-et-al.-2018/</link>
      <pubDate>Mon, 08 Jan 2018 00:00:00 -0500</pubDate>
      
      <guid>/publication/sette-et-al.-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-control, peer preference, and loneliness in Chinese children: A three-year longitudinal study</title>
      <link>/publication/liu-et-al.-2016/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 -0400</pubDate>
      
      <guid>/publication/liu-et-al.-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Maternal agreeableness moderates associations between young children’s emotion dysregulation and socio-emotional functioning at school</title>
      <link>/publication/hipson-et-al.-2017/</link>
      <pubDate>Wed, 22 Feb 2017 00:00:00 -0500</pubDate>
      
      <guid>/publication/hipson-et-al.-2017/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
