[{"authors":null,"categories":["R","Emotion Dynamics"],"content":"\rEmotion dynamics is the study of how emotions change over time. Sometimes our feelings are quite stable, but other times capricious. Measuring and predicting these patterns for different people is somewhat of a Holy Grail for emotion researchers. In particular, some researchers are aspiring to discover mathematical laws that capture the complexity of our inner emotional experiences - much like physicists divining the laws that govern objects in the natural environment. These discoveries would revolutionize our understanding of our everyday feelings and when our emotions can go awry.\nThis series of blog posts, which I kicked off earlier this month with a simulation of emotions during basketball games, is inspired by researchers like Peter Kuppens and Tom Hollenstein (to name a few) who have collected and analyzed reams of intensive self-reports on people’s feelings from one moment to the next. My approach is to reverse engineer these insights and generate models that simulate emotions evolving over time - like this:\nAffective State Space\rWe start with the affective state space - the theoretical landscape on which our conscious feelings roam free. This space is represented as two-dimensional, although we acknowledge that this fails to capture all aspects of conscious feeling. The first dimension, represented along the x-axis, is valence and this refers to how unpleasant vs. pleasant we feel. The second dimension, represented along the y-axis, is arousal. Somewhat less intuitive, arousal refers to how deactivated/sluggish/sleepy vs. activated/energized/awake we feel. At any time, our emotional state can be defined in terms of valence and arousal. So if you’re feeling stressed you would be low in valence and high in arousal. Let’s say you’re serene and calm, then you would be high in valence and low in arousal. Most of the time, we feel moderately high valence and moderate arousal (i.e., content), but if you’re the type of person who is chronically stressed, this would be different.\nThis is all well and good when we think about how we’re feeling right now, but it’s also worth considering how our emotions are changing. On a regular day, our emotions undergo minor fluctuations - sometimes in response to minor hassles or victories, and sometimes for no discernible reason. In this small paragraph, I’ve laid out a number of parameters, all of which vary between different people:\n\rAttractor: Our typical emotional state. At any given moment, our feelings are pulled toward this state. Some people tend to be happier, whereas others are less happy.\rStability: How emotionally stable one is. Some people are more emotionally stable than others. Even in the face of adversity, an emotionally stable person keeps their cool.\rDispersion: The range of our emotional landscape. Some people experience intense highs and lows, whereas others persist somewhere in the middle.\r\rWe’ll keep all of this in mind for the simulation. We’ll start with a fairly simple simulation with 100 hypothetical people. We’ll need the following packages.\nlibrary(psych)\rlibrary(tidyverse)\rlibrary(sn)\rAnd then we’ll create a function that performs the simulation. Note that each person i has their own attractor, recovery rate, stability, and dispersion. For now we’ll just model random fluctuations in emotions, a sort of Brownian motion. You can imagine our little simulatons (fun name for the hypothetical people in the simulation) sitting around on an average day doing nothing in particular.\nsimulate_affect \u0026lt;- function(n = 2, time = 250, negative_event_time = NULL) {\rdt \u0026lt;- data.frame(matrix(nrow = time, ncol = 1))\rcolnames(dt) \u0026lt;- \u0026quot;time\u0026quot;\rdt$time \u0026lt;- 1:time\rvalence \u0026lt;- data.frame(matrix(nrow = time, ncol = 0))\rarousal \u0026lt;- data.frame(matrix(nrow = time, ncol = 0))\rfor(i in 1:n) {\rattractor_v \u0026lt;- rnorm(1, mean = 3.35, sd = .75)\rinstability_v \u0026lt;- sample(3:12, 1, replace = TRUE, prob = c(.18, .22, .18, .15, .8, .6, .5, .4, .2, .1))\rdispersion_v \u0026lt;- abs(rsn(1, xi = .15, omega = .02, alpha = -6) * instability_v) #rsn simulates a skewed distribution.\rif(!is.null(negative_event_time)) {\rrecovery_rate \u0026lt;- sample(1:50, 1, replace = TRUE) + negative_event_time\rnegative_event \u0026lt;- (dt$time %in% negative_event_time:recovery_rate) * seq.int(50, 1, -1)\r}\relse {\rnegative_event \u0026lt;- 0\r}\rvalence[[i]] \u0026lt;- ksmooth(x = dt$time,\ry = (negative_event * -.10) + arima.sim(list(order = c(1, 0, 0),\rar = .50),\rn = time),\rbandwidth = time/instability_v, kernel = \u0026quot;normal\u0026quot;)$y * dispersion_v + attractor_v #instability is modelled in the bandwidth term of ksmooth, such that higher instability results in higher bandwidth (greater fluctuation). #dispersion scales the white noise (arima) parameter, such that there are higher peaks and troughs at higher dispersion.\rattractor_a \u0026lt;- rnorm(1, mean = .50, sd = .75) + sqrt(instability_v) #arousal attractor is dependent on instability. This is because high instability is associated with higher arousal states.\rinstability_a \u0026lt;- instability_v + sample(-1:1, 1, replace = TRUE)\rdispersion_a \u0026lt;- abs(rsn(1, xi = .15, omega = .02, alpha = -6) * instability_a)\rarousal[[i]] \u0026lt;- ksmooth(x = dt$time,\ry = (negative_event * .075) + arima.sim(list(order = c(1, 0, 0),\rar = .50),\rn = time),\rbandwidth = time/instability_a, kernel = \u0026quot;normal\u0026quot;)$y * dispersion_a + attractor_a\r}\rvalence[valence \u0026gt; 6] \u0026lt;- 6\rvalence[valence \u0026lt; 0] \u0026lt;- 0\rarousal[arousal \u0026gt; 6] \u0026lt;- 6\rarousal[arousal \u0026lt; 0] \u0026lt;- 0\rcolnames(valence) \u0026lt;- paste0(\u0026quot;valence_\u0026quot;, 1:n)\rcolnames(arousal) \u0026lt;- paste0(\u0026quot;arousal_\u0026quot;, 1:n)\rdt \u0026lt;- cbind(dt, valence, arousal)\rreturn(dt)\r}\rset.seed(190625)\remotions \u0026lt;- simulate_affect(n = 100, time = 300)\remotions %\u0026gt;%\rselect(valence_1, arousal_1) %\u0026gt;%\rhead()\r## valence_1 arousal_1\r## 1 1.328024 5.380643\r## 2 1.365657 5.385633\r## 3 1.401849 5.390470\r## 4 1.436284 5.395051\r## 5 1.468765 5.399162\r## 6 1.499062 5.402752\rSo we see the first six rows for participant 1’s valence and arousal. But if we want to plot these across multiple simulatons, we need to wrangle the data into long form. We’ll also compute some measures of within-person deviation. The Root Mean Square Successive Difference (RMSSD) takes into account gradual shifts in the mean. Those who are more emotionally unstable will have a higher RMSSD. For two dimensions (valence and arousal) we’ll just compute the mean RMSSD.\nemotions_long \u0026lt;- emotions %\u0026gt;%\rgather(key, value, -time) %\u0026gt;%\rseparate(key, into = c(\u0026quot;dimension\u0026quot;, \u0026quot;person\u0026quot;), sep = \u0026quot;_\u0026quot;) %\u0026gt;%\rspread(dimension, value) %\u0026gt;%\rgroup_by(person) %\u0026gt;%\rmutate(rmssd_v = rmssd(valence),\rrmssd_a = rmssd(arousal),\rrmssd_total = mean(rmssd_v + rmssd_a)) %\u0026gt;%\rungroup()\rLet’s see what this looks like for valence and arousal individually.\nemotions_long %\u0026gt;%\rggplot(aes(x = time, y = valence, group = person, color = rmssd_v)) +\rgeom_line(size = .75, alpha = .75) +\rscale_color_gradient2(low = \u0026quot;black\u0026quot;, mid = \u0026quot;grey\u0026quot;, high = \u0026quot;red\u0026quot;, midpoint = median(emotions_long$rmssd_v)) +\rlabs(x = \u0026quot;Time\u0026quot;,\ry = \u0026quot;Valence\u0026quot;,\rcolor = \u0026quot;Instability\u0026quot;,\rtitle = \u0026quot;Simulated Valence Scores over Time for 100 People\u0026quot;) +\rtheme_minimal(base_size = 16)\remotions_long %\u0026gt;%\rggplot(aes(x = time, y = arousal, group = person, color = rmssd_a)) +\rgeom_line(size = .75, alpha = .75) +\rscale_color_gradient2(low = \u0026quot;black\u0026quot;, mid = \u0026quot;grey\u0026quot;, high = \u0026quot;red\u0026quot;, midpoint = median(emotions_long$rmssd_a)) +\rlabs(x = \u0026quot;Time\u0026quot;,\ry = \u0026quot;Arousal\u0026quot;,\rcolor = \u0026quot;Instability\u0026quot;,\rtitle = \u0026quot;Simulated Arousal Scores over Time for 100 People\u0026quot;) +\rtheme_minimal(base_size = 16)\rWe see that some lines are fairly flat and others fluctuate more widely. More importantly, most people are somewhere in the middle.\nWe can get a sense of one simulated person’s affective state space as well. The goal here is to mimic the kinds of models shown in Kuppens, Oravecz, and Tuerlinckx (2010):\nemotions_long %\u0026gt;%\rfilter(person %in% sample(1:100, 6, replace = FALSE)) %\u0026gt;%\rggplot(aes(x = valence, y = arousal, group = person)) +\rgeom_path(size = .75) + scale_x_continuous(limits = c(0, 6)) +\rscale_y_continuous(limits = c(0, 6)) +\rlabs(x = \u0026quot;Valence\u0026quot;,\ry = \u0026quot;Arousal\u0026quot;,\rtitle = \u0026quot;Affective State Space for Six Randomly Simulated People\u0026quot;) +\rfacet_wrap(~person) +\rtheme_minimal(base_size = 18) +\rtheme(plot.title = element_text(size = 18, hjust = .5))\r\rAnimating the Affective State Space\rTo really appreciate what’s going on, we need to animate this over time. I’ll add some labels to the affective state space so that it’s easier to interpret what one might be feeling at that time. I’ll also add color to show which individuals are more unstable according to RMSSD.\nlibrary(gganimate)\rp \u0026lt;- emotions_long %\u0026gt;%\rggplot(aes(x = valence, y = arousal, color = rmssd_total)) +\rannotate(\u0026quot;text\u0026quot;, x = c(1.5, 4.5, 1.5, 4.5), y = c(1.5, 1.5, 4.5, 4.5), label = c(\u0026quot;Gloomy\u0026quot;, \u0026quot;Calm\u0026quot;, \u0026quot;Anxious\u0026quot;, \u0026quot;Happy\u0026quot;),\rsize = 10, alpha = .50) + annotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = 3, ymin = 0, ymax = 3, alpha = 0.25, color = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;) +\rannotate(\u0026quot;rect\u0026quot;, xmin = 3, xmax = 6, ymin = 0, ymax = 3, alpha = 0.25, color = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;) +\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = 3, ymin = 3, ymax = 6, alpha = 0.25, color = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;) +\rannotate(\u0026quot;rect\u0026quot;, xmin = 3, xmax = 6, ymin = 3, ymax = 6, alpha = 0.25, color = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;) +\rgeom_point(size = 3.5) +\rscale_color_gradient2(low = \u0026quot;black\u0026quot;, mid = \u0026quot;grey\u0026quot;, high = \u0026quot;red\u0026quot;, midpoint = median(emotions_long$rmssd_total)) +\rscale_x_continuous(limits = c(0, 6)) +\rscale_y_continuous(limits = c(0, 6)) +\rlabs(x = \u0026quot;Valence\u0026quot;,\ry = \u0026quot;Arousal\u0026quot;,\rcolor = \u0026quot;Instability\u0026quot;,\rtitle = \u0026#39;Time: {round(frame_time)}\u0026#39;) +\rtransition_time(time) +\rtheme_minimal(base_size = 18)\rani_p \u0026lt;- animate(p, nframes = 320, end_pause = 20, fps = 16, width = 550, height = 500)\rani_p\r\rThere’s a Storm Coming…\rOur simulation does a pretty good job at emulating the natural ebb and flow of emotions, but we know that emotions can be far more volatile. Let’s subject our simulation to a negative event. Perhaps all 100 simulatons co-authored a paper that just got rejected. In the function simulate_affect, there’s an optional argument negative_event_time that causes a negative event to occur at the specified time. For this, we need to consider one more emotion dynamics parameter:\n\rRecovery rate: How quickly one recovers from an emotional event. If something bad happens, how long does it take to return to the attractor. You can see how I’ve modelled this parameter in the function above.\r\rSo we’ll run the simulation with a negative event arising at t = 150. The negative event will cause a downward spike in valence and an upward spike in arousal.\nemotions_event \u0026lt;- simulate_affect(n = 100, time = 300, negative_event_time = 150)\remotions_event_long \u0026lt;- emotions_event %\u0026gt;%\rgather(key, value, -time) %\u0026gt;%\rseparate(key, into = c(\u0026quot;dimension\u0026quot;, \u0026quot;person\u0026quot;), sep = \u0026quot;_\u0026quot;) %\u0026gt;%\rspread(dimension, value) %\u0026gt;%\rgroup_by(person) %\u0026gt;%\rmutate(rmssd_v = rmssd(valence),\rrmssd_a = rmssd(arousal),\rrmssd_total = mean(rmssd_v + rmssd_a)) %\u0026gt;%\rungroup()\remotions_event_long %\u0026gt;%\rggplot(aes(x = time, y = valence, group = person, color = rmssd_v)) +\rgeom_line(size = .75, alpha = .75) +\rscale_color_gradient2(low = \u0026quot;black\u0026quot;, mid = \u0026quot;grey\u0026quot;, high = \u0026quot;red\u0026quot;, midpoint = median(emotions_event_long$rmssd_v)) +\rlabs(x = \u0026quot;Time\u0026quot;,\ry = \u0026quot;Valence\u0026quot;,\rcolor = \u0026quot;Instability\u0026quot;,\rtitle = \u0026quot;Simulated Valence Scores over Time for 100 People\u0026quot;) +\rtheme_minimal(base_size = 16)\remotions_event_long %\u0026gt;%\rggplot(aes(x = time, y = arousal, group = person, color = rmssd_a)) +\rgeom_line(size = .75, alpha = .75) +\rscale_color_gradient2(low = \u0026quot;black\u0026quot;, mid = \u0026quot;grey\u0026quot;, high = \u0026quot;red\u0026quot;, midpoint = median(emotions_event_long$rmssd_a)) +\rlabs(x = \u0026quot;Time\u0026quot;,\ry = \u0026quot;Arousal\u0026quot;,\rcolor = \u0026quot;Instability\u0026quot;,\rtitle = \u0026quot;Simulated Arousal Scores over Time for 100 People\u0026quot;) +\rtheme_minimal(base_size = 16)\rIt’s pretty clear that something bad happened. Of course, some of our simulatons are unflappable, but most experienced a drop in valence and spike in arousal that we might identify as anxiety. Again, let’s visualize this evolving over time. Pay close attention to when the timer hits 150.\np2 \u0026lt;- emotions_event_long %\u0026gt;%\rggplot(aes(x = valence, y = arousal, color = rmssd_total)) +\rannotate(\u0026quot;text\u0026quot;, x = c(1.5, 4.5, 1.5, 4.5), y = c(1.5, 1.5, 4.5, 4.5), label = c(\u0026quot;Gloomy\u0026quot;, \u0026quot;Calm\u0026quot;, \u0026quot;Anxious\u0026quot;, \u0026quot;Happy\u0026quot;),\rsize = 10, alpha = .50) + annotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = 3, ymin = 0, ymax = 3, alpha = 0.25, color = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;) +\rannotate(\u0026quot;rect\u0026quot;, xmin = 3, xmax = 6, ymin = 0, ymax = 3, alpha = 0.25, color = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;) +\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = 3, ymin = 3, ymax = 6, alpha = 0.25, color = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;) +\rannotate(\u0026quot;rect\u0026quot;, xmin = 3, xmax = 6, ymin = 3, ymax = 6, alpha = 0.25, color = \u0026quot;black\u0026quot;, fill = \u0026quot;white\u0026quot;) +\rgeom_point(size = 3.5) +\rscale_color_gradient2(low = \u0026quot;black\u0026quot;, mid = \u0026quot;grey\u0026quot;, high = \u0026quot;red\u0026quot;, midpoint = median(emotions_event_long$rmssd_total)) +\rscale_x_continuous(limits = c(0, 6)) +\rscale_y_continuous(limits = c(0, 6)) +\rlabs(x = \u0026quot;Valence\u0026quot;,\ry = \u0026quot;Arousal\u0026quot;,\rcolor = \u0026quot;Instability\u0026quot;,\rtitle = \u0026#39;Time: {round(frame_time)}\u0026#39;) +\rtransition_time(time) +\rtheme_minimal(base_size = 18)\rani_p2 \u0026lt;- animate(p2, nframes = 320, end_pause = 20, fps = 16, width = 550, height = 500)\rani_p2\rThe overall picture is that some are more emotionally resilient than others. As of now, all the simulatons return to their baseline attractor, but we would realistically expect some to stay stressed or gloomy following bad news. In the coming months I’ll be looking into how to incorporate emotion regulation into the simulation. For example, maybe some of the simulatons use better coping strategies than others? I’m also interested in incorporating appraisal mechanisms that allow for different reactions depending on the type of emotional stimulus.\n\rReferences\rKuppens, P., Oravecz, Z., \u0026amp; Tuerlinckx, F. (2010). Feelings change: Accounting for individual differences in the temporal dynamics of affect. Journal of Personality and Social Psychology, 99, 1042-1060\n\r","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561507200,"objectID":"6b413020ef7c50b5eba0676c107e7b50","permalink":"/post/emotion-simulation1/emotion_sim/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/post/emotion-simulation1/emotion_sim/","section":"post","summary":"Emotion dynamics is the study of how emotions change over time. Sometimes our feelings are quite stable, but other times capricious. Measuring and predicting these patterns for different people is somewhat of a Holy Grail for emotion researchers. In particular, some researchers are aspiring to discover mathematical laws that capture the complexity of our inner emotional experiences - much like physicists divining the laws that govern objects in the natural environment.","tags":["R"],"title":"A Model and Simulation of Emotion Dynamics","type":"post"},{"authors":null,"categories":["R","Emotion Dynamics"],"content":"\rSporting events host witness to a wide range of human emotion. The emotional ups and downs are especially clear among invested fans. Fans experience the joy and excitement of a triumphant comeback, or the anxiety and disappointment of a loss. It is particularly interesting to see how emotions differ from two opposing fan groups watching the same match.\nI decided to perform some simulations on how a crowd of fans would react during a basketball game. Why basketball? Two reasons: First is the frequency of scoring (more baskets = more reactive simulation), and second is that the NBA finals are in swing at time of writing.\nBelow I’ve described the parameters of the simulation in detail and provided reproducible code for all examples, but here’s the quick and dirty. I’m simulating fans’ happiness and nerves (valence and arousal). Happiness reflects baskets scored (up for team basket and down for enemy basket) and overall team performance. Nerves reflects baskets scored, time remaining in game, and the difference between score (higher for closer game). There are other parameters involved that are described in more detail below.\nLet’s run the simulation! I’m simulating a short game that’s about 16 minutes long, but the simulation is sped up. There are 25 fans per team with each dot representing a fan. Blue dots root for the blue team and red dots for red. In this game, blue starts ahead, but red makes a rallying comeback.\nWhat are we seeing here? As the game progresses, fans move into different emotional states. I’ve used the terms content, disappointed, nervous, and excited as helpful placeholders, but we should think of these in terms of varying along dimensions of valence (e.g., happiness) and arousal (e.g., nerves, excitedness). For example, when their team is doing well, fans are happier, but they’re only excited when the score is close. For each basket, there’s a brief spike in arousal and happiness. As the game draws closer to an end, fans get more excited/anxious, especially if the score is close.\nSo how would this play out if one team decimated the other? Let’s give the red team a slight edge over blue:\nNotice how ‘excited’ becomes ‘content’ and ‘nervous’ turns to ‘disappointment’ because the difference becomes insurmountable for the blue team. We can plug in any numbers for this function to produce different results. However, the algorithm is highly sensitive to small changes in scoring chance because this is calculated for every second of the game.\nGoing Behind the Simulation\rSkip this if you have no interest in the psychology of emotions. What I’m striving to simulate are the laws of emotion dynamics (Kuppens \u0026amp; Verduyn, 2017). Emotions change from moment to moment, but there’s also some stability from one moment to the next. Apart from when a basket is scored, most fans cluster around a particular state (this is called an attractor state). Any change is attributable to random fluctuations (e.g., one fan spills some of their beer, maybe another fan sees an amusing picture of a cat on their phone). When a basket is scored, this causes a temporary fluctuation away from the attractor state, after which people resort back to their attractor. More gradual situational factors result in small changes in attractors too. This is why arousal tends to increase with closer games and when the game is approaching the end.\nDetailed Aspects of Simulation and Code\rThere are two parts to this simulation. Part 1 simulates a basketball game for a specified duration of time (n_seconds) and specified probabilities of scoring. The simulation is also designed with a small post game period - sort of like a cool down after the game. During the post game, fans’ arousal returns to baseline.\nThe second and more complicated part of the simulation is the emotional part. Each fan has a score on valence and arousal at each second. These can be further broken down into fixed effects and random effects. The fixed effect for valence is described as follows:\n\\[Valence = 3 + \\beta_{1overall.difference}(.20) + \\beta_{2recent.score(1.5 + \\frac{current.time}{total.time})} - \\beta_{3recent.enemy.score}\\]\nThe fixed effect for arousal looks like this:\n\\[Arousal = 3 + \\beta_{1 (\\frac{current.time}{total.time})(.75)(post.game)} + \\beta_{2(1 - \\sqrt{|difference|})} + \\beta_{3|recent.basket|}\\]\nThe exact values for these coefficients are somewhat arbitrary. They resulted from a lot of trial and error to identify which best simulated emotion dynamics.\nEach fan’s valence and arousal is calculated using the above equations + variability. The variability is added by applying random variation to the first constant in the above equations and white noise with a simulated ARIMA model. On top of this, I used a kernel regression smoother to smooth out each fan’s trajectory. To simulate delayed reaction time to baskets, I added some lags at random intervals - otherwise the simulated fans appear to react quicker than the score board. Here’s what all of this looks like for one fan’s valence over time:\nlibrary(tidyverse)\rgame_long %\u0026gt;%\rfilter(fan == 1,\rteam == \u0026quot;team1\u0026quot;) %\u0026gt;%\rggplot(aes(x = second, y = valence)) +\rgeom_line(color = \u0026quot;red\u0026quot;, size = 1) +\rtheme_minimal(base_size = 16) +\rlabs(title = \u0026quot;Fan One\u0026#39;s Valence over Time\u0026quot;,\rx = \u0026quot;Time\u0026quot;,\ry = \u0026quot;Valence\u0026quot;) +\rtheme(legend.position = \u0026quot;none\u0026quot;,\raxis.title.x = element_text(size = 18),\raxis.title.y = element_text(size = 18),\rplot.title = element_text(size = 18, hjust = .5))\rWe can see this across all fans as well:\ngame_long %\u0026gt;%\rggplot(aes(x = instance, color = team)) +\rgeom_line(aes(y = valence), alpha = .65) +\rlabs(title = \u0026quot;Valence over Time\u0026quot;,\rx = \u0026quot;Time\u0026quot;,\ry = \u0026quot;Valence\u0026quot;) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;none\u0026quot;,\raxis.title.x = element_text(size = 18),\raxis.title.y = element_text(size = 18),\rplot.title = element_text(size = 18, hjust = .5))\rAnd for arousal…\ngame_long %\u0026gt;%\rggplot(aes(x = instance, color = team)) +\rgeom_line(aes(y = arousal), alpha = .65) +\rlabs(title = \u0026quot;Arousal over Time\u0026quot;,\rx = \u0026quot;Time\u0026quot;,\ry = \u0026quot;Arousal\u0026quot;) +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;none\u0026quot;,\raxis.title.x = element_text(size = 18),\raxis.title.y = element_text(size = 18),\rplot.title = element_text(size = 18, hjust = .5))\rFor those interested, here’s the full code for the simulation and first animation. Have fun with it! I’m also looking for ways to improve it. Ironically, I don’t watch a ton of sports, so if you think I’m open to input on how to make it more realistic!\nlibrary(tidyverse)\rlibrary(forecast)\rlibrary(gganimate)\rlibrary(extrafont)\rgame_simulation \u0026lt;- function(n_seconds = 1000, n_fans = 25, team1_prob = 0.012, team2_prob = 0.012) {\rscore_board \u0026lt;- data.frame(matrix(nrow = n_seconds, ncol = 5))\rcolnames(score_board) \u0026lt;- c(\u0026quot;second\u0026quot;, \u0026quot;team1_score\u0026quot;, \u0026quot;team2_score\u0026quot;, \u0026quot;difference\u0026quot;, \u0026quot;end_game\u0026quot;)\rscore_board$second \u0026lt;- c(1:n_seconds)\rscore_board$team1_score \u0026lt;- cumsum(sample(c(0, 1, 1.5), n_seconds, replace = TRUE, prob = c(1 - team1_prob, team1_prob * .90, team1_prob * .10)))\rscore_board$team2_score \u0026lt;- cumsum(sample(c(0, 1, 1.5), n_seconds, replace = TRUE, prob = c(1 - team2_prob, team2_prob * .90, team2_prob * .10)))\rscore_board$difference \u0026lt;- score_board$team1_score - score_board$team2_score\rscore_board$end_game = 1\rtotal_time \u0026lt;- n_seconds + n_seconds * .05\rend_time \u0026lt;- data.frame(max(score_board$second + 1):(total_time))\rcolnames(end_time) \u0026lt;- \u0026quot;second\u0026quot;\rend_state \u0026lt;- score_board[nrow(score_board), 2:5]\rpost_game \u0026lt;- data.frame(matrix(nrow = n_seconds * .05, ncol = 0),\r\u0026quot;team1_score\u0026quot; = end_state$team1_score,\r\u0026quot;team2_score\u0026quot; = end_state$team2_score,\r\u0026quot;difference\u0026quot; = end_state$difference,\r\u0026quot;end_game\u0026quot; = 0)\rpost_game \u0026lt;- cbind(end_time, post_game)\rscore_board \u0026lt;- rbind(score_board, post_game)\rvalence \u0026lt;- function(time, overall_difference, recent_team_score, recent_enemy_score) (\r(overall_difference * .20) + (recent_team_score * (1.5 + time/length(time))) - (recent_enemy_score)\r)\rarousal \u0026lt;- function(time, difference, recent_score, end_game) (\r((time/n_seconds) * .75 * end_game) + (1 - sqrt(abs(difference))) + abs(recent_score * 1.5)\r)\rteam1_valence \u0026lt;- data.frame(matrix(nrow = total_time, ncol = 0))\rteam1_arousal \u0026lt;- data.frame(matrix(nrow = total_time, ncol = 0))\rteam2_valence \u0026lt;- data.frame(matrix(nrow = total_time, ncol = 0))\rteam2_arousal \u0026lt;- data.frame(matrix(nrow = total_time, ncol = 0))\rfor(i in 1:n_fans) {\rteam1_valence[[i]] \u0026lt;- ksmooth(x = score_board$second,\ry = rnorm(n = 1, mean = 3, sd = .25) + (arima.sim(list(order = c(1, 0, 1), ma = -.1, ar = c(.9)), n = total_time) * .25) + valence(time = score_board$second,\roverall_difference = score_board$difference,\rrecent_team_score = lag(score_board$team1_score, n = sample(c(1:5), 1), default = 0) - lag(score_board$team1_score, n = sample(c(6:8), 1), default = 0),\rrecent_enemy_score = lag(score_board$team2_score, n = sample(c(1:5), 1), default = 0) - lag(score_board$team2_score, n = sample(c(6:8), 1), default = 0)),\rbandwidth = total_time/100, kernel = \u0026quot;normal\u0026quot;)$y\r}\rcolnames(team1_valence) \u0026lt;- paste0(\u0026quot;team1_\u0026quot;, \u0026quot;valence_\u0026quot;, 1:n_fans)\rteam1_valence[team1_valence \u0026gt; 6] \u0026lt;- 6\rteam1_valence[team1_valence \u0026lt; 0] \u0026lt;- 0\rfor(i in 1:n_fans) {\rteam1_arousal[[i]] \u0026lt;- ksmooth(x = score_board$second,\ry = rnorm(n = 1, mean = 3, sd = .25) + (arima.sim(list(order = c(1, 0, 1), ma = -.1, ar = c(.9)), n = total_time) * .25) + arousal(time = score_board$second,\rdifference = score_board$difference,\rrecent_score = lag(score_board$difference, n = sample(c(1:3), 1), default = 0) - lag(score_board$difference, n = sample(c(4:6), 1), default = 0),\rend_game = score_board$end_game),\rbandwidth = total_time/100, kernel = \u0026quot;normal\u0026quot;)$y\r}\rcolnames(team1_arousal) \u0026lt;- paste0(\u0026quot;team1_\u0026quot;, \u0026quot;arousal_\u0026quot;, 1:n_fans)\rteam1_arousal[team1_arousal \u0026gt; 6] \u0026lt;- 6\rteam1_arousal[team1_arousal \u0026lt; 0] \u0026lt;- 0\rfor(i in 1:n_fans) {\rteam2_valence[[i]] \u0026lt;- ksmooth(x = score_board$second,\ry = rnorm(n = 1, mean = 3, sd = .25) + (arima.sim(list(order = c(1, 0, 1), ma = -.1, ar = c(.9)), n = total_time) * .25) + valence(time = score_board$second,\roverall_difference = -score_board$difference,\rrecent_team_score = lag(score_board$team2_score, n = sample(c(1:5), 1), default = 0) - lag(score_board$team2_score, n = sample(c(6:8), 1), default = 0),\rrecent_enemy_score = lag(score_board$team1_score, n = sample(c(1:5), 1), default = 0) - lag(score_board$team1_score, n = sample(c(6:8), 1), default = 0)),\rbandwidth = total_time/100, kernel = \u0026quot;normal\u0026quot;)$y\r}\rcolnames(team2_valence) \u0026lt;- paste0(\u0026quot;team2_\u0026quot;, \u0026quot;valence_\u0026quot;, 1:n_fans)\rteam2_valence[team2_valence \u0026gt; 6] \u0026lt;- 6\rteam2_valence[team2_valence \u0026lt; 0] \u0026lt;- 0\rfor (i in 1:n_fans) {\rteam2_arousal[[i]] \u0026lt;- ksmooth(x = score_board$second,\ry = rnorm(n = 1, mean = 3, sd = .25) + (arima.sim(list(order = c(1, 0, 1), ma = -.1, ar = c(.9)), n = total_time) * .25) + arousal(time = score_board$second,\rdifference = score_board$difference,\rrecent_score = lag(score_board$difference, n = sample(c(1:3), 1), default = 0) - lag(score_board$difference, n = sample(c(4:6), 1), default = 0),\rend_game = score_board$end_game),\rbandwidth = total_time/100, kernel = \u0026quot;normal\u0026quot;)$y\r}\rcolnames(team2_arousal) \u0026lt;- paste0(\u0026quot;team2_\u0026quot;, \u0026quot;arousal_\u0026quot;, 1:n_fans)\rteam2_arousal[team2_arousal \u0026gt; 6] \u0026lt;- 6\rteam2_arousal[team2_arousal \u0026lt; 0] \u0026lt;- 0\rgame_affect \u0026lt;- cbind(score_board, team1_valence, team1_arousal, team2_valence, team2_arousal)\rreturn(game_affect)\r}\rset.seed(060519)\rgame \u0026lt;- game_simulation(n_seconds = 1000, n_fans = 25)\rgame_long \u0026lt;- game %\u0026gt;%\rmutate(instance = row_number(),\rteam1_score = team1_score * 2,\rteam2_score = team2_score * 2) %\u0026gt;%\rselect(instance, everything()) %\u0026gt;%\rselect(-end_game) %\u0026gt;%\rgather(key = \u0026quot;team\u0026quot;, value = \u0026quot;score\u0026quot;, c(team1_valence_1:length(game))) %\u0026gt;%\rseparate(col = team, into = c(\u0026quot;team\u0026quot;, \u0026quot;dimension\u0026quot;, \u0026quot;fan\u0026quot;), sep = \u0026quot;_\u0026quot;) %\u0026gt;%\rspread(dimension, score)\rp \u0026lt;- game_long %\u0026gt;%\rggplot() +\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = 3, ymin = 0, ymax = 3, alpha = 0.5, fill = \u0026quot;lightblue\u0026quot;) +\rannotate(\u0026quot;rect\u0026quot;, xmin = 3, xmax = 6, ymin = 0, ymax = 3, alpha = 0.5, fill = \u0026quot;lightgreen\u0026quot;) +\rannotate(\u0026quot;rect\u0026quot;, xmin = 0, xmax = 3, ymin = 3, ymax = 6, alpha = 0.2, fill = \u0026quot;red\u0026quot;) +\rannotate(\u0026quot;rect\u0026quot;, xmin = 3, xmax = 6, ymin = 3, ymax = 6, alpha = 0.2, fill = \u0026quot;yellow\u0026quot;) +\rannotate(\u0026quot;text\u0026quot;, x = c(1.5, 4.5, 1.5, 4.5), y = c(1.5, 1.5, 4.5, 4.5), label = c(\u0026quot;Disappointed\u0026quot;, \u0026quot;Content\u0026quot;, \u0026quot;Nervous\u0026quot;, \u0026quot;Excited\u0026quot;),\rsize = 10, alpha = .50, family = \u0026quot;Verdana\u0026quot;) +\rgeom_text(aes(x = 1.5, y = 5.5, label = paste(\u0026quot;Red:\u0026quot;, team1_score)), size = 8, color = \u0026quot;red\u0026quot;, family = \u0026quot;Verdana\u0026quot;) +\rgeom_text(aes(x = 4.5, y = 5.5, label = paste(\u0026quot;Blue:\u0026quot;, team2_score)), size = 8, color = \u0026quot;blue\u0026quot;, family = \u0026quot;Verdana\u0026quot;) +\rgeom_point(aes(x = valence, y = arousal, color = team), alpha = .70, size = 4) +\rscale_x_continuous(limits = c(0, 6)) +\rscale_y_continuous(limits = c(0, 6)) +\rscale_color_manual(values = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;)) +\rlabs(title = \u0026#39;Time: {round((max(game_long$second) - frame_time)/60, 2)}\u0026#39;,\rx = \u0026quot;Happiness\u0026quot;,\ry = \u0026quot;Nerves\u0026quot;) +\rtheme_minimal(base_size = 16) +\rtransition_time(second) +\rtheme(legend.position = \u0026quot;none\u0026quot;,\raxis.title.x = element_text(size = 18),\raxis.title.y = element_text(size = 18),\rplot.title = element_text(size = 18, hjust = .5),\rtext = element_text(family = \u0026quot;Verdana\u0026quot;))\rgame1 \u0026lt;- animate(p, nframes = 1062, width = 500, height = 500, fps = 14, end_pause = 12)\rgame1\r\rReferences\rKuppens P, Verduyn P. (2017). Emotion dynamics. Current Opinion in Psychology. 17, 22–26. doi: 10.1016/j.copsyc.2017.06.004.\n\r\r","date":1559692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559692800,"objectID":"1a369baf4edf942ee712ca1baebb98fa","permalink":"/post/basketball_sim/basketball_sim/","publishdate":"2019-06-05T00:00:00Z","relpermalink":"/post/basketball_sim/basketball_sim/","section":"post","summary":"Sporting events host witness to a wide range of human emotion. The emotional ups and downs are especially clear among invested fans. Fans experience the joy and excitement of a triumphant comeback, or the anxiety and disappointment of a loss. It is particularly interesting to see how emotions differ from two opposing fan groups watching the same match.\nI decided to perform some simulations on how a crowd of fans would react during a basketball game.","tags":["R"],"title":"Simulating Emotions during a Basketball Game - Just a Feeling in the Crowd","type":"post"},{"authors":["RJ Coplan","WE Hipson","KA Archbell","LL Ooi","D Baldwin","JC Bowker"],"categories":null,"content":"","date":1558843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558843200,"objectID":"24cefe5980854f84ad92636745ef750b","permalink":"/publication/coplan-et-al.-2019/","publishdate":"2019-05-26T00:00:00-04:00","relpermalink":"/publication/coplan-et-al.-2019/","section":"publication","summary":"Aloneliness is conceptualized as the negative feelings that arise from the perception that one is not spending enough time alone. We developed and validated an assessment of aloneliness and explored its role in the links between motivations for solitude, time spent alone, and wellbeing. Studies 1 (N=643) and 2 (N=379) described the construction and validation of the Solitude and Aloneness Scale (SolAS). Study 3 (N=418) examined the role of aloneliness as a mediator of the links between motivations for solitude and wellbeing. Study 4 (N=967) explored aloneliness as a moderator of links between time alone and depressive symptoms. Cumulatively, results supported the validity and theoretical utility of aloneliness in elucidating the complex associations being solitude and wellbeing.","tags":[],"title":"Seeking more solitude: Conceptualization, assessment, and implications of aloneliness","type":"publication"},{"authors":null,"categories":["R"],"content":"\r\r\r\r\r\r\rLatent Profile Analysis (LPA) tries to identify clusters of individuals (i.e., latent profiles) based on responses to a series of continuous variables (i.e., indicators). LPA assumes that there are unobserved latent profiles that generate patterns of responses on indicator items.\nHere, I will go through a quick example of LPA to identify groups of people based on their interests/hobbies. The data comes from the Young People Survey, available freely on Kaggle.com.\nHere’s a sneak peek at what we’re going for:\nTerminology note: People use the terms clusters, profiles, classes, and groups interchangeably, but there are subtle differences. I’ll mostly stick to profile to refer to a grouping of cases, in keeping with LPA terminology. We should note that LPA is a branch of Gaussian Finite Mixture Modeling, which includes Latent Class Analysis (LCA). The difference between LPA and LCA is conceptual, not computational: LPA uses continuous indicators and LCA uses binary indicators. LPA is a probabilistic model, which means that it models the probability of case belonging to a profile. This is superior to an approach like K-means that uses distance algorithms.\nWith that aside, let’s load in the data.\nlibrary(tidyverse)\rsurvey \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/whipson/tidytuesday/master/young_people.csv\u0026quot;) %\u0026gt;%\rselect(History:Pets)\rThe data is on 32 interests/hobbies. Each item is ranked 1 (not interested) to 5 (very interested).\nThe description on Kaggle suggests there may be careless responding (e.g., participants who selected the same value over and over). We can use the careless package to identify “string responding”. Let’s also look for multivariate outliers with Mahalanobis Distance (see my previous post on Mahalanobis for identifying outliers).\nlibrary(careless)\rlibrary(psych)\rinterests \u0026lt;- survey %\u0026gt;%\rmutate(string = longstring(.)) %\u0026gt;%\rmutate(md = outlier(., plot = FALSE))\rWe’ll cap string responding to a maximum of 10 and use a Mahalanobis D cutoff of alpha = .001.\ncutoff \u0026lt;- (qchisq(p = 1 - .001, df = ncol(interests)))\rinterests_clean \u0026lt;- interests %\u0026gt;%\rfilter(string \u0026lt;= 10,\rmd \u0026lt; cutoff) %\u0026gt;%\rselect(-string, -md)\rThe package mclust performs various types of model-based clustering and dimension reduction. Plus, it’s really intuitive to use. It requires complete data (no missing), so for this example we’ll remove cases with NAs. This is not the preferred approach; we’d be better off imputing. But for illustrative purposes, this works fine. I’m also going to standardize all of the indicators so when we plot the profiles it’s clearer to see the differences between clusters. Running this code will take a few minutes.\nlibrary(mclust)\rinterests_clustering \u0026lt;- interests_clean %\u0026gt;%\rna.omit() %\u0026gt;%\rmutate_all(list(scale))\rBIC \u0026lt;- mclustBIC(interests_clustering)\rWe’ll start by plotting Bayesian Information Criteria for all the models with profiles ranging from 1 to 9.\nplot(BIC)\rIt’s not immediately clear which model is the best since the y-axis is so large and many of the models score close together. summary(BIC) shows the top three models based on BIC.\nsummary(BIC)\r## Best BIC values:\r## VVE,3 VEE,3 EVE,3\r## BIC -75042.7 -75165.1484 -75179.165\r## BIC diff 0.0 -122.4442 -136.461\rThe highest BIC comes from VVE, 3. This says there are 3 clusters with variable volume, variable shape, equal orientation, and ellipsodial distribution (see Figure 2 from this paper for a visual). However, VEE, 3 is not far behind and actually may be a more theoretically useful model since it constrains the shape of the distribution to be equal. For this reason, we’ll go with VEE, 3.\nIf we want to look at this model more closely, we save it as an object and inspect it with summary().\nmod1 \u0026lt;- Mclust(interests_clustering, modelNames = \u0026quot;VEE\u0026quot;, G = 3, x = BIC)\rsummary(mod1)\r## ---------------------------------------------------- ## Gaussian finite mixture model fitted by EM algorithm ## ---------------------------------------------------- ## ## Mclust VEE (ellipsoidal, equal shape and orientation) model with 3\r## components: ## ## log.likelihood n df BIC ICL\r## -35455.83 874 628 -75165.15 -75216.14\r## ## Clustering table:\r## 1 2 3 ## 137 527 210\rThe output describes the geometric characteristics of the profiles and the number of cases classified into each of the three clusters.\nBIC is one of the best fit indices, but it’s always recommended to look for more evidence that the solution we’ve chosen is the correct one. We can also compare values of the Integrated Completed Likelikood (ICL) criterion. See this paper for more details. ICL isn’t much different from BIC, except that it adds a penalty on solutions with greater entropy or classification uncertainty.\nICL \u0026lt;- mclustICL(interests_clustering)\rplot(ICL)\rsummary(ICL)\r## Best ICL values:\r## VVE,3 VEE,3 EVE,3\r## ICL -75134.69 -75216.13551 -75272.891\r## ICL diff 0.00 -81.44795 -138.203\rWe see similar results. ICL suggests that model VEE, 3 fits quite well. Finally, we’ll perform the Bootstrap Likelihood Ratio Test (BLRT) which compares model fit between k-1 and k cluster models. In other words, it looks to see if an increase in profiles increases fit. Based on simulations by Nylund, Asparouhov, and Muthén (2007) BIC and BLRT are the best indicators for how many profiles there are. This line of code will take a long time to run, so if you’re just following along I suggest skipping it unless you want to step out for a coffee break.\nmclustBootstrapLRT(interests_clustering, modelName = \u0026quot;VEE\u0026quot;)\r## ------------------------------------------------------------- ## Bootstrap sequential LRT for the number of mixture components ## ------------------------------------------------------------- ## Model = VEE ## Replications = 999 ## LRTS bootstrap p-value\r## 1 vs 2 197.0384 0.001\r## 2 vs 3 684.8743 0.001\r## 3 vs 4 -124.1935 1.000\rBLRT also suggests that a 3-profile solution is ideal.\nVisualizing LPA\rNow that we’re confident in our choice of a 3-profile solution, let’s plot the results. Specifically, we want to see how the profiles differ on the indicators, that is, the items that made up the profiles. If the solution is theoretically meaningful, we should see differences that make sense.\nFirst, we’ll extract the means for each profile (remember, we chose these to be standardized). Then, we melt this into long form. Note that I’m trimming values exceeding +1 SD, otherwise we run into plotting issues.\nlibrary(reshape2)\rmeans \u0026lt;- data.frame(mod1$parameters$mean, stringsAsFactors = FALSE) %\u0026gt;%\rrownames_to_column() %\u0026gt;%\rrename(Interest = rowname) %\u0026gt;%\rmelt(id.vars = \u0026quot;Interest\u0026quot;, variable.name = \u0026quot;Profile\u0026quot;, value.name = \u0026quot;Mean\u0026quot;) %\u0026gt;%\rmutate(Mean = round(Mean, 2),\rMean = ifelse(Mean \u0026gt; 1, 1, Mean))\rHere’s the code for the plot. I’m reordering the indicators so that similar activities are close together.\nmeans %\u0026gt;%\rggplot(aes(Interest, Mean, group = Profile, color = Profile)) +\rgeom_point(size = 2.25) +\rgeom_line(size = 1.25) +\rscale_x_discrete(limits = c(\u0026quot;Active sport\u0026quot;, \u0026quot;Adrenaline sports\u0026quot;, \u0026quot;Passive sport\u0026quot;,\r\u0026quot;Countryside, outdoors\u0026quot;, \u0026quot;Gardening\u0026quot;, \u0026quot;Cars\u0026quot;,\r\u0026quot;Art exhibitions\u0026quot;, \u0026quot;Dancing\u0026quot;, \u0026quot;Musical instruments\u0026quot;, \u0026quot;Theatre\u0026quot;, \u0026quot;Writing\u0026quot;, \u0026quot;Reading\u0026quot;,\r\u0026quot;Geography\u0026quot;, \u0026quot;History\u0026quot;, \u0026quot;Law\u0026quot;, \u0026quot;Politics\u0026quot;, \u0026quot;Psychology\u0026quot;, \u0026quot;Religion\u0026quot;, \u0026quot;Foreign languages\u0026quot;,\r\u0026quot;Biology\u0026quot;, \u0026quot;Chemistry\u0026quot;, \u0026quot;Mathematics\u0026quot;, \u0026quot;Medicine\u0026quot;, \u0026quot;Physics\u0026quot;, \u0026quot;Science and technology\u0026quot;,\r\u0026quot;Internet\u0026quot;, \u0026quot;PC\u0026quot;,\r\u0026quot;Celebrities\u0026quot;, \u0026quot;Economy Management\u0026quot;, \u0026quot;Fun with friends\u0026quot;, \u0026quot;Shopping\u0026quot;, \u0026quot;Pets\u0026quot;)) +\rlabs(x = NULL, y = \u0026quot;Standardized mean interest\u0026quot;) +\rtheme_bw(base_size = 14) +\rtheme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = \u0026quot;top\u0026quot;)\rWe have a lot of indicators (more than typical for LPA), but we see some interesting differences. Clearly the red group is interested in science and the blue group shows greater interest in arts and humanities. The green group seems disinterested in both science and art, but moderately interested in other things.\nWe can make this plot more informative by plugging in profile names and proportions. I’m also going to save this plot as an object so that we can do something really cool with it!\np \u0026lt;- means %\u0026gt;%\rmutate(Profile = recode(Profile, X1 = \u0026quot;Science: 16%\u0026quot;,\rX2 = \u0026quot;Disinterest: 60%\u0026quot;,\rX3 = \u0026quot;Arts \u0026amp; Humanities: 24%\u0026quot;)) %\u0026gt;%\rggplot(aes(Interest, Mean, group = Profile, color = Profile)) +\rgeom_point(size = 2.25) +\rgeom_line(size = 1.25) +\rscale_x_discrete(limits = c(\u0026quot;Active sport\u0026quot;, \u0026quot;Adrenaline sports\u0026quot;, \u0026quot;Passive sport\u0026quot;,\r\u0026quot;Countryside, outdoors\u0026quot;, \u0026quot;Gardening\u0026quot;, \u0026quot;Cars\u0026quot;,\r\u0026quot;Art exhibitions\u0026quot;, \u0026quot;Dancing\u0026quot;, \u0026quot;Musical instruments\u0026quot;, \u0026quot;Theatre\u0026quot;, \u0026quot;Writing\u0026quot;, \u0026quot;Reading\u0026quot;,\r\u0026quot;Geography\u0026quot;, \u0026quot;History\u0026quot;, \u0026quot;Law\u0026quot;, \u0026quot;Politics\u0026quot;, \u0026quot;Psychology\u0026quot;, \u0026quot;Religion\u0026quot;, \u0026quot;Foreign languages\u0026quot;,\r\u0026quot;Biology\u0026quot;, \u0026quot;Chemistry\u0026quot;, \u0026quot;Mathematics\u0026quot;, \u0026quot;Medicine\u0026quot;, \u0026quot;Physics\u0026quot;, \u0026quot;Science and technology\u0026quot;,\r\u0026quot;Internet\u0026quot;, \u0026quot;PC\u0026quot;,\r\u0026quot;Celebrities\u0026quot;, \u0026quot;Economy Management\u0026quot;, \u0026quot;Fun with friends\u0026quot;, \u0026quot;Shopping\u0026quot;, \u0026quot;Pets\u0026quot;)) +\rlabs(x = NULL, y = \u0026quot;Standardized mean interest\u0026quot;) +\rtheme_bw(base_size = 14) +\rtheme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = \u0026quot;top\u0026quot;)\rp\rThe something really cool that I want to do is make an interactive plot. Why would I want to do this? Well, one of the problems with the static plot is that with so many indicators it’s tough to read the values for each indicator. An interactive plot lets the reader narrow in on specific indicators or profiles of interest. We’ll use plotly to turn our static plot into an interactive one.\nlibrary(plotly)\rggplotly(p, tooltip = c(\u0026quot;Interest\u0026quot;, \u0026quot;Mean\u0026quot;)) %\u0026gt;%\rlayout(legend = list(orientation = \u0026quot;h\u0026quot;, y = 1.2))\r\r{\"x\":{\"data\":[{\"x\":[14,17,16,22,24,26,27,29,20,21,12,13,19,23,15,6,7,18,4,8,9,11,3,1,5,28,31,25,10,30,2,32],\"y\":[-0.11,-0.13,-0.21,0.03,0.25,-0.14,-0.18,-0.48,1,1,0.18,-0.1,-0.1,1,-0.13,-0.09,-0.12,0.05,0.06,0.16,0.01,-0.41,-0.02,-0.01,0.28,-0.03,0.02,0.26,0.02,0.12,-0.06,0.28],\"text\":[\"Interest: History\nMean: -0.11\",\"Interest: Psychology\nMean: -0.13\",\"Interest: Politics\nMean: -0.21\",\"Interest: Mathematics\nMean: 0.03\",\"Interest: Physics\nMean: 0.25\",\"Interest: Internet\nMean: -0.14\",\"Interest: PC\nMean: -0.18\",\"Interest: Economy Management\nMean: -0.48\",\"Interest: Biology\nMean: 1.00\",\"Interest: Chemistry\nMean: 1.00\",\"Interest: Reading\nMean: 0.18\",\"Interest: Geography\nMean: -0.10\",\"Interest: Foreign languages\nMean: -0.10\",\"Interest: Medicine\nMean: 1.00\",\"Interest: Law\nMean: -0.13\",\"Interest: Cars\nMean: -0.09\",\"Interest: Art exhibitions\nMean: -0.12\",\"Interest: Religion\nMean: 0.05\",\"Interest: Countryside, outdoors\nMean: 0.06\",\"Interest: Dancing\nMean: 0.16\",\"Interest: Musical instruments\nMean: 0.01\",\"Interest: Writing\nMean: -0.41\",\"Interest: Passive sport\nMean: -0.02\",\"Interest: Active sport\nMean: -0.01\",\"Interest: Gardening\nMean: 0.28\",\"Interest: Celebrities\nMean: -0.03\",\"Interest: Shopping\nMean: 0.02\",\"Interest: Science and technology\nMean: 0.26\",\"Interest: Theatre\nMean: 0.02\",\"Interest: Fun with friends\nMean: 0.12\",\"Interest: Adrenaline sports\nMean: -0.06\",\"Interest: Pets\nMean: 0.28\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"opacity\":1,\"size\":8.50393700787402,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\"}},\"hoveron\":\"points\",\"name\":\"Science: 16%\",\"legendgroup\":\"Science: 16%\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[14,17,16,22,24,26,27,29,20,21,12,13,19,23,15,6,7,18,4,8,9,11,3,1,5,28,31,25,10,30,2,32],\"y\":[-0.08,-0.13,-0.01,0,-0.1,0.08,0.08,0.15,-0.35,-0.42,-0.23,-0.06,-0.06,-0.37,-0.02,0.14,-0.18,-0.11,-0.03,-0.12,-0.18,-0.52,0.07,0,-0.19,0.02,-0.01,-0.07,-0.15,0.02,0,-0.08],\"text\":[\"Interest: History\nMean: -0.08\",\"Interest: Psychology\nMean: -0.13\",\"Interest: Politics\nMean: -0.01\",\"Interest: Mathematics\nMean: 0.00\",\"Interest: Physics\nMean: -0.10\",\"Interest: Internet\nMean: 0.08\",\"Interest: PC\nMean: 0.08\",\"Interest: Economy Management\nMean: 0.15\",\"Interest: Biology\nMean: -0.35\",\"Interest: Chemistry\nMean: -0.42\",\"Interest: Reading\nMean: -0.23\",\"Interest: Geography\nMean: -0.06\",\"Interest: Foreign languages\nMean: -0.06\",\"Interest: Medicine\nMean: -0.37\",\"Interest: Law\nMean: -0.02\",\"Interest: Cars\nMean: 0.14\",\"Interest: Art exhibitions\nMean: -0.18\",\"Interest: Religion\nMean: -0.11\",\"Interest: Countryside, outdoors\nMean: -0.03\",\"Interest: Dancing\nMean: -0.12\",\"Interest: Musical instruments\nMean: -0.18\",\"Interest: Writing\nMean: -0.52\",\"Interest: Passive sport\nMean: 0.07\",\"Interest: Active sport\nMean: 0.00\",\"Interest: Gardening\nMean: -0.19\",\"Interest: Celebrities\nMean: 0.02\",\"Interest: Shopping\nMean: -0.01\",\"Interest: Science and technology\nMean: -0.07\",\"Interest: Theatre\nMean: -0.15\",\"Interest: Fun with friends\nMean: 0.02\",\"Interest: Adrenaline sports\nMean: 0.00\",\"Interest: Pets\nMean: -0.08\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,186,56,1)\",\"opacity\":1,\"size\":8.50393700787402,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,186,56,1)\"}},\"hoveron\":\"points\",\"name\":\"Disinterest: 60%\",\"legendgroup\":\"Disinterest: 60%\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[14,17,16,22,24,26,27,29,20,21,12,13,19,23,15,6,7,18,4,8,9,11,3,1,5,28,31,25,10,30,2,32],\"y\":[0.28,0.43,0.15,-0.03,0.08,-0.11,-0.09,-0.06,0.01,-0.08,0.47,0.23,0.23,0.06,0.14,-0.3,0.53,0.25,0.03,0.21,0.47,1,-0.18,0,0.29,-0.03,0.01,0,0.36,-0.13,0.04,0.01],\"text\":[\"Interest: History\nMean: 0.28\",\"Interest: Psychology\nMean: 0.43\",\"Interest: Politics\nMean: 0.15\",\"Interest: Mathematics\nMean: -0.03\",\"Interest: Physics\nMean: 0.08\",\"Interest: Internet\nMean: -0.11\",\"Interest: PC\nMean: -0.09\",\"Interest: Economy Management\nMean: -0.06\",\"Interest: Biology\nMean: 0.01\",\"Interest: Chemistry\nMean: -0.08\",\"Interest: Reading\nMean: 0.47\",\"Interest: Geography\nMean: 0.23\",\"Interest: Foreign languages\nMean: 0.23\",\"Interest: Medicine\nMean: 0.06\",\"Interest: Law\nMean: 0.14\",\"Interest: Cars\nMean: -0.30\",\"Interest: Art exhibitions\nMean: 0.53\",\"Interest: Religion\nMean: 0.25\",\"Interest: Countryside, outdoors\nMean: 0.03\",\"Interest: Dancing\nMean: 0.21\",\"Interest: Musical instruments\nMean: 0.47\",\"Interest: Writing\nMean: 1.00\",\"Interest: Passive sport\nMean: -0.18\",\"Interest: Active sport\nMean: 0.00\",\"Interest: Gardening\nMean: 0.29\",\"Interest: Celebrities\nMean: -0.03\",\"Interest: Shopping\nMean: 0.01\",\"Interest: Science and technology\nMean: 0.00\",\"Interest: Theatre\nMean: 0.36\",\"Interest: Fun with friends\nMean: -0.13\",\"Interest: Adrenaline sports\nMean: 0.04\",\"Interest: Pets\nMean: 0.01\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(97,156,255,1)\",\"opacity\":1,\"size\":8.50393700787402,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(97,156,255,1)\"}},\"hoveron\":\"points\",\"name\":\"Arts \u0026 Humanities: 24%\",\"legendgroup\":\"Arts \u0026 Humanities: 24%\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],\"y\":[-0.01,-0.06,-0.02,0.06,0.28,-0.09,-0.12,0.16,0.01,0.02,-0.41,0.18,-0.1,-0.11,-0.13,-0.21,-0.13,0.05,-0.1,1,1,0.03,1,0.25,0.26,-0.14,-0.18,-0.03,-0.48,0.12,0.02,0.28],\"text\":[\"Interest: Active sport\nMean: -0.01\",\"Interest: Adrenaline sports\nMean: -0.06\",\"Interest: Passive sport\nMean: -0.02\",\"Interest: Countryside, outdoors\nMean: 0.06\",\"Interest: Gardening\nMean: 0.28\",\"Interest: Cars\nMean: -0.09\",\"Interest: Art exhibitions\nMean: -0.12\",\"Interest: Dancing\nMean: 0.16\",\"Interest: Musical instruments\nMean: 0.01\",\"Interest: Theatre\nMean: 0.02\",\"Interest: Writing\nMean: -0.41\",\"Interest: Reading\nMean: 0.18\",\"Interest: Geography\nMean: -0.10\",\"Interest: History\nMean: -0.11\",\"Interest: Law\nMean: -0.13\",\"Interest: Politics\nMean: -0.21\",\"Interest: Psychology\nMean: -0.13\",\"Interest: Religion\nMean: 0.05\",\"Interest: Foreign languages\nMean: -0.10\",\"Interest: Biology\nMean: 1.00\",\"Interest: Chemistry\nMean: 1.00\",\"Interest: Mathematics\nMean: 0.03\",\"Interest: Medicine\nMean: 1.00\",\"Interest: Physics\nMean: 0.25\",\"Interest: Science and technology\nMean: 0.26\",\"Interest: Internet\nMean: -0.14\",\"Interest: PC\nMean: -0.18\",\"Interest: Celebrities\nMean: -0.03\",\"Interest: Economy Management\nMean: -0.48\",\"Interest: Fun with friends\nMean: 0.12\",\"Interest: Shopping\nMean: 0.02\",\"Interest: Pets\nMean: 0.28\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":4.7244094488189,\"color\":\"rgba(248,118,109,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Science: 16%\",\"legendgroup\":\"Science: 16%\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],\"y\":[0,0,0.07,-0.03,-0.19,0.14,-0.18,-0.12,-0.18,-0.15,-0.52,-0.23,-0.06,-0.08,-0.02,-0.01,-0.13,-0.11,-0.06,-0.35,-0.42,0,-0.37,-0.1,-0.07,0.08,0.08,0.02,0.15,0.02,-0.01,-0.08],\"text\":[\"Interest: Active sport\nMean: 0.00\",\"Interest: Adrenaline sports\nMean: 0.00\",\"Interest: Passive sport\nMean: 0.07\",\"Interest: Countryside, outdoors\nMean: -0.03\",\"Interest: Gardening\nMean: -0.19\",\"Interest: Cars\nMean: 0.14\",\"Interest: Art exhibitions\nMean: -0.18\",\"Interest: Dancing\nMean: -0.12\",\"Interest: Musical instruments\nMean: -0.18\",\"Interest: Theatre\nMean: -0.15\",\"Interest: Writing\nMean: -0.52\",\"Interest: Reading\nMean: -0.23\",\"Interest: Geography\nMean: -0.06\",\"Interest: History\nMean: -0.08\",\"Interest: Law\nMean: -0.02\",\"Interest: Politics\nMean: -0.01\",\"Interest: Psychology\nMean: -0.13\",\"Interest: Religion\nMean: -0.11\",\"Interest: Foreign languages\nMean: -0.06\",\"Interest: Biology\nMean: -0.35\",\"Interest: Chemistry\nMean: -0.42\",\"Interest: Mathematics\nMean: 0.00\",\"Interest: Medicine\nMean: -0.37\",\"Interest: Physics\nMean: -0.10\",\"Interest: Science and technology\nMean: -0.07\",\"Interest: Internet\nMean: 0.08\",\"Interest: PC\nMean: 0.08\",\"Interest: Celebrities\nMean: 0.02\",\"Interest: Economy Management\nMean: 0.15\",\"Interest: Fun with friends\nMean: 0.02\",\"Interest: Shopping\nMean: -0.01\",\"Interest: Pets\nMean: -0.08\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":4.7244094488189,\"color\":\"rgba(0,186,56,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Disinterest: 60%\",\"legendgroup\":\"Disinterest: 60%\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],\"y\":[0,0.04,-0.18,0.03,0.29,-0.3,0.53,0.21,0.47,0.36,1,0.47,0.23,0.28,0.14,0.15,0.43,0.25,0.23,0.01,-0.08,-0.03,0.06,0.08,0,-0.11,-0.09,-0.03,-0.06,-0.13,0.01,0.01],\"text\":[\"Interest: Active sport\nMean: 0.00\",\"Interest: Adrenaline sports\nMean: 0.04\",\"Interest: Passive sport\nMean: -0.18\",\"Interest: Countryside, outdoors\nMean: 0.03\",\"Interest: Gardening\nMean: 0.29\",\"Interest: Cars\nMean: -0.30\",\"Interest: Art exhibitions\nMean: 0.53\",\"Interest: Dancing\nMean: 0.21\",\"Interest: Musical instruments\nMean: 0.47\",\"Interest: Theatre\nMean: 0.36\",\"Interest: Writing\nMean: 1.00\",\"Interest: Reading\nMean: 0.47\",\"Interest: Geography\nMean: 0.23\",\"Interest: History\nMean: 0.28\",\"Interest: Law\nMean: 0.14\",\"Interest: Politics\nMean: 0.15\",\"Interest: Psychology\nMean: 0.43\",\"Interest: Religion\nMean: 0.25\",\"Interest: Foreign languages\nMean: 0.23\",\"Interest: Biology\nMean: 0.01\",\"Interest: Chemistry\nMean: -0.08\",\"Interest: Mathematics\nMean: -0.03\",\"Interest: Medicine\nMean: 0.06\",\"Interest: Physics\nMean: 0.08\",\"Interest: Science and technology\nMean: 0.00\",\"Interest: Internet\nMean: -0.11\",\"Interest: PC\nMean: -0.09\",\"Interest: Celebrities\nMean: -0.03\",\"Interest: Economy Management\nMean: -0.06\",\"Interest: Fun with friends\nMean: -0.13\",\"Interest: Shopping\nMean: 0.01\",\"Interest: Pets\nMean: 0.01\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":4.7244094488189,\"color\":\"rgba(97,156,255,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"name\":\"Arts \u0026 Humanities: 24%\",\"legendgroup\":\"Arts \u0026 Humanities: 24%\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":29.0178497301785,\"r\":9.29846409298464,\"b\":103.992103462279,\"l\":62.2997094229971},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":18.5969281859693},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,32.6],\"tickmode\":\"array\",\"ticktext\":[\"Active sport\",\"Adrenaline sports\",\"Passive sport\",\"Countryside, outdoors\",\"Gardening\",\"Cars\",\"Art exhibitions\",\"Dancing\",\"Musical instruments\",\"Theatre\",\"Writing\",\"Reading\",\"Geography\",\"History\",\"Law\",\"Politics\",\"Psychology\",\"Religion\",\"Foreign languages\",\"Biology\",\"Chemistry\",\"Mathematics\",\"Medicine\",\"Physics\",\"Science and technology\",\"Internet\",\"PC\",\"Celebrities\",\"Economy Management\",\"Fun with friends\",\"Shopping\",\"Pets\"],\"tickvals\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32],\"categoryorder\":\"array\",\"categoryarray\":[\"Active sport\",\"Adrenaline sports\",\"Passive sport\",\"Countryside, outdoors\",\"Gardening\",\"Cars\",\"Art exhibitions\",\"Dancing\",\"Musical instruments\",\"Theatre\",\"Writing\",\"Reading\",\"Geography\",\"History\",\"Law\",\"Politics\",\"Psychology\",\"Religion\",\"Foreign languages\",\"Biology\",\"Chemistry\",\"Mathematics\",\"Medicine\",\"Physics\",\"Science and technology\",\"Internet\",\"PC\",\"Celebrities\",\"Economy Management\",\"Fun with friends\",\"Shopping\",\"Pets\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":4.64923204649232,\"tickwidth\":0.845314917544058,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":14.8775425487754},\"tickangle\":-45,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.845314917544058,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":18.5969281859693},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-0.596,1.076],\"tickmode\":\"array\",\"ticktext\":[\"-0.5\",\"0.0\",\"0.5\",\"1.0\"],\"tickvals\":[-0.5,0,0.5,1],\"categoryorder\":\"array\",\"categoryarray\":[\"-0.5\",\"0.0\",\"0.5\",\"1.0\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":4.64923204649232,\"tickwidth\":0.845314917544058,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":14.8775425487754},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.845314917544058,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"Standardized mean interest\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":18.5969281859693},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":0.845314917544058,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":2.40515390121689,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.8775425487754},\"y\":1.2,\"orientation\":\"h\"},\"annotations\":[{\"text\":\"Profile\",\"x\":1.02,\"y\":1,\"showarrow\":false,\"ax\":0,\"ay\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":18.5969281859693},\"xref\":\"paper\",\"yref\":\"paper\",\"textangle\":-0,\"xanchor\":\"left\",\"yanchor\":\"bottom\",\"legendTitle\":true}],\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n // is this being viewed in RStudio?\\n if (location.search == '?viewer_pane=1') {\\n alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n } else {\\n window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n }\\n }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"4f542728345a\":{\"x\":{},\"y\":{},\"colour\":{},\"type\":\"scatter\"},\"4f545952a02\":{\"x\":{},\"y\":{},\"colour\":{}}},\"cur_data\":\"4f542728345a\",\"visdat\":{\"4f542728345a\":[\"function (y) \",\"x\"],\"4f545952a02\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]}\rThere’s a quick example of LPA. Overall, I think LPA is great tool for Exploratory Analysis, although I question its reproducibility. What’s important is that the statistician considers both fit indices and theory when deciding on the number of profiles.\nReferences \u0026amp; Resources\rBertoletti, M., Friel, N., \u0026amp; Rastelli, R. (2015). Choosing the number of clusters in a finite mixture model using an exact Integrated Completed Likelihood criterion. https://arxiv.org/pdf/1411.4257.pdf.\nNylund, K. L., Asparouhov, T., \u0026amp; Muthén, B. O. (2007). Deciding on the Number of Classes in Latent Class Analysis and Growth Mixture Modeling: A Monte Carlo Simulation Study. Structural Equation Modeling, 14, 535-569.\nScrucca, L., Fop, M., Murphy, T. B., \u0026amp; Raftery, A. E. (2016). mclust5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models. The R Journal, 8, 289-317.\n\r\r","date":1555718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555718400,"objectID":"671ce3f8ee8f16162c80c0805a433805","permalink":"/post/latent-profile/latent-profile/","publishdate":"2019-04-20T00:00:00Z","relpermalink":"/post/latent-profile/latent-profile/","section":"post","summary":"Latent Profile Analysis (LPA) tries to identify clusters of individuals (i.e., latent profiles) based on responses to a series of continuous variables (i.e., indicators). LPA assumes that there are unobserved latent profiles that generate patterns of responses on indicator items.\nHere, I will go through a quick example of LPA to identify groups of people based on their interests/hobbies. The data comes from the Young People Survey, available freely on Kaggle.","tags":["R"],"title":"Quick Example of Latent Profile Analysis in R","type":"post"},{"authors":["Will E Hipson","Saif M Mohammad"],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\n ","date":1553313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553313600,"objectID":"bcaac40e92a0e859b5d88b52dc34dc1e","permalink":"/talk/srcd2019/","publishdate":"2019-03-23T00:00:00-04:00","relpermalink":"/talk/srcd2019/","section":"talk","summary":"Click on the Slides button above to view the built-in slides feature.\n ","tags":[],"title":"Detecting Developmental Trends and Gender Differences in Emotion Expression in Children's Poetry Using Sentiment Analysis","type":"talk"},{"authors":null,"categories":["R","Emotion Dynamics"],"content":"\r\r\r\rIn this month’s post, I set out to create a visual network of emotions. Emotion Dynamics tells us that different emotions are highly interconnected, such that one emotion morphs into another and so on. I’ll be using a large dataset from an original study published in PLOS ONE by Trampe, Quoidbach, and Taquet (2015). Thanks to Google Dataset Search, I was able to locate this data. The data is collected from 11,000 participants who completed daily questionnaires on the emotions they felt at a given moment. The original paper is fascinating and I highly encourage checking it out - not to mention that the author’s analysis is the inspiration for this post. The raw data can be freely accessed from the author’s OSF page (link in online article) - props to them for publishing the data!\nWhat is a network? In a sentence, a network is a complex set of interrelations between variables. Some terminology: nodes are the variables (in this case, emotions), and edges are the relationships between the variables. Networks can be directed, which means that variables are linked in a sequence (e.g, from emotion A to emotion B), or undirected, which just shows the relationships. Trampe et al. (2015) created an undirected network in their paper, but the data also allows for a directed network - and this is what I’m going to make for this post.\nFirst, I’ll read in the data and fix up a few spelling errors from the original dataset.\nlibrary(tidyverse)\remotion_raw \u0026lt;- read_csv(\u0026quot;https://osf.io/e7uab/download\u0026quot;) %\u0026gt;%\rrename(Offense = Ofense,\rEmbarrassment = Embarassment)\remotion_raw\r## # A tibble: 69,544 x 21\r## id Hours Day Pride Love Hope Gratitude Joy Satisfaction Awe\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 1 3 0 0 0 0 0 0 0\r## 2 1 14 1 0 0 0 0 0 0 0\r## 3 1 14 2 0 0 0 0 0 0 0\r## 4 1 14 4 0 0 0 0 0 0 0\r## 5 1 15 3 0 0 0 0 0 0 0\r## 6 1 15 3 0 0 0 0 0 0 0\r## 7 1 19 1 0 0 0 0 0 0 0\r## 8 8 8 5 0 0 0 0 1 0 0\r## 9 8 9 2 0 0 0 0 1 0 0\r## 10 8 10 7 0 0 0 0 0 0 0\r## # ... with 69,534 more rows, and 11 more variables: Amusement \u0026lt;dbl\u0026gt;,\r## # Alertness \u0026lt;dbl\u0026gt;, Anxiety \u0026lt;dbl\u0026gt;, Disdain \u0026lt;dbl\u0026gt;, Offense \u0026lt;dbl\u0026gt;,\r## # Guilt \u0026lt;dbl\u0026gt;, Disgust \u0026lt;dbl\u0026gt;, Fear \u0026lt;dbl\u0026gt;, Embarrassment \u0026lt;dbl\u0026gt;,\r## # Sadness \u0026lt;dbl\u0026gt;, Anger \u0026lt;dbl\u0026gt;\rThe data is formatted as a sparse matrix (lots of zeros). We have participant id, Day, and Hour that the emotion was reported. To make this data network compatible, I need to wrangle it into a dataframe of edges - that is a from column and a to column. This will become more apparent shortly.\nI can use the function gather to turn the data into long format. By filtering for values of 1, I remove all the zeros from the sparse matrix and I’m left with a column that includes the emotion that was experienced at the time of reporting.\nemotion_long \u0026lt;- emotion_raw %\u0026gt;%\rgather(emotion_type, value, Pride:Anger) %\u0026gt;%\rarrange(id, Day) %\u0026gt;%\rfilter(value == 1) %\u0026gt;%\rselect(-value)\remotion_long\r## # A tibble: 187,426 x 4\r## id Hours Day emotion_type ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 19 1 Offense ## 2 1 19 1 Sadness ## 3 1 14 2 Disgust ## 4 1 15 3 Alertness ## 5 1 15 3 Anxiety ## 6 1 15 3 Embarrassment\r## 7 1 15 3 Sadness ## 8 1 14 4 Alertness ## 9 1 14 4 Embarrassment\r## 10 8 9 2 Joy ## # ... with 187,416 more rows\rStill, there are no edges here - no link between one emotion and the next. Because the data is arranged so that each subsequent row is the next emotion, I can create a new variable, second_emotion, that is the lead of the emotion in that row. Then, I make sure to remove the last row from each participant id (otherwise there would be a relationship between Participant #1’s last emotion and Participant #2’s first emotion).\nemotion_edges \u0026lt;- emotion_long %\u0026gt;%\rmutate(second_emotion = lead(emotion_type)) %\u0026gt;%\rrename(first_emotion = emotion_type) %\u0026gt;%\rselect(id, Day, Hours, first_emotion, second_emotion) %\u0026gt;%\rgroup_by(id) %\u0026gt;%\rslice(-length(id))\remotion_edges\r## # A tibble: 175,480 x 5\r## # Groups: id [11,332]\r## id Day Hours first_emotion second_emotion\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 1 19 Offense Sadness ## 2 1 1 19 Sadness Disgust ## 3 1 2 14 Disgust Alertness ## 4 1 3 15 Alertness Anxiety ## 5 1 3 15 Anxiety Embarrassment ## 6 1 3 15 Embarrassment Sadness ## 7 1 3 15 Sadness Alertness ## 8 1 4 14 Alertness Embarrassment ## 9 8 2 9 Joy Alertness ## 10 8 2 9 Alertness Alertness ## # ... with 175,470 more rows\rNotice how first and second emotion form a sort of chain - Offense to Sadness, Sadness to Disgust, Disgust to Alertness, etc.\nWe’re ignoring the fact that people are experiencing multiple emotions at once and in those instances we don’t know which emotion was experienced first.\nNow that we have our edges, we need to create an object containing the nodes. This is pretty simple, but I’ll add some information indicating the valence and frequency (n) of each emotion, which will help with the visualizations that follow.\nemotion_nodes \u0026lt;- emotion_long %\u0026gt;%\rcount(emotion_type) %\u0026gt;%\rrowid_to_column(\u0026quot;id\u0026quot;) %\u0026gt;%\rrename(label = emotion_type) %\u0026gt;%\rmutate(valence = ifelse(label %in% c(\u0026quot;Awe\u0026quot;, \u0026quot;Amusement\u0026quot;, \u0026quot;Joy\u0026quot;, \u0026quot;Alertness\u0026quot;,\r\u0026quot;Hope\u0026quot;, \u0026quot;Love\u0026quot;, \u0026quot;Gratitude\u0026quot;, \u0026quot;Pride\u0026quot;,\r\u0026quot;Satisfaction\u0026quot;), \u0026quot;positive\u0026quot;, \u0026quot;negative\u0026quot;))\remotion_nodes\r## # A tibble: 18 x 4\r## id label n valence ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 Alertness 17932 positive\r## 2 2 Amusement 11136 positive\r## 3 3 Anger 6394 negative\r## 4 4 Anxiety 19115 negative\r## 5 5 Awe 3830 positive\r## 6 6 Disdain 682 negative\r## 7 7 Disgust 7412 negative\r## 8 8 Embarrassment 3000 negative\r## 9 9 Fear 3457 negative\r## 10 10 Gratitude 7379 positive\r## 11 11 Guilt 3500 negative\r## 12 12 Hope 16665 positive\r## 13 13 Joy 23465 positive\r## 14 14 Love 18625 positive\r## 15 15 Offense 3160 negative\r## 16 16 Pride 9214 positive\r## 17 17 Sadness 13460 negative\r## 18 18 Satisfaction 19000 positive\rWe now have an object containing our nodes and an object containing our edges. Now it’s a matter of weighting (counting) the relationships between the emotions.\nemotion_network \u0026lt;- emotion_edges %\u0026gt;%\rgroup_by(first_emotion, second_emotion) %\u0026gt;%\rsummarize(weight = n()) %\u0026gt;%\rungroup() %\u0026gt;%\rselect(first_emotion, second_emotion, weight)\remotion_network\r## # A tibble: 315 x 3\r## first_emotion second_emotion weight\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 Alertness Alertness 6191\r## 2 Alertness Amusement 97\r## 3 Alertness Anger 214\r## 4 Alertness Anxiety 4784\r## 5 Alertness Awe 14\r## 6 Alertness Disdain 82\r## 7 Alertness Disgust 544\r## 8 Alertness Embarrassment 208\r## 9 Alertness Fear 109\r## 10 Alertness Gratitude 70\r## # ... with 305 more rows\rA few more modifications are needed to make it ready for visualization. I’m trimming some of the really high values using ifelse, just so that they don’t overwhelm the plotting screen.\nedges \u0026lt;- emotion_network %\u0026gt;%\rleft_join(emotion_nodes, by = c(\u0026quot;first_emotion\u0026quot; = \u0026quot;label\u0026quot;)) %\u0026gt;%\rrename(from = id)\redges \u0026lt;- edges %\u0026gt;%\rleft_join(emotion_nodes, by = c(\u0026quot;second_emotion\u0026quot; = \u0026quot;label\u0026quot;)) %\u0026gt;%\rrename(to = id) %\u0026gt;%\rselect(from, to, weight) %\u0026gt;%\rmutate(weight = ifelse(weight \u0026gt; 4500, 4500, weight))\redges\r## # A tibble: 315 x 3\r## from to weight\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 1 4500\r## 2 1 2 97\r## 3 1 3 214\r## 4 1 4 4500\r## 5 1 5 14\r## 6 1 6 82\r## 7 1 7 544\r## 8 1 8 208\r## 9 1 9 109\r## 10 1 10 70\r## # ... with 305 more rows\rWe need the tidygraph and ggraph packages for the visualization. I’ll note that there are a number of packages for visualizing networks, but ggraph seems to be preferred because it is compatible with ggplot terminology. The function tbl_graph will take the nodes and edges and make them ggraph ready.\nlibrary(tidygraph)\rlibrary(ggraph)\rnetwork \u0026lt;- tbl_graph(emotion_nodes, edges, directed = TRUE)\rset.seed(190318)\rggraph(network, layout = \u0026quot;graphopt\u0026quot;) +\rgeom_edge_link(aes(width = weight, color = scale(weight), alpha = weight), check_overlap = TRUE) +\rscale_edge_color_gradient2(low = \u0026quot;darkgrey\u0026quot;, mid = \u0026quot;#00BFFF\u0026quot;, midpoint = 1.5, high = \u0026quot;dodgerblue2\u0026quot;) +\rscale_edge_width(range = c(.2, 1.75)) +\rgeom_node_label(aes(label = label, fill = valence), size = 4) +\rscale_fill_manual(values = c(\u0026quot;#FF6A6A\u0026quot;, \u0026quot;#43CD80\u0026quot;)) +\rtheme_graph() +\rtheme(legend.position = \u0026quot;none\u0026quot;, plot.background = element_rect(fill = \u0026quot;black\u0026quot;))\rStronger relationships show up as thicker lines. Positive emotions seem to be more pronounced and interconnected, which is what was found in the original article. Unfortunately, we don’t get a good sense of temporality (adding directional arrows creates more of a mess than anything). An interactive plot might be more informative, so let’s try that using networkD3.\nlibrary(networkD3)\rnodes_d3 \u0026lt;- emotion_nodes %\u0026gt;%\rmutate(id = id - 1,\rn = (scale(n) + 3)^3)\redges_d3 \u0026lt;- edges %\u0026gt;%\rmutate(from = from - 1, to = to - 1,\rweight = ifelse(weight \u0026lt; 600, 0, log(weight)))\rIt’s VERY important to transform the values to base 0, which is why I’m using mutate -1. networkD3 won’t work on base 1 values.\nAgain, I’ve made a few adjustments for visualization purposes. Namely, I’m removing relationships that occur less than 600 times and scaling the values somewhat arbitrarily. Of course, this is exploratory analysis, but caution should be taken when interpreting these results. The function forceNetwork takes the nodes and edges specified above and turns them into something beautiful.\nforceNetwork(Links = edges_d3, Nodes = nodes_d3, Source = \u0026quot;from\u0026quot;, Nodesize = \u0026quot;n\u0026quot;,\rTarget = \u0026quot;to\u0026quot;, NodeID = \u0026quot;label\u0026quot;, Group = \u0026quot;valence\u0026quot;, Value = \u0026quot;weight\u0026quot;, fontFamily = \u0026quot;sans-serif\u0026quot;,\rcolourScale = JS(\u0026#39;d3.scaleOrdinal().domain([\u0026quot;negative\u0026quot;, \u0026quot;positive\u0026quot;]).range([\u0026quot;#FF6A6A\u0026quot;, \u0026quot;#43CD80\u0026quot;])\u0026#39;),\ropacity = 1, fontSize = 24, linkDistance = 300, linkColour = c(\u0026quot;#8DB6CD\u0026quot;),\rarrows = TRUE, zoom = TRUE, bounded = TRUE, legend = TRUE)\r## Links is a tbl_df. Converting to a plain data frame.\r## Nodes is a tbl_df. Converting to a plain data frame.\r\r{\"x\":{\"links\":{\"source\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17,17],\"target\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,9,10,11,12,13,14,15,16,17,0,1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,6,7,9,10,11,12,13,14,15,16,17,0,1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\"value\":[8.41183267575841,0,0,8.41183267575841,0,0,0,0,0,0,0,0,0,6.55393340402581,0,6.78219205600679,6.52795791762255,0,8.33591109419694,7.97108575350561,0,7.26262860097424,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7.13169851046691,0,0,0,0,0,0,0,0,0,0,6.80903930604298,0,6.60934924316738,0,0,0,0,0,8.41183267575841,0,0,7.38770923908104,0,6.77308037565554,0,7.16317239084664,0,0,6.68959926917897,7.24921505711439,6.44571981938558,7.46336304552002,0,6.4425401664682,7.38523092306657,0,0,6.89365635460264,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7.46508273639955,0,6.99209642741589,0,0,0,0,0,0,0,7.78030308790837,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7.11151211649616,0,0,0,0,0,0,0,6.49978704065585,6.61606518513282,0,0,0,0,0,0,0,7.06561336359772,0,0,0,0,0,0,0,0,7.93092537248339,0,0,8.15679704667565,0,0,0,0,0,0,0,0,0,0,7.03790596344718,0,0,0,6.54821910276237,0,0,0,0,0,0,0,6.4377516497364,0,0,6.58063913728495,0,0,0,0,0,8.02092771898158,0,8.41183267575841,8.3513747067213,0,0,0,0,6.63463335786169,7.13009851012558,7.40488757561612,0,6.82219739062049,0,0,0,0,0,0,0,0,8.41183267575841,0,0,0,0,8.41183267575841,0,0,0,6.69456205852109,0,0,0,0,0,0,0,8.41183267575841,7.99361999482774,8.41183267575841,0,0,0,0,0,0,0,0,6.94119005506837,0,0,0,6.50876913697168,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7.27309259599952,6.55393340402581,8.17723488551019,0,7.92334821193015,0,0,0,0,8.10379671298179,0,0,0,0,0,0,0,0,0,0,6.96129604591017,0,6.60123011872888,8.33471162182092,0,7.87853419614036,8.12058871174027,0,7.23128700432762,7.57250298502038,0,0,0,0,0,0,0,0,0,0,0,0,8.41183267575841],\"colour\":[\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\",\"#8DB6CD\"]},\"nodes\":{\"name\":[\"Alertness\",\"Amusement\",\"Anger\",\"Anxiety\",\"Awe\",\"Disdain\",\"Disgust\",\"Embarrassment\",\"Fear\",\"Gratitude\",\"Guilt\",\"Hope\",\"Joy\",\"Love\",\"Offense\",\"Pride\",\"Sadness\",\"Satisfaction\"],\"group\":[\"positive\",\"positive\",\"negative\",\"negative\",\"positive\",\"negative\",\"negative\",\"negative\",\"negative\",\"positive\",\"negative\",\"positive\",\"positive\",\"positive\",\"negative\",\"positive\",\"negative\",\"positive\"],\"nodesize\":[66.4777260049541,29.823109822341,14.4971585620182,74.9545346567671,8.99863483398379,4.41089086229159,17.1859473592041,7.57540966372343,8.33882236853888,17.0939636079722,8.41318408245376,58.1352372636087,112.27983133578,71.3610761571251,7.83713818129178,22.7248729842467,40.2101604728943,74.1005872923404]},\"options\":{\"NodeID\":\"label\",\"Group\":\"valence\",\"colourScale\":\"d3.scaleOrdinal().domain([\\\"negative\\\", \\\"positive\\\"]).range([\\\"#FF6A6A\\\", \\\"#43CD80\\\"])\",\"fontSize\":24,\"fontFamily\":\"sans-serif\",\"clickTextSize\":60,\"linkDistance\":300,\"linkWidth\":\"function(d) { return Math.sqrt(d.value); }\",\"charge\":-30,\"opacity\":1,\"zoom\":true,\"legend\":true,\"arrows\":true,\"nodesize\":true,\"radiusCalculation\":\" Math.sqrt(d.nodesize)+6\",\"bounded\":true,\"opacityNoHover\":0,\"clickAction\":null}},\"evals\":[],\"jsHooks\":[]}\rHovering over the nodes shows the emotion label and its relationships with other emotions. The arrows indicate directionality in time. It’s a good enough graph, although I would like for the labels to show up at all times. I still have lots to learn about network analysis.\nAs a final note, I’ll mention that I neglected to adjust for the nested structure of the data - emotions nested within hours, days, and participants. This is crucial when conducting formal statistical tests, but should also be accounted for in visualizations.\nReferences \u0026amp; Resources\rThis blog post by Jesse Sadler really helped in the initial stages of my learning on network analysis.\nTrampe, D., Quoidbach, J., Taquet, M. (2015). Emotions in everyday life. PLOS ONE. https://doi.org/10.1371/journal.pone.0145450\n\r","date":1552867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552867200,"objectID":"c300a73a3c264dc8d8ffeb26b08ccddc","permalink":"/post/emotion-network/emotion-network/","publishdate":"2019-03-18T00:00:00Z","relpermalink":"/post/emotion-network/emotion-network/","section":"post","summary":"In this month’s post, I set out to create a visual network of emotions. Emotion Dynamics tells us that different emotions are highly interconnected, such that one emotion morphs into another and so on. I’ll be using a large dataset from an original study published in PLOS ONE by Trampe, Quoidbach, and Taquet (2015). Thanks to Google Dataset Search, I was able to locate this data. The data is collected from 11,000 participants who completed daily questionnaires on the emotions they felt at a given moment.","tags":["R","Emotion Dynamics"],"title":"Network Analysis of Emotions","type":"post"},{"authors":["WE Hipson"],"categories":null,"content":"","date":1550898000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550898000,"objectID":"a519c280cd39108c9161149eb2b867cb","permalink":"/publication/hipson-2019/","publishdate":"2019-02-23T00:00:00-05:00","relpermalink":"/publication/hipson-2019/","section":"publication","summary":"Sentiment analysis is a computational method that automatically analyzes the valence of massive quantities of text. Basic sentiment analysis involves extracting and counting emotionally-laden keywords from passages of text (e.g., hate, love, happy, sad). This study describes using sentiment analysis to explore changes in emotion expression in a developmental context. A sample of n = 8,688 poems published online by children and adolescents from Grade 4 to Grade 12 was analyzed. Sentiment analysis coded words as positive or negative and these were averaged within each poem to obtain its relative percentage of positive and negative sentiment. Polynomial regressions explored linear and nonlinear trends in sentiment scores by grade. Among the results, negative sentiment demonstrated an upward curvilinear trend, increasing sharply from Grade 6 to Grade 11 and then decreasing afterward. Positive sentiment demonstrated a sinusoidal pattern throughout development. Overall, these findings are consistent with previous research on the progressions of emotion expression in childhood and adolescence. Despite some limitations, sentiment analysis presents an opportunity for researchers in developmental psychology to explore basic questions in emotional development using large quantities of data.","tags":["Sentiment Analysis","R"],"title":"Using sentiment analysis to detect affect in children’s and adolescents’ poetry","type":"publication"},{"authors":["WE Hipson","RJ Coplan","DG Seguin"],"categories":null,"content":"","date":1549861200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549861200,"objectID":"c823456012b84015636e7ced937dec23","permalink":"/publication/hipson-et-al.-2019/","publishdate":"2019-02-11T00:00:00-05:00","relpermalink":"/publication/hipson-et-al.-2019/","section":"publication","summary":"Shyness is characterized by the experience of heightened fear, anxiety, and social‐evaluative concerns in social situations and is associated with increased risk for social adjustment difficulties. Previous research suggests that shy children have difficulty regulating negative emotions, such as anger and disappointment, which contributes to problems interacting with others. However, it remains unclear precisely which strategies are involved among these associations. Accordingly, the goal of this study was to explore the mediating role of emotion regulation strategies in the links between young children's shyness and social adjustment at preschool. Participants were 248 preschool children aged 2.5 to 5 years. Parents rated children's shyness and emotion regulation strategies in the context of anger and fear. Early childhood educators assessed indices of social adjustment four months later. Among the results, active regulation mediated associations between shyness and subsequent prosocial and socially withdrawn behaviors. Child gender further moderated these linkages, such that the model predicting socially withdrawn behavior was stronger among boys. These results expand on our understanding of emotion regulation strategies in shy children's early socio‐emotional development.","tags":[],"title":"Active emotion regulation mediates links between shyness and social adjustment in preschool","type":"publication"},{"authors":null,"categories":["R","Emotion Dynamics"],"content":"\r\rI was recently introduced to Google Dataset Search, an extension that searches for open access datasets. There I stumbled upon this dataset on childrens’ and adult’s ratings of facial expressions. The data comes from a published article by Vesker et al. (2018). Briefly, this study involved having adults and 9-year-old children rate a series of 48 faces on two dimensions of emotion, valence (positive vs. negative) and arousal (activated vs. deactivated) (see my previous post for more info on valence and arousal). The authors made some interesting observations about differences in childrens’ and adult’s ratings of these facial expressions.\nHowever, absent from the writeup was a discussion about how reliable these ratings are. We might wonder about the extent to which people agree on the valence or arousal of a face and whether this varies between children and adults. Here, I tackle the issue of intraclass correlation (ICC) using the dataset published by Vesker et al. (2018). The data itself is openly accessible here.\nFirst, I’ll load up the tidyverse and readxl packages, which will help with the data cleaning.\nlibrary(readxl)\rlibrary(tidyverse)\roptions(digits = 3, scipen = -2)\rUpon downloading the data, we’re immediately presented with an issue: it’s an xlsx document (Excel) containing multiple sheets, with each sheet representing a “condition” (Child vs. Adult) and (Valence vs. Arousal). On Stack Overflow, I found a useful function for reading in multiple sheets (see here for original post).\nread_excel_allsheets \u0026lt;- function(filename, tibble = TRUE) {\rsheets \u0026lt;- readxl::excel_sheets(filename)\rx \u0026lt;- lapply(sheets, function(x) readxl::read_excel(filename, sheet = x))\rif(!tibble) x \u0026lt;- lapply(x, as.data.frame)\rnames(x) \u0026lt;- sheets\rx\r}\rNow, I’ll read in the data and extract each sheet. I’m using VA to refer to valence and AR for arousal.\nfaces \u0026lt;- read_excel_allsheets(\u0026quot;C:/Users/wille/Downloads/adults and 9yo all AR and VAL face ratings zenodo.xlsx\u0026quot;)\rVA_adult_raw \u0026lt;- faces$`VAL adult faces`\rVA_child_raw \u0026lt;- faces$`Val 9yo faces`\rAR_adult_raw \u0026lt;- faces$`AR adult faces`\rAR_child_raw \u0026lt;- faces$`AR 9yo faces`\rLet’s get a look at one of these datasets.\nhead(VA_adult_raw[, 1:20]) #Limiting preview to n = 20\r## # A tibble: 6 x 20\r## ..1 ..2 ..3 ..4 ..5 ..6 ..7 ..8 ..9 ..10 ..11 ..12\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 dl sa ~ 4 4 3 4 2 3 4 3 3 5 3\r## 2 dl sa ~ 4 4 3 3 2 3 3 3 3 4 4\r## 3 lp sa ~ 2 3 4 3 3 3 2 3 3 3 2\r## 4 lp sa ~ 4 3 2 3 3 3 4 2 2 4 2\r## 5 ma sa ~ 1 1 1 2 1 2 1 1 2 3 2\r## 6 md sa ~ 2 1 1 2 2 3 1 1 2 3 2\r## # ... with 8 more variables: ..13 \u0026lt;dbl\u0026gt;, ..14 \u0026lt;dbl\u0026gt;, ..15 \u0026lt;dbl\u0026gt;,\r## # ..16 \u0026lt;dbl\u0026gt;, ..17 \u0026lt;dbl\u0026gt;, ..18 \u0026lt;dbl\u0026gt;, ..19 \u0026lt;dbl\u0026gt;, ..20 \u0026lt;dbl\u0026gt;\rIt’s not immediately apparent what is being displayed here because the columns aren’t labeled. The article tells us that participants rated 48 faces, so based on the dimensions we can assume that each row is a face and each column is a participant who rated that face. Admittedly, it’s a less intuitive way of representing the data, but its actually ideal for computing ICCs.\nStill, there’s a lot of data cleaning and wrangling to be done here. First, we have some rows and columns that aren’t relevant, so we’ll get rid of those. Of note, dplyr’s lesser known slice function is helpful for identifying which rows we want to keep.\nVA_adult \u0026lt;- VA_adult_raw %\u0026gt;%\rslice(1:48) %\u0026gt;%\rselect(-1, -mean, -SD, -`M mean`, -`F mean`, -`dist from 4`, -`0`, -aa, -Valence) %\u0026gt;%\rmutate(face = row_number()) %\u0026gt;%\rselect(face, everything())\rNow, to make the columns more intuitive, we’ll label them properly.\ncolnames(VA_adult)[2:161] \u0026lt;- paste(\u0026quot;rater\u0026quot;, 1:160)\rPlotting the Scores\rWe may want to plot the data to get a sense of the variability around raters’ labeling of valence across the 48 faces. Our data is currently in wide form, but we need to set it up such that all of the ratings are in one column. This is where reshape2 comes into action.\nlibrary(reshape2)\rThe function melt will take our wide dataset and make it long. We supply it with an id.vars which tells it which of our original columns we want to stay as a column. Then it takes all of the other columns and condenses them into one variable.\nVA_adult_melt \u0026lt;- VA_adult %\u0026gt;%\rmelt(id.vars = \u0026quot;face\u0026quot;, value.name = \u0026quot;valence\u0026quot;, variable.name = \u0026quot;rater\u0026quot;)\rhead(VA_adult_melt)\r## face rater valence\r## 1 1 rater 1 4\r## 2 2 rater 1 4\r## 3 3 rater 1 2\r## 4 4 rater 1 4\r## 5 5 rater 1 1\r## 6 6 rater 1 2\rNow we’ll turn this into a line graph with each line representing an individual rater’s valence ratings for each of the 48 faces. It will be crowded, but that’s OK. We just want to see if the lines cluster around each other or not.\nVA_adult_melt %\u0026gt;%\rggplot(aes(face, valence, group = rater, color = rater)) +\rgeom_line(size = .8, alpha = .5) +\rscale_x_discrete(limits = 1:48) +\rgeom_vline(xintercept = 24.5, size = 1.5, color = \u0026quot;red\u0026quot;) +\rlabs(x = \u0026quot;Face\u0026quot;, y = \u0026quot;Valence (higher = more positive)\u0026quot;,\rtitle = \u0026quot;Adult Valence Ratings for 48 Faces\u0026quot;,\rsubtitle = \u0026quot;Red line indicates where faces become positive\u0026quot;) +\rtheme_bw() +\rtheme(legend.position = \u0026quot;none\u0026quot;)\rThere’s a clear division between the positive and negative faces. It seems that there’s strong agreement among adults as to what constitutes a positive or negative face.\nWhat if we looked at the distributions of the ratings for each face?\nVA_adult_melt %\u0026gt;%\rgroup_by(face) %\u0026gt;%\rsummarize(mean = mean(valence),\rsd = sd(valence)) %\u0026gt;%\rungroup() %\u0026gt;%\rggplot(aes(face, mean)) +\rgeom_errorbar(aes(ymin = mean - 1.96*(sd/sqrt(ncol(VA_adult))),\rymax = mean + 1.96*(sd/sqrt(ncol(VA_adult))))) +\rgeom_point(size = 2) +\rscale_x_discrete(limits = 1:48) +\rgeom_vline(xintercept = 24.5, color = \u0026quot;red\u0026quot;) +\rlabs(x = \u0026quot;Face\u0026quot;, y = \u0026quot;Valence (higher = more positive)\u0026quot;,\rtitle = \u0026quot;Adult - Average Valence Ratings for 48 Faces\u0026quot;,\rsubtitle = \u0026quot;Red line indicates where faces become positive; error bars = 95% CI\u0026quot;) +\rtheme_bw()\rIt tells the same story, but it’s a more polished figure. Notice how the error bars for the 95% confidence interval around the mean are quite small.\nCleaning the Remaining Datasets\rFirst, we’ll look at the dataset for 9-year-olds’ ratings of valence. Note that there are a few modifications to the script due to idiosyncracies with the original datasets.\nVA_child \u0026lt;- VA_child_raw %\u0026gt;%\rslice(2:49) %\u0026gt;%\rselect(-1, -`average child ratings`, -33, -code,\r-`pic name`, -emotion, -`Child M`, -`Child F`, -sex, -Valence) %\u0026gt;%\rmutate(face = row_number()) %\u0026gt;%\rselect(face, everything())\rcolnames(VA_child)[2:31] \u0026lt;- paste(\u0026quot;rater\u0026quot;, 1:30)\rVA_child \u0026lt;- tbl_df(lapply(VA_child, function(x){ #Need to use a function to convert to numeric\ras.numeric(as.character(x)) }))\rVA_child_melt \u0026lt;- VA_child %\u0026gt;%\rmelt(id.vars = \u0026quot;face\u0026quot;, value.name = \u0026quot;valence\u0026quot;, variable.name = \u0026quot;rater\u0026quot;)\rVA_child_melt %\u0026gt;%\rggplot(aes(face, valence, group = rater, color = rater)) +\rgeom_line(size = .8, alpha = .5) +\rscale_x_discrete(limits = 1:48) +\rgeom_vline(xintercept = 24.5, size = 1.5, color = \u0026quot;red\u0026quot;) +\rlabs(x = \u0026quot;Face\u0026quot;, y = \u0026quot;Valence (higher = more positive)\u0026quot;,\rtitle = \u0026quot;Child Valence Ratings for 48 Faces\u0026quot;,\rsubtitle = \u0026quot;Red line indicates where faces become positive\u0026quot;) +\rtheme_bw() +\rtheme(legend.position = \u0026quot;none\u0026quot;)\rVA_child_melt %\u0026gt;%\rgroup_by(face) %\u0026gt;%\rsummarize(mean = mean(valence, na.rm = TRUE),\rsd = sd(valence, na.rm = TRUE)) %\u0026gt;%\rungroup() %\u0026gt;%\rggplot(aes(face, mean)) +\rgeom_errorbar(aes(ymin = mean - 1.96*(sd/sqrt(ncol(VA_child))),\rymax = mean + 1.96*(sd/sqrt(ncol(VA_child))))) +\rgeom_point(size = 2) +\rscale_x_discrete(limits = 1:48) +\rgeom_vline(xintercept = 24.5, color = \u0026quot;red\u0026quot;) +\rlabs(x = \u0026quot;Face\u0026quot;, y = \u0026quot;Valence (higher = more positive)\u0026quot;,\rtitle = \u0026quot;Child - Average Valence Ratings for 48 Faces\u0026quot;,\rsubtitle = \u0026quot;Red line indicates where faces become positive; error bars = 95% CI\u0026quot;) +\rtheme_bw()\rThe results are similar to those from the adults. We can’t trust the wider confidence intervals to tell us about reliability though, because there are far fewer child raters than adult raters.\n\rRepeating the Procedure for Ratings of Arousal\rFinally, we repeat the analysis for measures of arousal, starting with adults then with children.\nAR_adult \u0026lt;- AR_adult_raw %\u0026gt;%\rslice(1:48) %\u0026gt;%\rselect(-1, -43, -SD, -`M mean`, -`F mean`, -mean, -`0`, -Valence) %\u0026gt;%\rmutate(face = row_number()) %\u0026gt;%\rselect(face, everything())\rcolnames(AR_adult)[2:42] \u0026lt;- paste(\u0026quot;rater\u0026quot;, 1:41)\rAR_adult_melt \u0026lt;- AR_adult %\u0026gt;%\rmelt(id.vars = \u0026quot;face\u0026quot;, value.name = \u0026quot;arousal\u0026quot;, variable.name = \u0026quot;rater\u0026quot;)\rAR_adult_melt %\u0026gt;%\rggplot(aes(face, arousal, group = rater, color = rater)) +\rgeom_line(size = .8, alpha = .5) +\rscale_x_discrete(limits = 1:48) +\rgeom_vline(xintercept = 24.5, size = 1.5, color = \u0026quot;red\u0026quot;) +\rlabs(x = \u0026quot;Face\u0026quot;, y = \u0026quot;Arousal (higher = more activated)\u0026quot;,\rtitle = \u0026quot;Adult Arousal Ratings for 48 Faces\u0026quot;,\rsubtitle = \u0026quot;Red line indicates where faces become positive\u0026quot;) +\rtheme_bw() +\rtheme(legend.position = \u0026quot;none\u0026quot;)\rThere seems to be much less consensus for ratings of arousal. We do notice that there is no differentiation between positive and negative faces - this is good because the theory suggests that arousal is independent of valence. Someone can be positively aroused (e.g., excited) or negatively aroused (e.g., stressed). However, if there was high consensus we would still see the lines converging. Instead, they’re all over the place.\nAR_adult_melt %\u0026gt;%\rgroup_by(face) %\u0026gt;%\rsummarize(mean = mean(arousal),\rsd = sd(arousal)) %\u0026gt;%\rungroup() %\u0026gt;%\rggplot(aes(face, mean)) +\rgeom_errorbar(aes(ymin = mean - 1.96*(sd/sqrt(ncol(AR_adult))),\rymax = mean + 1.96*(sd/sqrt(ncol(AR_adult))))) +\rgeom_point(size = 2) +\rscale_x_discrete(limits = 1:48) +\rgeom_vline(xintercept = 24.5, color = \u0026quot;red\u0026quot;) +\rlabs(x = \u0026quot;Face\u0026quot;, y = \u0026quot;Arousal (higher = more activated)\u0026quot;,\rtitle = \u0026quot;Adult - Average Arousal Ratings for 48 Faces\u0026quot;,\rsubtitle = \u0026quot;Red line indicates where faces become positive; error bars = 95% CI\u0026quot;) +\rtheme_bw()\rConfidence intervals are much wider too, but again, we have a smaller sample size so that adds some uncertainty. Still, it seems like adults have difficulty agreeing on ratings of arousal compared to ratings of valence. Let’s go back to 9-year-olds.\nAR_child \u0026lt;- AR_child_raw %\u0026gt;%\rslice(2:49) %\u0026gt;%\rselect(-`photo ID`, -child, -`adult ratings PELL`, -`Photo ID`,\r-30, -image, -emotion, -`m child`, -`f child`, -Valence) %\u0026gt;%\rmutate(face = row_number()) %\u0026gt;%\rselect(face, everything())\rcolnames(AR_child)[2:31] \u0026lt;- paste(\u0026quot;rater\u0026quot;, 1:30)\rAR_child \u0026lt;- tbl_df(lapply(AR_child, function(x){ #Need to use a function to convert to numeric\ras.numeric(as.character(x)) #Note: There is one missing value from original dataset\r}))\r## Warning in FUN(X[[i]], ...): NAs introduced by coercion\rAR_child_melt \u0026lt;- AR_child %\u0026gt;%\rmelt(id.vars = \u0026quot;face\u0026quot;, value.name = \u0026quot;arousal\u0026quot;, variable.name = \u0026quot;rater\u0026quot;)\rAR_child_melt %\u0026gt;%\rggplot(aes(face, arousal, group = rater, color = rater)) +\rgeom_line(size = .8, alpha = .5) +\rscale_x_discrete(limits = 1:48) +\rgeom_vline(xintercept = 24.5, size = 1.5, color = \u0026quot;red\u0026quot;) +\rlabs(x = \u0026quot;Face\u0026quot;, y = \u0026quot;Arousal (higher = more activated)\u0026quot;,\rtitle = \u0026quot;Child Arousal Ratings for 48 Faces\u0026quot;,\rsubtitle = \u0026quot;Red line indicates where faces become positive\u0026quot;) +\rtheme_bw() +\rtheme(legend.position = \u0026quot;none\u0026quot;)\r## Warning: Removed 48 rows containing missing values (geom_path).\rAR_child_melt %\u0026gt;%\rgroup_by(face) %\u0026gt;%\rsummarize(mean = mean(arousal, na.rm = TRUE),\rsd = sd(arousal, na.rm = TRUE)) %\u0026gt;%\rungroup() %\u0026gt;%\rggplot(aes(face, mean)) +\rgeom_errorbar(aes(ymin = mean - 1.96*(sd/sqrt(ncol(AR_child))),\rymax = mean + 1.96*(sd/sqrt(ncol(AR_child))))) +\rgeom_point(size = 2) +\rscale_x_discrete(limits = 1:48) +\rgeom_vline(xintercept = 24.5, color = \u0026quot;red\u0026quot;) +\rlabs(x = \u0026quot;Face\u0026quot;, y = \u0026quot;Arousal (higher = more activated)\u0026quot;,\rtitle = \u0026quot;Child - Average Arousal Ratings for 48 Faces\u0026quot;,\rsubtitle = \u0026quot;Red line indicates where faces become positive; error bars = 95% CI\u0026quot;) +\rtheme_bw()\rResults look similar for children. I won’t spend much time discussing mean differences in valence and arousal between children and adults - the original article expands on this. However, I am interested in the variability in ratings of arousal vs. valence.\n\r\rQuantifying Interrater Agreement\rSo far, we’ve created a series of plots showing the variability in childrens’ and adult’s ratings of emotional facial expressions. We get a sense that both children and adults reliably label faces as positive or negative, but they struggle to agree on arousal. Although this is apparent from the plots, we may want to test this more formally. This is actually very important because our estimates of variability (e.g., 95% CI) are sensitive to sample size, which varies by adults and children in this dataset.\nIntra-correlation coefficient (ICC)\rThe ICC is an index of reliability or agreement for continuous ratings. ICCs range from 0 (no agreement) to 1 (perfect agreement). We will use ICC to quantify agreement on ratings of emotional facial expressions, but ICC is applicable to other situations, such as quantifying heritability or assessing items in a test bank. Here, we will calculate four ICCs: (1) Adult ratings of Valence, (2) Child ratings of Valence, (3) Adults ratings of Arousal, and (4) Child ratings of Arousal.\nShrout and Fleiss (1979), and later McGraw and Wong (1996), describe several different calculations for ICC that depend on the characteristics of the sample. In our case, we will use a two-way random model for single measurements to quantify absolute agreement, also known as ICC2.\nTwo way random, single measures, absolute (ICC2):\n\\[\\rho = \\frac{\\sigma^2_r}{\\sigma^2_r + \\sigma^2_c + \\sigma^2_e}\\]\nWhere \\(\\rho\\) is the population parameter for the ICC, \\(\\sigma^2_r\\) is the row variability (variability between raters), \\(\\sigma^2_c\\) is the column variability (variability between faces), and \\(\\sigma^2_e\\) is the error.\nWe’re using a two way random model because we expect variability between subjects, but also within (faces have different underlying valence and arousal). Also note that the ‘single measures’ part is referring to the fact that each rating is a single score, not an average of scores.\nWe’ll use the ICC function from the psych package to compute the ICCs.\nlibrary(psych)\rVA_adult_icc \u0026lt;- VA_adult %\u0026gt;%\rselect(-face) %\u0026gt;%\rICC()\rVA_child_icc \u0026lt;- VA_child %\u0026gt;%\rselect(-face) %\u0026gt;%\rICC()\rAR_adult_icc \u0026lt;- AR_adult %\u0026gt;%\rselect(-face) %\u0026gt;%\rICC()\rAR_child_icc \u0026lt;- AR_child %\u0026gt;%\rselect(-face) %\u0026gt;%\rICC()\rNow, we’ll use the kableExtra package to generate a table of the results. Note that I’m extracting the 2nd value for the ICC results because it is the ICC2. If we expected no column variability then we might use ICC1.\nlibrary(kableExtra)\rkable(data.frame(matrix(c(VA_adult_icc$results$ICC[2], VA_child_icc$results$ICC[2],\rAR_adult_icc$results$ICC[2], AR_child_icc$results$ICC[2]),\rnrow = 2, ncol = 2),\rrow.names = c(\u0026quot;Adult\u0026quot;, \u0026quot;Child\u0026quot;)),\rcol.names = c(\u0026quot;ICC Valence\u0026quot;, \u0026quot;ICC Arousal\u0026quot;)) %\u0026gt;%\rkable_styling()\r\r\r\rICC Valence\r\rICC Arousal\r\r\r\r\r\rAdult\r\r0.806\r\r0.194\r\r\r\rChild\r\r0.795\r\r0.220\r\r\r\r\rClearly and not surprisingly, the ICCs for arousal (~.20) are much lower than those for valence (~.80). Using Cicchetti’s (1994) guidelines, we would interpret the valence ICCs as indicating excellent agreement and the arousal ICCs as poor agreement. It is also worth noting that adults and children seem equally (un)reliable in their reporting.\n\r\rConclusions\rThe findings suggest that we should give pause before attempting to interpret differences between children and adults in their overall ratings of arousal in facial expressions. Such disagreement is actually expected according to dimensional theories of emotion (Russell, 2003) because emotions are not viewed as prototypical things, and there can be wide variability in facial expressions even across similar situations. In other words, there’s no universal facial expression for high arousal (in fact, there’s little reason to believe in universality for any emotional expression).\nReferences\rCicchetti, D. V. (1994). Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology. Psychological Assessment, 6, 284-290.\nMcGraw, K. O., \u0026amp; Wong, S. P. (1996). Forming inferences about some intraclass correlation coefficients. Psychological Methods, 1, 30-46.\nRussell, J. A. (2003). Core affect and the psychological construction of emotion. Psychological Review, 110, 145-172.\nShrout, P. E., \u0026amp; Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing reliability. Psychological Bulletin, 86, 420-428.\nVesker, M., Bahn, D., Dege, F., Kauschke, C., \u0026amp; Gudrun, S. (2018). Perceiving arousal and valence in facial expressions: Differences between children and adults. European Journal of Developmental Psychology, 15, 411-425.\n\r\r","date":1549238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549238400,"objectID":"81d475e096f41e181e265a219a7fdcf9","permalink":"/post/emotion-expression-icc/emotion-expression/","publishdate":"2019-02-04T00:00:00Z","relpermalink":"/post/emotion-expression-icc/emotion-expression/","section":"post","summary":"I was recently introduced to Google Dataset Search, an extension that searches for open access datasets. There I stumbled upon this dataset on childrens’ and adult’s ratings of facial expressions. The data comes from a published article by Vesker et al. (2018). Briefly, this study involved having adults and 9-year-old children rate a series of 48 faces on two dimensions of emotion, valence (positive vs. negative) and arousal (activated vs.","tags":["R","Emotion Dynamics"],"title":"The Face of (Dis)Agreement - Intraclass Correlations","type":"post"},{"authors":null,"categories":["R","Emotion Dynamics"],"content":"\rI’m a strong adherent to the circumplex model of emotions introduced by James Russell in the late 1980s. Russell argued that all emotional experience can be boiled down to two dimensions: valence and arousal, with valence being how positive or negative you feel and arousal being how sluggish or emotionally activated you feel. The emotions we commonly label as anger, sadness, joy, etc. can be mapped within this affective two-dimensional space, such that joy is a high valence, high arousal emotion, whereas boredom is a moderately low valence and low arousal emotion.\nThis kind of model is great in the field of emotion dynamics, where we are interested in how emotions change over time. It’s great because we don’t have to get bogged down in philosophical debates about whether someone is in a state of sadness or not, but can instead focus on quantifying and mapping changes in valence and arousal. For my doctoral dissertation, I’m using the circumplex model of emotions to explore how emotions change over time in instances when people are alone or with others. Briefly, I’m interested in whether being alone reduces arousal (i.e., makes you more calm). Some evidence in support of this is offered in a recent paper by Nguyen, Ryan, \u0026amp; Deci (2018), although they didn’t use a circumplex approach to emotions.\nThe circumplex model shines in another way: because it models emotional states in two dimensions, it can be presented visually. This is what I’m attempting to do for my own research. So for now, I’ll simulate some data akin to what I’ll be analyzing in my dissertation, starting simply with just two time points and one condition. The data will represent participants’ valence and arousal (Likert scale of 1-7) at baseline and the same measurements one hour later. I’ll use the simstudy package to generate the data.\nlibrary(simstudy)\rlibrary(tidyverse)\rHere’s the code for simulating the data and binding it all together. I’m using the function genCorGen to simulate and generate correlated data for valence and arousal, respectively. In this call, the params refer to the mean and standard deviations, while rho is the correlation coefficient.\nset.seed(190113)\rdx \u0026lt;- genCorGen(600, nvars = 2, params1 = c(4.79, 4.02), params2 = c(1.3, .9), dist = \u0026quot;normal\u0026quot;,\rrho = .67, corstr = \u0026quot;cs\u0026quot;, wide = TRUE,\rcnames = c(\u0026quot;valence1\u0026quot;, \u0026quot;valence2\u0026quot;))\rdv \u0026lt;- genCorGen(600, nvars = 2, params1 = c(4.12, 3.97), params2 = c(.80, 1.2), dist = \u0026quot;normal\u0026quot;,\rrho = .43, corstr = \u0026quot;cs\u0026quot;, wide = TRUE,\rcnames = c(\u0026quot;arousal1\u0026quot;, \u0026quot;arousal2\u0026quot;))\rcore \u0026lt;- data.frame(round(dx), round(dv[, c(2, 3)]))\rcore$valence1[core$valence1 \u0026gt; 7] \u0026lt;- 7\rAfter generating the data for valence and arousal, I binded the two variables, rounded them to the nearest integer, and trimmed cases that exceeded the 7-point cut-off.\nlibrary(psych)\rdescribe(core)\r## vars n mean sd median trimmed mad min max range skew\r## id 1 600 300.50 173.35 300.5 300.50 222.39 1 600 599 0.00\r## valence1 2 600 4.81 1.10 5.0 4.84 1.48 1 7 6 -0.16\r## valence2 3 600 4.06 0.94 4.0 4.05 1.48 1 7 6 -0.01\r## arousal1 4 600 4.14 0.92 4.0 4.15 1.48 2 7 5 -0.10\r## arousal2 5 600 4.08 1.14 4.0 4.09 1.48 1 7 6 -0.05\r## kurtosis se\r## id -1.21 7.08\r## valence1 -0.12 0.04\r## valence2 0.17 0.04\r## arousal1 -0.26 0.04\r## arousal2 -0.05 0.05\rThe summary statistics check out. So now it’s time to plot the data. The function I’m quaintly calling circumplexi takes four vectors as inputs (time 1 valence, time 2 valence, time 1 arousal, time 2 arousal) and returns a circumplex plot. As it stands, it’s not the most intuitive function, but it produces a decent looking plot.\ncircumplexi \u0026lt;- function(valence_time1, valence_time2, arousal_time1, arousal_time2) {\rv1 \u0026lt;- valence_time1\rv2 \u0026lt;- valence_time2\ra1 \u0026lt;- arousal_time1\ra2 \u0026lt;- arousal_time2\rv1mean \u0026lt;- mean(valence_time1, na.rm = TRUE)\rv2mean \u0026lt;- mean(valence_time2, na.rm = TRUE)\ra1mean \u0026lt;- mean(arousal_time1, na.rm = TRUE)\ra2mean \u0026lt;- mean(arousal_time2, na.rm = TRUE)\rggplot() +\rgeom_segment(aes(x = (min(v1) + max(v1))/2, y = min(v1), xend = (min(v1) + max(v1))/2, yend = max(v1)), color = \u0026quot;gray60\u0026quot;, size = 1) +\rgeom_segment(aes(x = min(v1), y = (min(v1) + max(v1))/2, xend = max(v1), yend = (min(v1) + max(v1))/2), color = \u0026quot;gray60\u0026quot;, size = 1) +\rgeom_point(aes(x = a1mean, y = v1mean, size = 5, color = \u0026quot;Time 1\u0026quot;)) +\rgeom_point(aes(x = a2mean, y = v2mean, size = 5, color = \u0026quot;Time 2\u0026quot;)) +\rscale_x_discrete(name = \u0026quot;arousal\u0026quot;, limits = c(min(v1):max(v1)), expand = c(0, 0)) +\rscale_y_discrete(name = \u0026quot;valence\u0026quot;, limits = c(min(v1):max(v1)), expand = c(0, 0)) +\rgeom_segment(aes(x = a1mean,\ry = v1mean, xend = a2mean,\ryend = v2mean),\rarrow = arrow(type = \u0026quot;closed\u0026quot;, length = unit(.125, \u0026quot;inches\u0026quot;))) +\rcoord_fixed() + theme_light() +\rlabs(title = \u0026quot;Change in Affect from Time 1 to Time 2\u0026quot;,\rsubtitle = \u0026quot;Red dot is affect at Time 1. Blue dot is affect at Time 2\u0026quot;) +\rtheme(legend.position = \u0026quot;none\u0026quot;)\r}\rcircumplexi(core$valence1, core$valence2, core$arousal1, core$arousal2)\rIn this example, the plot shows that affect becomes more neutral (i.e, returns to baseline) following Time 1. In my own research, I’ll be using circumplex plots to depict this change between multiple groups as well. For now, this is a good start.\n","date":1547510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547510400,"objectID":"f66c054bb66b2d692213bedfed3159f6","permalink":"/post/circumplex/circumplex/","publishdate":"2019-01-15T00:00:00Z","relpermalink":"/post/circumplex/circumplex/","section":"post","summary":"I’m a strong adherent to the circumplex model of emotions introduced by James Russell in the late 1980s. Russell argued that all emotional experience can be boiled down to two dimensions: valence and arousal, with valence being how positive or negative you feel and arousal being how sluggish or emotionally activated you feel. The emotions we commonly label as anger, sadness, joy, etc. can be mapped within this affective two-dimensional space, such that joy is a high valence, high arousal emotion, whereas boredom is a moderately low valence and low arousal emotion.","tags":["R","Emotion Dynamics"],"title":"Plotting the Affect Circumplex in R","type":"post"},{"authors":null,"categories":["R"],"content":"\rPsychologists often have a standoffish attitude toward outliers. Developmental psychologists, in particular, seem uncomfortable with removing cases because of the challenges inherent in obtaining data in the first place. However, the process of identifying and (sometimes) removing outliers is not a witch hunt to cleanse datasets of “weird” cases; rather, dealing with outliers is an important step toward solid, reproducible science. As I’ll demonstrate in this simulated example, a few outliers can completely reverse the conclusions derived from statistical analyses.\nlibrary(psych)\rlibrary(tidyverse)\rlibrary(simstudy)\rlibrary(jtools)\rA Hypothetical Case\rI’ll pretend that I have data on participants’ self-reported affinity for aloneness (i.e., how much time they like being alone), time alone (i.e., number of hours typically spent alone per week), and loneliness. We might expect that people who spend more time alone feel more loneliness. However, if you’re the kind of person who enjoys being alone, maybe being by yourself isn’t so bad. In other words, I’m interested in the moderating effect of time alone on the association between affinity for aloneness and loneliness.\n\rGenerating the Data\rI’ll simulate 600 cases using the simstudy package. Because I want the variables correlated, I’ll specify a correlation matrix that makes theoretical sense.\nc \u0026lt;- matrix(c(1, .43, .28, .43, 1, .12, .28, .12, 1), nrow = 3)\rc\r## [,1] [,2] [,3]\r## [1,] 1.00 0.43 0.28\r## [2,] 0.43 1.00 0.12\r## [3,] 0.28 0.12 1.00\rNow, I can use the correlation matrix when I generate the data. In the function genCorData, mu refers to the sample means and sigma refers to their respective standard deviations.\nset.seed(206134)\rdata \u0026lt;- genCorData(600, mu = c(2.65, 3.56, 2.21), sigma = c(.56, 1.12, .70), corMatrix = c)\rdata \u0026lt;- data %\u0026gt;%\rselect(-id) %\u0026gt;%\rrename(alone_affinity = V1, time_alone = V2, loneliness = V3)\rdata\r## alone_affinity time_alone loneliness\r## 1: 2.053861 2.880370 1.750774\r## 2: 2.782888 5.131749 1.646151\r## 3: 2.429589 1.488717 2.333513\r## 4: 2.289647 3.711900 2.780851\r## 5: 3.177230 3.629568 2.694580\r## --- ## 596: 2.660343 4.055748 1.811799\r## 597: 1.564866 2.921037 1.842257\r## 598: 2.742394 4.205703 2.598651\r## 599: 1.439261 2.065413 1.547111\r## 600: 3.137692 4.936879 2.580417\rWith the data generated, I can take a look at the univariate and multivariate distributions in one fell swoop using the function pairs.panels from the psych package.\npairs.panels(data, stars = TRUE)\rEverything looks normal and the correlations are pretty close to the ones that I chose.\n\rBring in the Outliers!\rTo make this example more pathological, I’ll introduce some multivariate outliers. I won’t show the code for this, but all I’ve done is manually change 20 cases.\nLooking at the data again, it’s clear that the outliers have an effect. The sample correlations are still significant, but quite off the mark.\npairs.panels(data_outlier, stars = TRUE)\rModel 1: All Data - Including Outliers\rWhat if we ran a linear regression on these variables? Here, I’ll run a hierarchical linear regression with the first step predicting loneliness from affinity for aloneness and time alone. The second step adds an interaction (this is the moderation I mentioned earlier).\nmodel1 \u0026lt;- lm(loneliness ~ .*time_alone, data = data_outlier)\rsummary(model1)\r## ## Call:\r## lm(formula = loneliness ~ . * time_alone, data = data_outlier)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1.99016 -0.48682 0.01538 0.46143 2.48231 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.66138 0.36935 1.791 0.0739 . ## alone_affinity 0.58212 0.13974 4.166 3.56e-05 ***\r## time_alone 0.24982 0.10581 2.361 0.0185 * ## alone_affinity:time_alone -0.08935 0.03772 -2.369 0.0182 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.718 on 596 degrees of freedom\r## Multiple R-squared: 0.06105, Adjusted R-squared: 0.05632 ## F-statistic: 12.92 on 3 and 596 DF, p-value: 3.478e-08\rOverall, affinity for aloneness and time alone both uniquely positively predict loneliness. More importantly though, the interaction is statistically significant with a p-value at .018. We can visualize this more clearly with simple slopes:\nmodel1_int \u0026lt;- lm(loneliness ~ time_alone * alone_affinity, data = data_outlier)\rinteract_plot(model1_int, pred = \u0026quot;time_alone\u0026quot;, modx = \u0026quot;alone_affinity\u0026quot;) +\rtheme_apa()\rA pristine looking interaction plot! Our simulated data shows that at higher affinity for aloneness the association between time alone and loneliness becomes more negative. This is what was expected.\nIf this were real data, these results are potentially publishable. What is not immediately clear though is that outliers have a severe impact on this finding. Let’s look at the simple slopes a bit differently:\ninteract_plot(model1_int, pred = \u0026quot;time_alone\u0026quot;, modx = \u0026quot;alone_affinity\u0026quot;, linearity.check = TRUE) +\rtheme_apa()\rOh dear… The assumption of linearity for these subsamples is clearly not met. It looks like some cases are skewing the associations among the high and low affinity groups.\n\rModel 2 - Mahalanobis Distance\rA popular way to identify and deal with multivariate outliers is to use Mahalanobis Distance (MD). MD calculates the distance of each case from the central mean. Larger values indicate that a case is farther from where most of the points cluster. The psych package contains a function that quickly calculates and plots MDs:\noutlier(data_outlier)\rWow, one case is way out there, you can hardly see it! Otherwise, most of the points appear to follow in line. We might prefer a more formal test of outliers by using a cut-off score for MD. Here, I’ll recalcuate the MDs using the mahalanobis function and identify those that fall above the cut-off score for a chi-square with k degrees of freedom (3 for 3 variables, but I’ll use ncol in case I want to add or remove variables later):\nmd \u0026lt;- mahalanobis(data, center = colMeans(data_outlier), cov = cov(data_outlier))\ralpha \u0026lt;- .001\rcutoff \u0026lt;- (qchisq(p = 1 - alpha, df = ncol(data_outlier)))\rnames_outliers_MH \u0026lt;- which(md \u0026gt; cutoff)\rexcluded_mh \u0026lt;- names_outliers_MH\rdata_clean_mh \u0026lt;- data_outlier[-excluded_mh, ]\rdata[excluded_mh, ]\r## alone_affinity time_alone loneliness\r## 1: 4.6 1.4 4.2\rUsing this cut-off, only one outlier was identified. Not surprisingly, it’s the case with a huge MD relative to the others. Probing this simulated case closely, we see that this hypothetical individual really likes being alone, spent little time alone, and reported feeling very lonely.\nNow we can rerun the model with this outlier omitted:\nmodel2 \u0026lt;- lm(loneliness ~ .*time_alone, data = data_clean_mh)\rsummary(model2)\r## ## Call:\r## lm(formula = loneliness ~ . * time_alone, data = data_clean_mh)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1.98403 -0.48734 0.01331 0.45859 2.48196 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.79882 0.37897 2.108 0.035461 * ## alone_affinity 0.52131 0.14476 3.601 0.000343 ***\r## time_alone 0.21964 0.10738 2.045 0.041259 * ## alone_affinity:time_alone -0.07595 0.03861 -1.967 0.049624 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.7171 on 595 degrees of freedom\r## Multiple R-squared: 0.05384, Adjusted R-squared: 0.04907 ## F-statistic: 11.29 on 3 and 595 DF, p-value: 3.289e-07\rThe interaction is still significant, but just barely, with a p-value of .049.\n\rModel 3 - Minimum Covariance Determinant\rIs this enough to conclude that the data supports the model? Many would probably be content to stop here, but we haven’t adequately dealt with the outlier infestation. This demonstrates the fallability of MD, which Leys et al. (2018) argue is not a robust way to determine outliers. The problem lies with the fact that MD uses the means and covariances of all the data - including the outliers - and bases the individual difference scores from these values. If we’re really interested in identifying cases that stray from the pack, it makes more sense to base the criteria for removal using a subset of the data that is the most central. This is the idea behind Minimum Covariance Determinant, which calculates the mean and covariance matrix based on the most central subset of the data.\nWe’ll use this to calculate new distance scores from a 75% subset of the data that is highly central. For this, we need the MASS package. The approach for calculating the distance scores is similar, and we can use the same cut-off score as before.\nlibrary(MASS)\routput75 \u0026lt;- cov.mcd(data_outlier, quantile.used = nrow(data_outlier)*.75)\rmhmcd75 \u0026lt;- mahalanobis(data_outlier, output75$center, output75$cov)\rnames_outlier_MCD75 \u0026lt;- which(mhmcd75 \u0026gt; cutoff)\rexcluded_mcd75 \u0026lt;- names_outlier_MCD75\rdata_clean_mcd \u0026lt;- data_outlier[-excluded_mcd75, ]\rdata_outlier[excluded_mcd75, ]\rThis approach identified 9 outliers, as opposed to the 1 identified with the traditional MD. Let’s see whether removing these cases changes the results:\nmodel3 \u0026lt;- lm(loneliness ~ .*time_alone, data = data_clean_mcd)\rsummary(model3)\r## ## Call:\r## lm(formula = loneliness ~ . * time_alone, data = data_clean_mcd)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1.9695 -0.4725 0.0168 0.4519 2.5129 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.18154 0.39213 3.013 0.0027 **\r## alone_affinity 0.36392 0.15217 2.391 0.0171 * ## time_alone 0.08494 0.11128 0.763 0.4456 ## alone_affinity:time_alone -0.02316 0.04057 -0.571 0.5683 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.7064 on 587 degrees of freedom\r## Multiple R-squared: 0.05802, Adjusted R-squared: 0.0532 ## F-statistic: 12.05 on 3 and 587 DF, p-value: 1.153e-07\rWow. Removing 9 data points was enough to decimate the significance of the interaction - the p-value is now .568. This is clearly demonstrated in the simple slopes:\nmodel3_int \u0026lt;- lm(loneliness ~ time_alone * alone_affinity, data = data_clean_mcd)\rinteract_plot(model3_int, pred = \u0026quot;time_alone\u0026quot;, modx = \u0026quot;alone_affinity\u0026quot;) +\rtheme_apa()\rOf course, this would be a disappointing realization for any researcher. We do see, however, that the correlations are better estimated now that these outliers are removed:\npairs.panels(data_clean_mcd, stars = TRUE)\r\r\rConclusion\rThis simulation was a pathological (but realistic) example of how outliers can dramatically skew results, even with reasonably large samples. The Minimum Covariance Determinant version of MD is a more robust method of identifying and removing outliers that would otherwise go unnoticed with traditional MD.\nMany researchers in psychology are uncomfortable with removing outliers because they worry about losing statistical power. Others feel that removing outliers is in some way dissociating their data from reality because “in the real world, there are outliers - people are different!”. Although true, the argument shouldn’t be about whether outliers exist or not, but how much they impact the conclusions we draw from our data. In this simulation, we saw that a difference of 8 cases out of 600 was enough to turn a non-significant result significant. If our goal is to generalize our findings to a larger population, it would be foolish to do so on the basis of 8 outlying cases.\nThe article by Leys et al. (2018) offers suggestions about how to approach outliers. Ideally, a researcher should pre-register their plan for handling outliers. In a post-hoc situation, they advise publishing results with and without outliers. At the very least, we should be acknowledging outliers, rather than pretending the don’t exist.\nAs a final note, I highly recommend reading the article by Leys et al. (2018). It provides a better theoeretical grasp of MD and MCD. Some of the code used in this example (specifically, the codes for calculating MD and MCD) was used from their article. See below for the full reference.\nReferences\nLeys, C., Klein, O., Dominicy, Y., \u0026amp; Ley, C. (2018). Detecting multivariate outliers: Use a robust variant of Mahalanobis distance. Journal of Experimental Social Psychology, 74, 150-156.\n\r","date":1547078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547078400,"objectID":"ed0e4216a285b65090af3d2e949902cc","permalink":"/post/outliers/outliers/","publishdate":"2019-01-10T00:00:00Z","relpermalink":"/post/outliers/outliers/","section":"post","summary":"Psychologists often have a standoffish attitude toward outliers. Developmental psychologists, in particular, seem uncomfortable with removing cases because of the challenges inherent in obtaining data in the first place. However, the process of identifying and (sometimes) removing outliers is not a witch hunt to cleanse datasets of “weird” cases; rather, dealing with outliers is an important step toward solid, reproducible science. As I’ll demonstrate in this simulated example, a few outliers can completely reverse the conclusions derived from statistical analyses.","tags":["R"],"title":"A New Way to Handle Multivariate Outliers","type":"post"},{"authors":null,"categories":null,"content":"We often think of emotions as being there one moment and gone the next. However, we are always experiencing some sort of emotion - whether it\u0026rsquo;s boiling anger or vapid listlessness. The study of emotion dynamics is all about how emotions change over time. Emotions can change from one moment to the next - intensifying or deintensifying. I\u0026rsquo;m interested in exploring what predicts these complex changes. In particular, I\u0026rsquo;m interested in whether solitude impacts our emotional state.\nExtending this further, we can describe change in emotion over developmental time as well. For instance, we know that adolescents often experience more intense emotions compared to adults. My work attempts to explore emotion dynamics across multiple time scales.\n","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"7ce2c80028345baa66224fd76a89ee7d","permalink":"/project/emotion-dynamics/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/emotion-dynamics/","section":"project","summary":"Investigations into how emotions change over time in the context of being alone.","tags":["Emotion Dynamics"],"title":"Emotion Dynamics of Solitude","type":"project"},{"authors":null,"categories":null,"content":"","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"3b5b96fd41e553bcd4522e47ef68eae3","permalink":"/project/emotion-regulation/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/emotion-regulation/","section":"project","summary":"Managing emotions and its impact on well-being and adjustment","tags":["Social Development","Emotion Dynamics"],"title":"Emotion Regulation","type":"project"},{"authors":null,"categories":null,"content":"R is a powerful tool for data science, which is why more and more researchers are turning to R for their analyses. I\u0026rsquo;m interested in using R for a variety of purposes, including (but not limited to) text analysis, linear mixed models, Monte Carlo simulations, and data visualization. I blog relatively frequently about using R to solve analytical problems in psychology.\n","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"0336037297dcc4c6450121fda8a7d747","permalink":"/project/r/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/r/","section":"project","summary":"Insights in statistics, visualization, and programming in R.","tags":["R"],"title":"R","type":"project"},{"authors":null,"categories":null,"content":"In the era of Big Data, we are awash in online textual data, such as Tweets, Google Books, and countless blogs. These sources represent a wealth of data. Sentiment analysis is a technique that automatically assesses the emotional content of text. In partnership with Dr. Saif Mohammad of the National Research Council Canada, I\u0026rsquo;m exploring developmental trends in emotions from thousands of online poetry submitted by children and adolescents. The goal of this project is to provide insight into how emotions change over developmental time. The Children\u0026rsquo;s Poems (Hipson \u0026amp; Mohammad, 2019) corpus is freely available on my Github.\n","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"fcd9fde7217a46963a2f8aa66fe9e1c4","permalink":"/project/sentiment-analysis/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/sentiment-analysis/","section":"project","summary":"Identifying emotions in text - on massive scales.","tags":["Sentiment Analysis"],"title":"Sentiment Analysis","type":"project"},{"authors":null,"categories":null,"content":"","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"b10a15cb942f64a2c55723092ecd2489","permalink":"/project/social-development-in-different-cultures/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/social-development-in-different-cultures/","section":"project","summary":"Research on socio-emotional development in different cultures","tags":["Social Development"],"title":"Social Development in Different Cultures","type":"project"},{"authors":null,"categories":null,"content":"","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"9505ac3cd505c795e1dfbb350b31c3ba","permalink":"/project/social-withdrawal-and-socio-emotional-development/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/social-withdrawal-and-socio-emotional-development/","section":"project","summary":"Exploring the socio-emotional consequences of social withdrawal across development.","tags":["Social Development"],"title":"Social Withdrawal and Socio-emotional Development","type":"project"},{"authors":["J Liu","B Xiao","WE Hipson","RJ Coplan","P Yang","C Cheah"],"categories":null,"content":"","date":1535774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535774400,"objectID":"a12d111d608ba103d805e9c9a921fba3","permalink":"/publication/liu-et-al.-2018/","publishdate":"2018-09-01T00:00:00-04:00","relpermalink":"/publication/liu-et-al.-2018/","section":"publication","summary":"The ability to intentionally control behavior to achieve specific goals helps children concentrate in school and behave appropriately in social situations. In Chinese culture, where self-regulation is highly valued by parents and teachers, children’s difficulties self-regulating may contribute to increased learning problems and subsequent authoritarian parenting. In this study we explored the longitudinal linkages among Chinese children’s self-regulation, learning problems, and authoritarian parenting using a developmental cascades model. Participants were N = 617 primary school students in Shanghai, P.R. China followed over three years from Grade 3–4 to Grade 5–6. Measures of children’s self-regulation, learning problems, and maternal authoritarian parenting were obtained each year from a combination of child self-reports and maternal and teacher ratings. Among the results: (1) compared with the unidirectional and bidirectional models, the developmental cascades model was deemed the best fit for the data; (2) earlier self-regulation negatively predicted later authoritarian parenting via a pathway through academic performance; (3) academic performance directly and indirectly contributed to greater self-regulation. Results are discussed in terms of the implications of self-regulation for Chinese children’s academic success and authoritarian parenting practices.","tags":[],"title":"Self-regulation, learning problems, and maternal authoritarian parenting in Chinese children: A developmental cascades model","type":"publication"},{"authors":["S Sette","WE Hipson","F Zava","E Baumgartner","RJ Coplan"],"categories":null,"content":"","date":1515387600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515387600,"objectID":"6960812a1a073e28c62c193a091c2b0c","permalink":"/publication/sette-et-al.-2018/","publishdate":"2018-01-08T00:00:00-05:00","relpermalink":"/publication/sette-et-al.-2018/","section":"publication","summary":"The aim of the present study was to examine the moderating role of inhibitory control (IC) in the associations between shyness and young children’s social and school adjustment. Participants were 112 Italian children (M = 56.85 months, SD = 10.14) enrolled in preschool. Parents and teachers assessed child shyness and IC as well as indices of social and school adjustment. Children were interviewed to assess vocabulary. Results from hierarchical multiple regression analyses revealed several significant interaction effects between shyness and IC in the prediction of outcome variables. Follow-up simple slope analyses indicated that among children with higher levels of IC, shyness was negatively related to prosocial behavior and popularity. In contrast, among children with lower levels of IC, shyness was positively associated with regulated school behaviors. Practice or Policy: The findings provide evidence to suggest that the combination of shyness and IC may contribute to children’s behavioral rigidity, which in turn may promote social and school adjustment difficulties.","tags":[],"title":"Linking shyness with social and school adjustment in early childhood: The moderating role of inhibitory control","type":"publication"},{"authors":["J Liu","B Xiao","WE Hipson","RJ Coplan","D Li","X Chen"],"categories":null,"content":"","date":1509508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509508800,"objectID":"05ed29bd3f5d8c66b7fa16c1f607b5fc","permalink":"/publication/liu-et-al.-2016/","publishdate":"2017-11-01T00:00:00-04:00","relpermalink":"/publication/liu-et-al.-2016/","section":"publication","summary":"The purpose of this study was to explore the longitudinal links among Chinese children's self‐control, social experiences, and loneliness, largely from a developmental cascades perspective (which postulates mechanisms about how effects within a particular domain of functioning can impact across additional domains over time). Participants were N = 1,066 primary school students in Shanghai, P. R. China, who were followed over three years from Grade 3 to Grade 5. Measures of children's behavioral self‐control, peer preference, and loneliness were obtained each year from peer nominations and child self‐reports. Results indicated that as compared with the unidirectional and bidirectional models, the developmental cascade model represented the best fit for the data. Within this model, a number of significant direct and indirect pathways were identified among variables and over time. For example, self‐control was found to indirectly contribute to later decreases in loneliness via a pathway through peer preference. As well, peer preference both directly and indirectly contributed to later increases in self‐control. Finally, loneliness directly led to decreases in self‐control from Grade 3 to Grade 4, but not from Grade 4 to Grade 5. Results are discussed in terms of the implications of self‐control for Chinese children's social and emotional functioning over time.","tags":[],"title":"Self-control, peer preference, and loneliness in Chinese children: A three-year longitudinal study","type":"publication"},{"authors":["WE Hipson","SL Gardiner","RJ Coplan","LL Ooi"],"categories":null,"content":"","date":1487739600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487739600,"objectID":"fe7474526c8e397ce138e41b5cb96c6d","permalink":"/publication/hipson-et-al.-2017/","publishdate":"2017-02-22T00:00:00-05:00","relpermalink":"/publication/hipson-et-al.-2017/","section":"publication","summary":"The goal of this study was to explore associations among maternal agreeableness, child temperament (i.e., emotion dysregulation), and children's social adjustment at school. Participants were 146 children in kindergarten and Grade 1 (76 girls; Mage = 67.78 months, SD = 10.81 months). Mothers provided ratings of their own agreeableness and their child's temperament, and teachers assessed indices of children's socioemotional functioning at school. Among the results, maternal agreeableness moderated associations between child dysregulation and aspects of adjustment at school. Specifically, at higher levels of maternal agreeableness, the relations between child dysregulation and both anxiety with peers and their prosocial behavior were attenuated. Overall, the results suggest that maternal agreeableness may serve as a protective factor for dysregulated children. Implications for research and practice are discussed.","tags":[],"title":"Maternal agreeableness moderates associations between young children’s emotion dysregulation and socio-emotional functioning at school","type":"publication"}]