[{"authors":["WE Hipson"],"categories":null,"content":"","date":1547528400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547528400,"objectID":"a519c280cd39108c9161149eb2b867cb","permalink":"/publication/hipson-2019/","publishdate":"2019-01-15T00:00:00-05:00","relpermalink":"/publication/hipson-2019/","section":"publication","summary":"Sentiment analysis is a computational method that automatically analyzes the valence of massive quantities of text. Basic sentiment analysis involves extracting and counting emotionally-laden keywords from passages of text (e.g., hate, love, happy, sad). This study describes using sentiment analysis to explore changes in emotion expression in a developmental context. A sample of n = 8,688 poems published online by children and adolescents from Grade 4 to Grade 12 was analyzed. Sentiment analysis coded words as positive or negative and these were averaged within each poem to obtain its relative percentage of positive and negative sentiment. Polynomial regressions explored linear and nonlinear trends in sentiment scores by grade. Among the results, negative sentiment demonstrated an upward curvilinear trend, increasing sharply from Grade 6 to Grade 11 and then decreasing afterward. Positive sentiment demonstrated a sinusoidal pattern throughout development. Overall, these findings are consistent with previous research on the progressions of emotion expression in childhood and adolescence. Despite some limitations, sentiment analysis presents an opportunity for researchers in developmental psychology to explore basic questions in emotional development using large quantities of data.","tags":["Sentiment Analysis","R"],"title":"Using sentiment analysis to detect affect in children’s and adolescents’ poetry (*in press*)","type":"publication"},{"authors":null,"categories":["R","Emotion Dynamics"],"content":"\rI’m a strong adherent to the circumplex model of emotions introduced by James Russell in the late 1980s. Russell argued that all emotional experience can be boiled down to two dimensions: valence and arousal, with valence being how positive or negative you feel and arousal being how sluggish or emotionally activated you feel. The emotions we commonly label as anger, sadness, joy, etc. can be mapped within this affective two-dimensional space, such that joy is a high valence, high arousal emotion, whereas boredom is a moderately low valence and low arousal emotion.\nThis kind of model is great in the field of emotion dynamics, where we are interested in how emotions change over time. It’s great because we don’t have to get bogged down in philosophical debates about whether someone is in a state of sadness or not, but can instead focus on quantifying and mapping changes in valence and arousal. For my doctoral dissertation, I’m using the circumplex model of emotions to explore how emotions change over time in instances when people are alone or with others. Briefly, I’m interested in whether being alone reduces arousal (i.e., makes you more calm). Some evidence in support of this is offered in a recent paper by Nguyen, Ryan, \u0026amp; Deci (2018), although they didn’t use a circumplex approach to emotions.\nThe circumplex model shines in another way: because it models emotional states in two dimensions, it can be presented visually. This is what I’m attempting to do for my own research. So for now, I’ll simulate some data akin to what I’ll be analyzing in my dissertation, starting simply with just two time points and one condition. The data will represent participants’ valence and arousal (Likert scale of 1-7) at baseline and the same measurements one hour later. I’ll use the simstudy package to generate the data.\nlibrary(simstudy)\rlibrary(tidyverse)\rHere’s the code for simulating the data and binding it all together. I’m using the function genCorGen to simulate and generate correlated data for valence and arousal, respectively. In this call, the params refer to the mean and standard deviations, while rho is the correlation coefficient.\nset.seed(190113)\rdx \u0026lt;- genCorGen(600, nvars = 2, params1 = c(4.79, 4.02), params2 = c(1.3, .9), dist = \u0026quot;normal\u0026quot;,\rrho = .67, corstr = \u0026quot;cs\u0026quot;, wide = TRUE,\rcnames = c(\u0026quot;valence1\u0026quot;, \u0026quot;valence2\u0026quot;))\rdv \u0026lt;- genCorGen(600, nvars = 2, params1 = c(4.12, 3.97), params2 = c(.80, 1.2), dist = \u0026quot;normal\u0026quot;,\rrho = .43, corstr = \u0026quot;cs\u0026quot;, wide = TRUE,\rcnames = c(\u0026quot;arousal1\u0026quot;, \u0026quot;arousal2\u0026quot;))\rcore \u0026lt;- data.frame(round(dx), round(dv[, c(2, 3)]))\rcore$valence1[core$valence1 \u0026gt; 7] \u0026lt;- 7\rAfter generating the data for valence and arousal, I binded the two variables, rounded them to the nearest integer, and trimmed cases that exceeded the 7-point cut-off.\nlibrary(psych)\rdescribe(core)\r## vars n mean sd median trimmed mad min max range skew\r## id 1 600 300.50 173.35 300.5 300.50 222.39 1 600 599 0.00\r## valence1 2 600 4.81 1.10 5.0 4.84 1.48 1 7 6 -0.16\r## valence2 3 600 4.06 0.94 4.0 4.05 1.48 1 7 6 -0.01\r## arousal1 4 600 4.14 0.92 4.0 4.15 1.48 2 7 5 -0.10\r## arousal2 5 600 4.08 1.14 4.0 4.09 1.48 1 7 6 -0.05\r## kurtosis se\r## id -1.21 7.08\r## valence1 -0.12 0.04\r## valence2 0.17 0.04\r## arousal1 -0.26 0.04\r## arousal2 -0.05 0.05\rThe summary statistics check out. So now it’s time to plot the data. The function I’m quaintly calling circumplexi takes four vectors as inputs (time 1 valence, time 2 valence, time 1 arousal, time 2 arousal) and returns a circumplex plot. As it stands, it’s not the most intuitive function, but it produces a decent looking plot.\ncircumplexi \u0026lt;- function(valence_time1, valence_time2, arousal_time1, arousal_time2) {\rv1 \u0026lt;- valence_time1\rv2 \u0026lt;- valence_time2\ra1 \u0026lt;- arousal_time1\ra2 \u0026lt;- arousal_time2\rv1mean \u0026lt;- mean(valence_time1, na.rm = TRUE)\rv2mean \u0026lt;- mean(valence_time2, na.rm = TRUE)\ra1mean \u0026lt;- mean(arousal_time1, na.rm = TRUE)\ra2mean \u0026lt;- mean(arousal_time2, na.rm = TRUE)\rggplot() +\rgeom_segment(aes(x = (min(v1) + max(v1))/2, y = min(v1), xend = (min(v1) + max(v1))/2, yend = max(v1)), color = \u0026quot;gray60\u0026quot;, size = 1) +\rgeom_segment(aes(x = min(v1), y = (min(v1) + max(v1))/2, xend = max(v1), yend = (min(v1) + max(v1))/2), color = \u0026quot;gray60\u0026quot;, size = 1) +\rgeom_point(aes(x = a1mean, y = v1mean, size = 5, color = \u0026quot;Time 1\u0026quot;)) +\rgeom_point(aes(x = a2mean, y = v2mean, size = 5, color = \u0026quot;Time 2\u0026quot;)) +\rscale_x_discrete(name = \u0026quot;arousal\u0026quot;, limits = c(min(v1):max(v1)), expand = c(0, 0)) +\rscale_y_discrete(name = \u0026quot;valence\u0026quot;, limits = c(min(v1):max(v1)), expand = c(0, 0)) +\rgeom_segment(aes(x = a1mean,\ry = v1mean, xend = a2mean,\ryend = v2mean),\rarrow = arrow(type = \u0026quot;closed\u0026quot;, length = unit(.125, \u0026quot;inches\u0026quot;))) +\rcoord_fixed() + theme_light() +\rlabs(title = \u0026quot;Change in Affect from Time 1 to Time 2\u0026quot;,\rsubtitle = \u0026quot;Red dot is affect at Time 1. Blue dot is affect at Time 2\u0026quot;) +\rtheme(legend.position = \u0026quot;none\u0026quot;)\r}\rcircumplexi(core$valence1, core$valence2, core$arousal1, core$arousal2)\rIn this example, the plot shows that affect becomes more neutral (i.e, returns to baseline) following Time 1. In my own research, I’ll be using circumplex plots to depict this change between multiple groups as well. For now, this is a good start.\n","date":1547510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547510400,"objectID":"f66c054bb66b2d692213bedfed3159f6","permalink":"/post/circumplex/circumplex/","publishdate":"2019-01-15T00:00:00Z","relpermalink":"/post/circumplex/circumplex/","section":"post","summary":"I’m a strong adherent to the circumplex model of emotions introduced by James Russell in the late 1980s. Russell argued that all emotional experience can be boiled down to two dimensions: valence and arousal, with valence being how positive or negative you feel and arousal being how sluggish or emotionally activated you feel. The emotions we commonly label as anger, sadness, joy, etc. can be mapped within this affective two-dimensional space, such that joy is a high valence, high arousal emotion, whereas boredom is a moderately low valence and low arousal emotion.","tags":["R","Emotion Dynamics"],"title":"Plotting the Affect Circumplex in R","type":"post"},{"authors":null,"categories":["R"],"content":"\rPsychologists often have a standoffish attitude toward outliers. Developmental psychologists, in particular, seem uncomfortable with removing cases because of the challenges inherent in obtaining data in the first place. However, the process of identifying and (sometimes) removing outliers is not a witch hunt to cleanse datasets of “weird” cases; rather, dealing with outliers is an important step toward solid, reproducible science. As I’ll demonstrate in this simulated example, a few outliers can completely reverse the conclusions derived from statistical analyses.\nlibrary(psych)\rlibrary(tidyverse)\rlibrary(simstudy)\rlibrary(jtools)\rA Hypothetical Case\rI’ll pretend that I have data on participants’ self-reported affinity for aloneness (i.e., how much time they like being alone), time alone (i.e., number of hours typically spent alone per week), and loneliness. We might expect that people who spend more time alone feel more loneliness. However, if you’re the kind of person who enjoys being alone, maybe being by yourself isn’t so bad. In other words, I’m interested in the moderating effect of time alone on the association between affinity for aloneness and loneliness.\n\rGenerating the Data\rI’ll simulate 600 cases using the simstudy package. Because I want the variables correlated, I’ll specify a correlation matrix that makes theoretical sense.\nc \u0026lt;- matrix(c(1, .43, .28, .43, 1, .12, .28, .12, 1), nrow = 3)\rc\r## [,1] [,2] [,3]\r## [1,] 1.00 0.43 0.28\r## [2,] 0.43 1.00 0.12\r## [3,] 0.28 0.12 1.00\rNow, I can use the correlation matrix when I generate the data. In the function genCorData, mu refers to the sample means and sigma refers to their respective standard deviations.\nset.seed(206134)\rdata \u0026lt;- genCorData(600, mu = c(2.65, 3.56, 2.21), sigma = c(.56, 1.12, .70), corMatrix = c)\rdata \u0026lt;- data %\u0026gt;%\rselect(-id) %\u0026gt;%\rrename(alone_affinity = V1, time_alone = V2, loneliness = V3)\rdata\r## alone_affinity time_alone loneliness\r## 1: 2.053861 2.880370 1.750774\r## 2: 2.782888 5.131749 1.646151\r## 3: 2.429589 1.488717 2.333513\r## 4: 2.289647 3.711900 2.780851\r## 5: 3.177230 3.629568 2.694580\r## --- ## 596: 2.660343 4.055748 1.811799\r## 597: 1.564866 2.921037 1.842257\r## 598: 2.742394 4.205703 2.598651\r## 599: 1.439261 2.065413 1.547111\r## 600: 3.137692 4.936879 2.580417\rWith the data generated, I can take a look at the univariate and multivariate distributions in one fell swoop using the function pairs.panels from the psych package.\npairs.panels(data, stars = TRUE)\rEverything looks normal and the correlations are pretty close to the ones that I chose.\n\rBring in the Outliers!\rTo make this example more pathological, I’ll introduce some multivariate outliers. I won’t show the code for this, but all I’ve done is manually change 20 cases.\nLooking at the data again, it’s clear that the outliers have an effect. The sample correlations are still significant, but quite off the mark.\npairs.panels(data_outlier, stars = TRUE)\rModel 1: All Data - Including Outliers\rWhat if we ran a linear regression on these variables? Here, I’ll run a hierarchical linear regression with the first step predicting loneliness from affinity for aloneness and time alone. The second step adds an interaction (this is the moderation I mentioned earlier).\nmodel1 \u0026lt;- lm(loneliness ~ .*time_alone, data = data_outlier)\rsummary(model1)\r## ## Call:\r## lm(formula = loneliness ~ . * time_alone, data = data_outlier)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1.99016 -0.48682 0.01538 0.46143 2.48231 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.66138 0.36935 1.791 0.0739 . ## alone_affinity 0.58212 0.13974 4.166 3.56e-05 ***\r## time_alone 0.24982 0.10581 2.361 0.0185 * ## alone_affinity:time_alone -0.08935 0.03772 -2.369 0.0182 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.718 on 596 degrees of freedom\r## Multiple R-squared: 0.06105, Adjusted R-squared: 0.05632 ## F-statistic: 12.92 on 3 and 596 DF, p-value: 3.478e-08\rOverall, affinity for aloneness and time alone both uniquely positively predict loneliness. More importantly though, the interaction is statistically significant with a p-value at .018. We can visualize this more clearly with simple slopes:\nmodel1_int \u0026lt;- lm(loneliness ~ time_alone * alone_affinity, data = data_outlier)\rinteract_plot(model1_int, pred = \u0026quot;time_alone\u0026quot;, modx = \u0026quot;alone_affinity\u0026quot;) +\rtheme_apa()\rA pristine looking interaction plot! Our simulated data shows that at higher affinity for aloneness the association between time alone and loneliness becomes more negative. This is what was expected.\nIf this were real data, these results are potentially publishable. What is not immediately clear though is that outliers have a severe impact on this finding. Let’s look at the simple slopes a bit differently:\ninteract_plot(model1_int, pred = \u0026quot;time_alone\u0026quot;, modx = \u0026quot;alone_affinity\u0026quot;, linearity.check = TRUE) +\rtheme_apa()\rOh dear… The assumption of linearity for these subsamples is clearly not met. It looks like some cases are skewing the associations among the high and low affinity groups.\n\rModel 2 - Mahalanobis Distance\rA popular way to identify and deal with multivariate outliers is to use Mahalanobis Distance (MD). MD calculates the distance of each case from the central mean. Larger values indicate that a case is farther from where most of the points cluster. The psych package contains a function that quickly calculates and plots MDs:\noutlier(data_outlier)\rWow, one case is way out there, you can hardly see it! Otherwise, most of the points appear to follow in line. We might prefer a more formal test of outliers by using a cut-off score for MD. Here, I’ll recalcuate the MDs using the mahalanobis function and identify those that fall above the cut-off score for a chi-square with k degrees of freedom (3 for 3 variables, but I’ll use ncol in case I want to add or remove variables later):\nmd \u0026lt;- mahalanobis(data, center = colMeans(data_outlier), cov = cov(data_outlier))\ralpha \u0026lt;- .001\rcutoff \u0026lt;- (qchisq(p = 1 - alpha, df = ncol(data_outlier)))\rnames_outliers_MH \u0026lt;- which(md \u0026gt; cutoff)\rexcluded_mh \u0026lt;- names_outliers_MH\rdata_clean_mh \u0026lt;- data_outlier[-excluded_mh, ]\rdata[excluded_mh, ]\r## alone_affinity time_alone loneliness\r## 1: 4.6 1.4 4.2\rUsing this cut-off, only one outlier was identified. Not surprisingly, it’s the case with a huge MD relative to the others. Probing this simulated case closely, we see that this hypothetical individual really likes being alone, spent little time alone, and reported feeling very lonely.\nNow we can rerun the model with this outlier omitted:\nmodel2 \u0026lt;- lm(loneliness ~ .*time_alone, data = data_clean_mh)\rsummary(model2)\r## ## Call:\r## lm(formula = loneliness ~ . * time_alone, data = data_clean_mh)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1.98403 -0.48734 0.01331 0.45859 2.48196 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.79882 0.37897 2.108 0.035461 * ## alone_affinity 0.52131 0.14476 3.601 0.000343 ***\r## time_alone 0.21964 0.10738 2.045 0.041259 * ## alone_affinity:time_alone -0.07595 0.03861 -1.967 0.049624 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.7171 on 595 degrees of freedom\r## Multiple R-squared: 0.05384, Adjusted R-squared: 0.04907 ## F-statistic: 11.29 on 3 and 595 DF, p-value: 3.289e-07\rThe interaction is still significant, but just barely, with a p-value of .049.\n\rModel 3 - Minimum Covariance Determinant\rIs this enough to conclude that the data supports the model? Many would probably be content to stop here, but we haven’t adequately dealt with the outlier infestation. This demonstrates the fallability of MD, which Leys et al. (2018) argue is not a robust way to determine outliers. The problem lies with the fact that MD uses the means and covariances of all the data - including the outliers - and bases the individual difference scores from these values. If we’re really interested in identifying cases that stray from the pack, it makes more sense to base the criteria for removal using a subset of the data that is the most central. This is the idea behind Minimum Covariance Determinant, which calculates the mean and covariance matrix based on the most central subset of the data.\nWe’ll use this to calculate new distance scores from a 75% subset of the data that is highly central. For this, we need the MASS package. The approach for calculating the distance scores is similar, and we can use the same cut-off score as before.\nlibrary(MASS)\routput75 \u0026lt;- cov.mcd(data_outlier, quantile.used = nrow(data_outlier)*.75)\rmhmcd75 \u0026lt;- mahalanobis(data_outlier, output75$center, output75$cov)\rnames_outlier_MCD75 \u0026lt;- which(mhmcd75 \u0026gt; cutoff)\rexcluded_mcd75 \u0026lt;- names_outlier_MCD75\rdata_clean_mcd \u0026lt;- data_outlier[-excluded_mcd75, ]\rdata_outlier[excluded_mcd75, ]\rThis approach identified 9 outliers, as opposed to the 1 identified with the traditional MD. Let’s see whether removing these cases changes the results:\nmodel3 \u0026lt;- lm(loneliness ~ .*time_alone, data = data_clean_mcd)\rsummary(model3)\r## ## Call:\r## lm(formula = loneliness ~ . * time_alone, data = data_clean_mcd)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1.9695 -0.4725 0.0168 0.4519 2.5129 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 1.18154 0.39213 3.013 0.0027 **\r## alone_affinity 0.36392 0.15217 2.391 0.0171 * ## time_alone 0.08494 0.11128 0.763 0.4456 ## alone_affinity:time_alone -0.02316 0.04057 -0.571 0.5683 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.7064 on 587 degrees of freedom\r## Multiple R-squared: 0.05802, Adjusted R-squared: 0.0532 ## F-statistic: 12.05 on 3 and 587 DF, p-value: 1.153e-07\rWow. Removing 9 data points was enough to decimate the significance of the interaction - the p-value is now .568. This is clearly demonstrated in the simple slopes:\nmodel3_int \u0026lt;- lm(loneliness ~ time_alone * alone_affinity, data = data_clean_mcd)\rinteract_plot(model3_int, pred = \u0026quot;time_alone\u0026quot;, modx = \u0026quot;alone_affinity\u0026quot;) +\rtheme_apa()\rOf course, this would be a disappointing realization for any researcher. We do see, however, that the correlations are better estimated now that these outliers are removed:\npairs.panels(data_clean_mcd, stars = TRUE)\r\r\rConclusion\rThis simulation was a pathological (but realistic) example of how outliers can dramatically skew results, even with reasonably large samples. The Minimum Covariance Determinant version of MD is a more robust method of identifying and removing outliers that would otherwise go unnoticed with traditional MD.\nMany researchers in psychology are uncomfortable with removing outliers because they worry about losing statistical power. Others feel that removing outliers is in some way dissociating their data from reality because “in the real world, there are outliers - people are different!”. Although true, the argument shouldn’t be about whether outliers exist or not, but how much they impact the conclusions we draw from our data. In this simulation, we saw that a difference of 8 cases out of 600 was enough to turn a non-significant result significant. If our goal is to generalize our findings to a larger population, it would be foolish to do so on the basis of 8 outlying cases.\nThe article by Leys et al. (2018) offers suggestions about how to approach outliers. Ideally, a researcher should pre-register their plan for handling outliers. In a post-hoc situation, they advise publishing results with and without outliers. At the very least, we should be acknowledging outliers, rather than pretending the don’t exist.\nAs a final note, I highly recommend reading the article by Leys et al. (2018). It provides a better theoeretical grasp of MD and MCD. Some of the code used in this example (specifically, the codes for calculating MD and MCD) was used from their article. See below for the full reference.\nReferences\nLeys, C., Klein, O., Dominicy, Y., \u0026amp; Ley, C. (2018). Detecting multivariate outliers: Use a robust variant of Mahalanobis distance. Journal of Experimental Social Psychology, 74, 150-156.\n\r","date":1547078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547078400,"objectID":"ed0e4216a285b65090af3d2e949902cc","permalink":"/post/outliers/outliers/","publishdate":"2019-01-10T00:00:00Z","relpermalink":"/post/outliers/outliers/","section":"post","summary":"Psychologists often have a standoffish attitude toward outliers. Developmental psychologists, in particular, seem uncomfortable with removing cases because of the challenges inherent in obtaining data in the first place. However, the process of identifying and (sometimes) removing outliers is not a witch hunt to cleanse datasets of “weird” cases; rather, dealing with outliers is an important step toward solid, reproducible science. As I’ll demonstrate in this simulated example, a few outliers can completely reverse the conclusions derived from statistical analyses.","tags":["R"],"title":"A New Way to Handle Multivariate Outliers","type":"post"},{"authors":null,"categories":null,"content":"We often think of emotions as being there one moment and gone the next. However, we are always experiencing some sort of emotion - whether it\u0026rsquo;s boiling anger or vapid listlessness. The study of emotion dynamics is all about how emotions change over time. Emotions can change from one moment to the next - intensifying or deintensifying. I\u0026rsquo;m interested in exploring what predicts these complex changes. In particular, I\u0026rsquo;m interested in whether solitude impacts our emotional state.\nExtending this further, we can describe change in emotion over developmental time as well. For instance, we know that adolescents often experience more intense emotions compared to adults. My work attempts to explore emotion dynamics across multiple time scales.\n","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"7ce2c80028345baa66224fd76a89ee7d","permalink":"/project/emotion-dynamics/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/emotion-dynamics/","section":"project","summary":"Investigations into how emotions change over time in the context of being alone.","tags":["Emotion Dynamics"],"title":"Emotion Dynamics of Solitude","type":"project"},{"authors":null,"categories":null,"content":"","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"3b5b96fd41e553bcd4522e47ef68eae3","permalink":"/project/emotion-regulation/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/emotion-regulation/","section":"project","summary":"Managing emotions and its impact on well-being and adjustment","tags":["Social Development","Emotion Dynamics"],"title":"Emotion Regulation","type":"project"},{"authors":null,"categories":null,"content":"R is a powerful tool for data science, which is why more and more researchers are turning to R for their analyses. I\u0026rsquo;m interested in using R for a variety of purposes, including (but not limited to) text analysis, linear mixed models, Monte Carlo simulations, and data visualization. I blog relatively frequently about using R to solve analytical problems in psychology.\n","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"0336037297dcc4c6450121fda8a7d747","permalink":"/project/r/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/r/","section":"project","summary":"Insights in statistics, visualization, and programming in R.","tags":["R"],"title":"R","type":"project"},{"authors":null,"categories":null,"content":"In the era of Big Data, we are awash in online textual data, such as Tweets, Google Books, and countless blogs. These sources represent a wealth of data. Sentiment analysis is a technique that automatically assesses the emotional content of text. In partnership with Dr. Saif Mohammad of the National Research Council Canada, I\u0026rsquo;m using sentiment analysis on thousands of online poetry submitted by children and adolescents. The goal of this project is to provide insight into how emotions change over developmental time.\n","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"fcd9fde7217a46963a2f8aa66fe9e1c4","permalink":"/project/sentiment-analysis/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/sentiment-analysis/","section":"project","summary":"Identifying emotions in text - on massive scales.","tags":["Sentiment Analysis"],"title":"Sentiment Analysis","type":"project"},{"authors":null,"categories":null,"content":"","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"b10a15cb942f64a2c55723092ecd2489","permalink":"/project/social-development-in-different-cultures/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/social-development-in-different-cultures/","section":"project","summary":"Research on socio-emotional development in different cultures","tags":["Social Development"],"title":"Social Development in Different Cultures","type":"project"},{"authors":null,"categories":null,"content":"","date":1547010000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547010000,"objectID":"9505ac3cd505c795e1dfbb350b31c3ba","permalink":"/project/social-withdrawal-and-socio-emotional-development/","publishdate":"2019-01-09T00:00:00-05:00","relpermalink":"/project/social-withdrawal-and-socio-emotional-development/","section":"project","summary":"Exploring the socio-emotional consequences of social withdrawal across development.","tags":["Social Development"],"title":"Social Withdrawal and Socio-emotional Development","type":"project"},{"authors":["J Liu","B Xiao","WE Hipson","RJ Coplan","P Yang","C Cheah"],"categories":null,"content":"","date":1535774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535774400,"objectID":"a12d111d608ba103d805e9c9a921fba3","permalink":"/publication/liu-et-al.-2018/","publishdate":"2018-09-01T00:00:00-04:00","relpermalink":"/publication/liu-et-al.-2018/","section":"publication","summary":"The ability to intentionally control behavior to achieve specific goals helps children concentrate in school and behave appropriately in social situations. In Chinese culture, where self-regulation is highly valued by parents and teachers, children’s difficulties self-regulating may contribute to increased learning problems and subsequent authoritarian parenting. In this study we explored the longitudinal linkages among Chinese children’s self-regulation, learning problems, and authoritarian parenting using a developmental cascades model. Participants were N = 617 primary school students in Shanghai, P.R. China followed over three years from Grade 3–4 to Grade 5–6. Measures of children’s self-regulation, learning problems, and maternal authoritarian parenting were obtained each year from a combination of child self-reports and maternal and teacher ratings. Among the results: (1) compared with the unidirectional and bidirectional models, the developmental cascades model was deemed the best fit for the data; (2) earlier self-regulation negatively predicted later authoritarian parenting via a pathway through academic performance; (3) academic performance directly and indirectly contributed to greater self-regulation. Results are discussed in terms of the implications of self-regulation for Chinese children’s academic success and authoritarian parenting practices.","tags":[],"title":"Self-regulation, learning problems, and maternal authoritarian parenting in Chinese children: A developmental cascades model","type":"publication"},{"authors":["S Sette","WE Hipson","F Zava","E Baumgartner","RJ Coplan"],"categories":null,"content":"","date":1515387600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515387600,"objectID":"6960812a1a073e28c62c193a091c2b0c","permalink":"/publication/sette-et-al.-2018/","publishdate":"2018-01-08T00:00:00-05:00","relpermalink":"/publication/sette-et-al.-2018/","section":"publication","summary":"The aim of the present study was to examine the moderating role of inhibitory control (IC) in the associations between shyness and young children’s social and school adjustment. Participants were 112 Italian children (M = 56.85 months, SD = 10.14) enrolled in preschool. Parents and teachers assessed child shyness and IC as well as indices of social and school adjustment. Children were interviewed to assess vocabulary. Results from hierarchical multiple regression analyses revealed several significant interaction effects between shyness and IC in the prediction of outcome variables. Follow-up simple slope analyses indicated that among children with higher levels of IC, shyness was negatively related to prosocial behavior and popularity. In contrast, among children with lower levels of IC, shyness was positively associated with regulated school behaviors. Practice or Policy: The findings provide evidence to suggest that the combination of shyness and IC may contribute to children’s behavioral rigidity, which in turn may promote social and school adjustment difficulties.","tags":[],"title":"Linking shyness with social and school adjustment in early childhood: The moderating role of inhibitory control","type":"publication"},{"authors":["J Liu","B Xiao","WE Hipson","RJ Coplan","D Li","X Chen"],"categories":null,"content":"","date":1509508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509508800,"objectID":"05ed29bd3f5d8c66b7fa16c1f607b5fc","permalink":"/publication/liu-et-al.-2016/","publishdate":"2017-11-01T00:00:00-04:00","relpermalink":"/publication/liu-et-al.-2016/","section":"publication","summary":"The purpose of this study was to explore the longitudinal links among Chinese children's self‐control, social experiences, and loneliness, largely from a developmental cascades perspective (which postulates mechanisms about how effects within a particular domain of functioning can impact across additional domains over time). Participants were N = 1,066 primary school students in Shanghai, P. R. China, who were followed over three years from Grade 3 to Grade 5. Measures of children's behavioral self‐control, peer preference, and loneliness were obtained each year from peer nominations and child self‐reports. Results indicated that as compared with the unidirectional and bidirectional models, the developmental cascade model represented the best fit for the data. Within this model, a number of significant direct and indirect pathways were identified among variables and over time. For example, self‐control was found to indirectly contribute to later decreases in loneliness via a pathway through peer preference. As well, peer preference both directly and indirectly contributed to later increases in self‐control. Finally, loneliness directly led to decreases in self‐control from Grade 3 to Grade 4, but not from Grade 4 to Grade 5. Results are discussed in terms of the implications of self‐control for Chinese children's social and emotional functioning over time.","tags":[],"title":"Self-control, peer preference, and loneliness in Chinese children: A three-year longitudinal study","type":"publication"},{"authors":["WE Hipson","SL Gardiner","RJ Coplan","LL Ooi"],"categories":null,"content":"","date":1487739600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487739600,"objectID":"fe7474526c8e397ce138e41b5cb96c6d","permalink":"/publication/hipson-et-al.-2017/","publishdate":"2017-02-22T00:00:00-05:00","relpermalink":"/publication/hipson-et-al.-2017/","section":"publication","summary":"The goal of this study was to explore associations among maternal agreeableness, child temperament (i.e., emotion dysregulation), and children's social adjustment at school. Participants were 146 children in kindergarten and Grade 1 (76 girls; Mage = 67.78 months, SD = 10.81 months). Mothers provided ratings of their own agreeableness and their child's temperament, and teachers assessed indices of children's socioemotional functioning at school. Among the results, maternal agreeableness moderated associations between child dysregulation and aspects of adjustment at school. Specifically, at higher levels of maternal agreeableness, the relations between child dysregulation and both anxiety with peers and their prosocial behavior were attenuated. Overall, the results suggest that maternal agreeableness may serve as a protective factor for dysregulated children. Implications for research and practice are discussed.","tags":[],"title":"Maternal agreeableness moderates associations between young children’s emotion dysregulation and socio-emotional functioning at school","type":"publication"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]