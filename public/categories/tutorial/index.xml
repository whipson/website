<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tutorial on Will Hipson</title>
    <link>/categories/tutorial/</link>
    <description>Recent content in Tutorial on Will Hipson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Sun, 27 Oct 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/tutorial/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Building a Shiny App for Cycling in Ottawa</title>
      <link>/post/bicycle_app/bicycle_app/</link>
      <pubDate>Sun, 27 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bicycle_app/bicycle_app/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/leaflet/leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/leafletfix/leafletfix.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/Proj4Leaflet/proj4-compressed.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/Proj4Leaflet/proj4leaflet.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/rstudio_leaflet/rstudio_leaflet.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet-binding/leaflet.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet-providers/leaflet-providers.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/leaflet-providers-plugin/leaflet-providers-plugin.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;This is a different kind of post, but one that I think is kind of fun. I currently live in Ottawa, which for those who don’t know, is the capital city of Canada. For a capital city, it’s fairly small, but it’s increasingly urbanizing (we just got lightrail transit). Segregated bicycle lanes and paths are becoming more common too and many of these paths have trackers on them that count how many bicycles cross a particular street or path each day. What’s great is that this &lt;a href=&#34;https://open.ottawa.ca/datasets/bicycle-trip-counters&#34;&gt;data is shared publicly&lt;/a&gt; by the city.&lt;/p&gt;
&lt;p&gt;I started looking into this data, &lt;a href=&#34;https://github.com/whipson/Ottawa_Bicycles/blob/master/cleaning.R&#34;&gt;cleaned it up&lt;/a&gt;, and eventually put it together in an &lt;a href=&#34;https://whipson.shinyapps.io/Ottawa_Bike_Counters/&#34;&gt;interactive web app&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/post/bicycle_app/Bicycle_App_files/ottawa%20bike%20counters.png&#34; alt=&#34;Click here to go the app.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Click &lt;a href=&#34;https://whipson.shinyapps.io/Ottawa_Bike_Counters/&#34;&gt;here&lt;/a&gt; to go the app.&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(leaflet)
library(leafpop)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll start by reading in the data from the GitHub repositiory. There’s a lot of missing data, so much that R gets confused about the data structure of some of the columns. We need to add another argument to &lt;em&gt;read_csv&lt;/em&gt; telling it the type of data in each column. The &lt;em&gt;col_types&lt;/em&gt; argument takes a letter for each column, with &lt;em&gt;?&lt;/em&gt; meaning that we let R decide what the data is and &lt;em&gt;n&lt;/em&gt; meaning ‘numeric’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bikes &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/whipson/Ottawa_Bicycles/master/bikes_app.csv&amp;quot;, col_types = c(&amp;quot;?nnnnnnnnnnnnnn&amp;quot;))

bikes&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3,560 x 15
##    date                alexandra_bridge eastern_canal ottawa_river
##    &amp;lt;dttm&amp;gt;                         &amp;lt;dbl&amp;gt;         &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 2010-01-01 00:00:00                0             0            0
##  2 2010-01-02 00:00:00                0             0            0
##  3 2010-01-03 00:00:00                0             0            0
##  4 2010-01-04 00:00:00                0             0            0
##  5 2010-01-05 00:00:00                0             0            0
##  6 2010-01-06 00:00:00                0             0            0
##  7 2010-01-07 00:00:00                0             0            0
##  8 2010-01-08 00:00:00                0             0            0
##  9 2010-01-09 00:00:00                0             0            0
## 10 2010-01-10 00:00:00                0             0            0
## # ... with 3,550 more rows, and 11 more variables: western_canal &amp;lt;dbl&amp;gt;,
## #   laurier_bay &amp;lt;dbl&amp;gt;, laurier_lyon &amp;lt;dbl&amp;gt;, laurier_metcalfe &amp;lt;dbl&amp;gt;,
## #   somerset_bridge &amp;lt;dbl&amp;gt;, otrain_young &amp;lt;dbl&amp;gt;, otrain_gladstone &amp;lt;dbl&amp;gt;,
## #   otrain_bayview &amp;lt;dbl&amp;gt;, portage_bridge &amp;lt;dbl&amp;gt;, adawe_crossing_a &amp;lt;dbl&amp;gt;,
## #   adawe_crossing_b &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each row is a day and the columns are bicycle counters spread across the city. Let’s start by creating the graphs we want in the Shiny app. It’s easier to do this outside of the Shiny framework first. We’ll start by plotting total bicycle counts over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bikes_total &amp;lt;- bikes %&amp;gt;%
  pivot_longer(names_to = &amp;quot;counter&amp;quot;, values_to = &amp;quot;count&amp;quot;, -date) %&amp;gt;%
  group_by(date) %&amp;gt;%
  mutate(daily_total = sum(count, na.rm = TRUE))

bikes_total&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 49,840 x 4
## # Groups:   date [3,560]
##    date                counter          count daily_total
##    &amp;lt;dttm&amp;gt;              &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
##  1 2010-01-01 00:00:00 alexandra_bridge     0           0
##  2 2010-01-01 00:00:00 eastern_canal        0           0
##  3 2010-01-01 00:00:00 ottawa_river         0           0
##  4 2010-01-01 00:00:00 western_canal       NA           0
##  5 2010-01-01 00:00:00 laurier_bay         NA           0
##  6 2010-01-01 00:00:00 laurier_lyon        NA           0
##  7 2010-01-01 00:00:00 laurier_metcalfe    NA           0
##  8 2010-01-01 00:00:00 somerset_bridge     NA           0
##  9 2010-01-01 00:00:00 otrain_young        NA           0
## 10 2010-01-01 00:00:00 otrain_gladstone    NA           0
## # ... with 49,830 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now to plot it over time:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bikes_total %&amp;gt;%
  ggplot(aes(x = date, y = daily_total)) +
  geom_line(size = .5, alpha = .80, color = &amp;quot;#36648B&amp;quot;) +
  scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
  labs(x = NULL,
       y = &amp;quot;Count&amp;quot;,
       title = &amp;quot;Total Bicycle Crossings in Ottawa&amp;quot;,
       subtitle = &amp;quot;Jan 2010 - Sep 2019&amp;quot;) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5),
        axis.text.x = element_text(size = 16))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bicycle_app/Bicycle_App_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s clear seasonality, with bicycle crossings peaking in the summer months and troughing in the winter. There also appears to be a trend, increasing from 2010 to 2017, then leveling out. Does this mean that bicycling is leveling off in Ottawa? We may want to look at specific counters to get a better sense of this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bikes %&amp;gt;%
  pivot_longer(names_to = &amp;quot;counter&amp;quot;, values_to = &amp;quot;count&amp;quot;, -date) %&amp;gt;% 
  ggplot(aes(x = date, y = count)) +
  geom_line(size = .5, alpha = .80, color = &amp;quot;#36648B&amp;quot;) +
  labs(x = NULL,
       y = &amp;quot;Count&amp;quot;,
       title = &amp;quot;Bicycle Crossings in Ottawa by Location&amp;quot;,
       subtitle = &amp;quot;Jan 2010 - Sep 2019&amp;quot;) +
  facet_wrap(~counter) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5),
        axis.text.x = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 2191 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bicycle_app/Bicycle_App_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This graph tells us that we have to be a bit careful about interpreting the total count because some counters are introduced later or go out of commission. The drop in total counts for 2018 could be due to the Western Canal counter going offline that year. What about average counts over time?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bikes %&amp;gt;%
  pivot_longer(names_to = &amp;quot;counter&amp;quot;, values_to = &amp;quot;count&amp;quot;, -date) %&amp;gt;%
  group_by(date) %&amp;gt;%
  mutate(daily_average = mean(count, na.rm = TRUE)) %&amp;gt;%
  ggplot(aes(x = date, y = daily_average)) +
  geom_line(size = .5, alpha = .80, color = &amp;quot;#36648B&amp;quot;) +
  scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
  labs(x = NULL,
       y = &amp;quot;Count&amp;quot;,
       title = &amp;quot;Average Bicycle Crossings in Ottawa&amp;quot;,
       subtitle = &amp;quot;Jan 2010 - Sep 2019&amp;quot;) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5),
        axis.text.x = element_text(size = 16))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bicycle_app/Bicycle_App_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There may be an upward trend, but it’s less clear compared to the total count. We again have to be careful because earlier years have fewer counters online so the average is based on less data. However, knowing both the total and the average counts gives us a pretty clear picture of how cycling is changing over time in Ottawa.&lt;/p&gt;
&lt;div id=&#34;maps-with-leaflet&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Maps with Leaflet&lt;/h2&gt;
&lt;p&gt;Now we’ll add the functionality of an interactive map - one that shows where the counters are located geographically and allows the user to select specific counters. Earlier we loaded up the &lt;em&gt;leaflet&lt;/em&gt; and &lt;em&gt;leafpop&lt;/em&gt; packages. These will help us construct our map of Ottawa.&lt;/p&gt;
&lt;p&gt;We’ll also need the latitude and longitude coordinates of the counters. Using information from the &lt;a href=&#34;https://open.ottawa.ca/datasets/bicycle-trip-counters&#34;&gt;Open Data Ottawa&lt;/a&gt;, I found the location of each counter and obtained its latitude and longitude using Google Maps. I also added a bit of descriptive information for each counter. We can put all of this in a dataframe as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coords &amp;lt;- data.frame(counter = names(bikes[,2:15]),
                     name = c(&amp;quot;Alexandra Bridge&amp;quot;, &amp;quot;Eastern Canal Pathway&amp;quot;, &amp;quot;Ottawa River Pathway&amp;quot;, &amp;quot;Western Canal Pathway&amp;quot;,
                              &amp;quot;Laurier at Bay&amp;quot;, &amp;quot;Laurier at Lyon&amp;quot;, &amp;quot;Laurier at Metcalfe&amp;quot;, &amp;quot;Somerset Bridge&amp;quot;, &amp;quot;OTrain at Young&amp;quot;,
                              &amp;quot;OTrain at Gladstone&amp;quot;, &amp;quot;OTrain at Bayview&amp;quot;, &amp;quot;Portage Bridge&amp;quot;, &amp;quot;Adawe Crossing A&amp;quot;, &amp;quot;Adawe Crossing B&amp;quot;),
                     lat = c(45.430366, 45.420924, 45.411959, 45.406280,
                             45.415893, 45.417036, 45.419790, 45.420512,
                             45.402859, 45.404599, 45.408636, 45.421980, 
                             45.426282, 45.426575),
                     long = c(-75.704761, -75.685060, -75.723424, -75.681814,
                              -75.705328, -75.702613, -75.697623, -75.684625,
                              -75.712760, -75.714812, -75.723644, -75.713324,
                              -75.670234, -75.669765),
                     desc = c(&amp;quot;Ottawa approach to the NCC Alexandra Bridge Bikeway. This counter was not operational for most of 2010
                              due to bridge construction. This is one of the more consistent counters, until the internal battery
                              failed in August 2019.&amp;quot;,
                              &amp;quot;NCC Eastern Canal Pathway approximately 100m north of the Corktown Bridge.&amp;quot;,
                              &amp;quot;NCC Ottawa River Pathway approximately 100m east of the Prince of Wales Bridge. Canada Day in 2011
                              boasts the highest single day count of any counter.&amp;quot;,
                              &amp;quot;NCC Western Canal Pathway approximately 200m north of “The Ritz”. Out of operation for much of 2018.
                              MEC Bikefest on May 17, 2015 accounts for the large spike that day.&amp;quot;,
                              &amp;quot;Laurier Segregated Bike lane just west of Bay. Minimal data available due to inactivity after 2014.&amp;quot;,
                              &amp;quot;Laurier Segregated Bike lane just east of Lyon. No longer in operation since 2016.&amp;quot;,
                              &amp;quot;Laurier Segregated Bike lane just west of Metcalfe. Construction in late 2012 accounts for unusual dip
                              in counts.&amp;quot;,
                              &amp;quot;Somerset bridge over O-Train west-bound direction only. Inexplicably large spike in 2012 followed by a
                              typical seasonal pattern. Inactive since late 2018.&amp;quot;,
                              &amp;quot;O-Train Pathway just north of Young Street. Minimal data available due to inactivity after 2016. See
                              O-Train at Gladstone counter for a better estimate.&amp;quot;,
                              &amp;quot;O-Train Pathway just north of Gladstone Avenue. In operation since mid-2013. Shows unusual spike in
                              November of 2017.&amp;quot;,
                              &amp;quot;O-Train Pathway just north of Bayview Station. In operation since mid-2013. Trending upward.&amp;quot;,
                              &amp;quot;Portage Bridge connecting Gatineau to Ottawa. Installed in late 2013, this counter registered
                              relatively high traffic but seems to have experienced outages during Winter months. Inactive since early
                              2016.&amp;quot;,
                              &amp;quot;Adàwe Crossing Bridge bike lane. This counter is one of a pair on this pedestrian bridge. Installed in
                              2016, it seems to have experienced an outage during the Winter of its inaugural year.&amp;quot;,
                              &amp;quot;The second of two counters on the Adàwe Crossing Bridge. This counter may pick up more pedestrian than
                              bike traffic, as suggested by the trend over time.&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we just pipe the coordinate data into leaflet.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;leaflet(data = coords) %&amp;gt;%
  addTiles() %&amp;gt;%
  addMarkers(~long, ~lat)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;leaflet html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;options&#34;:{&#34;crs&#34;:{&#34;crsClass&#34;:&#34;L.CRS.EPSG3857&#34;,&#34;code&#34;:null,&#34;proj4def&#34;:null,&#34;projectedBounds&#34;:null,&#34;options&#34;:{}}},&#34;calls&#34;:[{&#34;method&#34;:&#34;addTiles&#34;,&#34;args&#34;:[&#34;//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&#34;,null,null,{&#34;minZoom&#34;:0,&#34;maxZoom&#34;:18,&#34;tileSize&#34;:256,&#34;subdomains&#34;:&#34;abc&#34;,&#34;errorTileUrl&#34;:&#34;&#34;,&#34;tms&#34;:false,&#34;noWrap&#34;:false,&#34;zoomOffset&#34;:0,&#34;zoomReverse&#34;:false,&#34;opacity&#34;:1,&#34;zIndex&#34;:1,&#34;detectRetina&#34;:false,&#34;attribution&#34;:&#34;&amp;copy; &lt;a href=\&#34;http://openstreetmap.org\&#34;&gt;OpenStreetMap&lt;\/a&gt; contributors, &lt;a href=\&#34;http://creativecommons.org/licenses/by-sa/2.0/\&#34;&gt;CC-BY-SA&lt;\/a&gt;&#34;}]},{&#34;method&#34;:&#34;addMarkers&#34;,&#34;args&#34;:[[45.430366,45.420924,45.411959,45.40628,45.415893,45.417036,45.41979,45.420512,45.402859,45.404599,45.408636,45.42198,45.426282,45.426575],[-75.704761,-75.68506,-75.723424,-75.681814,-75.705328,-75.702613,-75.697623,-75.684625,-75.71276,-75.714812,-75.723644,-75.713324,-75.670234,-75.669765],null,null,null,{&#34;interactive&#34;:true,&#34;draggable&#34;:false,&#34;keyboard&#34;:true,&#34;title&#34;:&#34;&#34;,&#34;alt&#34;:&#34;&#34;,&#34;zIndexOffset&#34;:0,&#34;opacity&#34;:1,&#34;riseOnHover&#34;:false,&#34;riseOffset&#34;:250},null,null,null,null,null,{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null]}],&#34;limits&#34;:{&#34;lat&#34;:[45.402859,45.430366],&#34;lng&#34;:[-75.723644,-75.669765]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Leaflet automatically generates a map of size to fit all the markers. There are a few modifications to make though. One is to have it so that when the user hovers the mouse over a marker a label pops up with the name of that counter. Another is to make the map more aesthetically pleasing. Finally, we may want to add some bounds so that the user can’t scroll too far away from the markers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;leaflet(data = coords) %&amp;gt;%
  addTiles() %&amp;gt;%
  addMarkers(~long, ~lat, label = ~name) %&amp;gt;%
  setMaxBounds(-75.65, 45.38, -75.75, 45.46) %&amp;gt;%
  addProviderTiles(providers$CartoDB.Positron)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;leaflet html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;options&#34;:{&#34;crs&#34;:{&#34;crsClass&#34;:&#34;L.CRS.EPSG3857&#34;,&#34;code&#34;:null,&#34;proj4def&#34;:null,&#34;projectedBounds&#34;:null,&#34;options&#34;:{}}},&#34;calls&#34;:[{&#34;method&#34;:&#34;addTiles&#34;,&#34;args&#34;:[&#34;//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&#34;,null,null,{&#34;minZoom&#34;:0,&#34;maxZoom&#34;:18,&#34;tileSize&#34;:256,&#34;subdomains&#34;:&#34;abc&#34;,&#34;errorTileUrl&#34;:&#34;&#34;,&#34;tms&#34;:false,&#34;noWrap&#34;:false,&#34;zoomOffset&#34;:0,&#34;zoomReverse&#34;:false,&#34;opacity&#34;:1,&#34;zIndex&#34;:1,&#34;detectRetina&#34;:false,&#34;attribution&#34;:&#34;&amp;copy; &lt;a href=\&#34;http://openstreetmap.org\&#34;&gt;OpenStreetMap&lt;\/a&gt; contributors, &lt;a href=\&#34;http://creativecommons.org/licenses/by-sa/2.0/\&#34;&gt;CC-BY-SA&lt;\/a&gt;&#34;}]},{&#34;method&#34;:&#34;addMarkers&#34;,&#34;args&#34;:[[45.430366,45.420924,45.411959,45.40628,45.415893,45.417036,45.41979,45.420512,45.402859,45.404599,45.408636,45.42198,45.426282,45.426575],[-75.704761,-75.68506,-75.723424,-75.681814,-75.705328,-75.702613,-75.697623,-75.684625,-75.71276,-75.714812,-75.723644,-75.713324,-75.670234,-75.669765],null,null,null,{&#34;interactive&#34;:true,&#34;draggable&#34;:false,&#34;keyboard&#34;:true,&#34;title&#34;:&#34;&#34;,&#34;alt&#34;:&#34;&#34;,&#34;zIndexOffset&#34;:0,&#34;opacity&#34;:1,&#34;riseOnHover&#34;:false,&#34;riseOffset&#34;:250},null,null,null,null,[&#34;Alexandra Bridge&#34;,&#34;Eastern Canal Pathway&#34;,&#34;Ottawa River Pathway&#34;,&#34;Western Canal Pathway&#34;,&#34;Laurier at Bay&#34;,&#34;Laurier at Lyon&#34;,&#34;Laurier at Metcalfe&#34;,&#34;Somerset Bridge&#34;,&#34;OTrain at Young&#34;,&#34;OTrain at Gladstone&#34;,&#34;OTrain at Bayview&#34;,&#34;Portage Bridge&#34;,&#34;Adawe Crossing A&#34;,&#34;Adawe Crossing B&#34;],{&#34;interactive&#34;:false,&#34;permanent&#34;:false,&#34;direction&#34;:&#34;auto&#34;,&#34;opacity&#34;:1,&#34;offset&#34;:[0,0],&#34;textsize&#34;:&#34;10px&#34;,&#34;textOnly&#34;:false,&#34;className&#34;:&#34;&#34;,&#34;sticky&#34;:true},null]},{&#34;method&#34;:&#34;setMaxBounds&#34;,&#34;args&#34;:[45.38,-75.65,45.46,-75.75]},{&#34;method&#34;:&#34;addProviderTiles&#34;,&#34;args&#34;:[&#34;CartoDB.Positron&#34;,null,null,{&#34;errorTileUrl&#34;:&#34;&#34;,&#34;noWrap&#34;:false,&#34;detectRetina&#34;:false}]}],&#34;limits&#34;:{&#34;lat&#34;:[45.402859,45.430366],&#34;lng&#34;:[-75.723644,-75.669765]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Great. So we now have the two components of the app: the time plots and the map. Time to bring in Shiny and put it all together. Now, if you have never used Shiny before, this probably isn’t the easiest example to start with. I’d highly recommend this &lt;a href=&#34;https://shiny.rstudio.com/tutorial/&#34;&gt;set of tutorial videos by Garrett Grolemund&lt;/a&gt; to get started.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-shiny-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating the Shiny App&lt;/h2&gt;
&lt;p&gt;There are two parts to every Shiny app: the &lt;em&gt;UI&lt;/em&gt; or User Interface and the &lt;em&gt;Server&lt;/em&gt;. The UI is like the look and feel of the app, it’s where we tell Shiny what kinds of inputs and outputs we want, how we want to organize the panels, and so on. In contrast, the Server is the engine of the app. We’ll start by constructing the UI. It’s important to note that it’s easier to build a Shiny app in a new R script. So we’re basically going to start over in a new script, which means we’ll reload the packages and the data as if we were starting new:&lt;/p&gt;
&lt;div id=&#34;create-a-new-r-script&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create a new R script&lt;/h3&gt;
&lt;p&gt;We’ll start with the packages and data. We haven’t done anything with the UI or Server yet. We usually want to keep the data outside the UI. We’ll also transform our data as we did earlier to generate the total and average time plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(leaflet)
library(leafpop)
library(shiny)
library(shinythemes)
library(shinyWidgets)

bikes &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/whipson/Ottawa_Bicycles/master/bikes_app.csv&amp;quot;, col_types = c(&amp;quot;?nnnnnnnnnnnnnn&amp;quot;))

#For ease, I&amp;#39;ve put the coordinates in a separate file, but you could just as easily rerun the &amp;#39;coords&amp;#39; object above

coords &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/whipson/Ottawa_Bicycles/master/coords.csv&amp;quot;)

bikes_plot &amp;lt;- bikes %&amp;gt;%
  pivot_longer(names_to = &amp;quot;counter&amp;quot;, values_to = &amp;quot;count&amp;quot;, -date) %&amp;gt;%
  left_join(coords, by = &amp;quot;counter&amp;quot;)

bikes_total &amp;lt;- bikes_plot %&amp;gt;%
  group_by(date) %&amp;gt;%
  summarize(count = sum(count, na.rm = TRUE))

bikes_mean &amp;lt;- bikes_plot %&amp;gt;%
  group_by(date) %&amp;gt;%
  summarize(count = mean(count, na.rm = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, still in the same R script, we can build the UI. It’s going to look a bit strange with parentheses all over the place. It’s just customary Shiny scripting to use hanging parentheses.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specifying-the-ui&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specifying the UI&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ui &amp;lt;- fluidPage(theme = shinytheme(&amp;quot;flatly&amp;quot;),

  sidebarLayout(  #Layout
    
    sidebarPanel(id = &amp;quot;Sidebar&amp;quot;,  #Side panel
                 h2(&amp;quot;Ottawa Bicycle Counters&amp;quot;, align = &amp;quot;center&amp;quot;, tags$style(&amp;quot;#Sidebar{font-family: Verdana;}&amp;quot;)),
                 fluidRow(  # Row 1 of side panel
                   htmlOutput(&amp;quot;caption&amp;quot;),  # Caption output, provides descriptive text
                   tags$style(&amp;quot;#caption{font-size: 16px; height: 200px; font-family: Verdana;}&amp;quot;)
                 ),
                 fluidRow(  # Row 2 of side panel
                   htmlOutput(&amp;quot;stats&amp;quot;),  # Statistics output, provides descriptive statistics 
                   tags$style(&amp;quot;#stats{font-size: 16px; height: 125px; font-family: Verdana;}&amp;quot;)
                 ),
                 fluidRow(  # Row 3 of side panel
                   switchInput(&amp;quot;average&amp;quot;,  # User input, allows the user to turn a switch to display the average
                               &amp;quot;Display Average&amp;quot;,
                               value = FALSE)
                 ),
                 fluidRow(  # Row 4 of side panel
                   htmlOutput(&amp;quot;caption2&amp;quot;),  # More caption output
                   tags$style(&amp;quot;#caption2{font-size: 12px; height: 80px; font-family: Verdana;}&amp;quot;)
                   ),
                 fluidRow(  # Row 5 of side panel 
                   downloadButton(&amp;quot;download&amp;quot;, &amp;quot;Download Data&amp;quot;)  # A button so that users can download the data
                   )
                 ),
    mainPanel(id = &amp;quot;Main&amp;quot;,  # Main panel (this is where the plots and map go)
              fluidRow(  # Row 1 of main panel
                leafletOutput(&amp;quot;map&amp;quot;, height = 400)  # Here&amp;#39;s the output for the map
                ),
              fluidRow(  # Row 2 of main panel
                plotOutput(&amp;quot;timeplot&amp;quot;, height = 300)  # Here&amp;#39;s the output for the time plots
                )
              )
    )
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There’s the code for the UI. Starting from the top, we use the &lt;em&gt;FluidPage&lt;/em&gt; function and here I’m using the theme &lt;em&gt;flatly&lt;/em&gt;. Then I say that I want to use a &lt;em&gt;sidebarLayout&lt;/em&gt;. From here, I split the code into a &lt;em&gt;sidebarPanel&lt;/em&gt; and a &lt;em&gt;mainPanel&lt;/em&gt;. I further split things into &lt;em&gt;fluidRows&lt;/em&gt; which just helps to organize the layout. All of the #s are notes, of course, and will not actually be run.&lt;/p&gt;
&lt;p&gt;The big thing to notice is that there are inputs and outputs. The only input is a &lt;em&gt;switchInput&lt;/em&gt; which lets the user choose whether to display totals or averages. Everything else is an output. Each of these gets a name, for example, I’m calling the leafletOutput &lt;em&gt;map&lt;/em&gt;. These names are important, as they will correspond with what we provide in the server part.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;specifying-the-server&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specifying the Server&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;server &amp;lt;- function(input, output) {
  
  output$map &amp;lt;- renderLeaflet({  # Map output
      leaflet(data = coords) %&amp;gt;%
         addTiles() %&amp;gt;%
         addMarkers(~long, ~lat, label = ~name) %&amp;gt;%
         setMaxBounds(-75.65, 45.38, -75.75, 45.46) %&amp;gt;%
         addProviderTiles(providers$CartoDB.Positron)
    })
  
  output$caption2 &amp;lt;- renderUI({  # Lower caption output
    str1 &amp;lt;- paste(&amp;quot;Created by &amp;quot;, a(&amp;quot;Will Hipson.&amp;quot;, href = &amp;quot;https://willhipson.netlify.com/&amp;quot;))
    str2 &amp;lt;- paste(&amp;quot;Data courtesy of &amp;quot;, a(&amp;quot;Open Data Ottawa.&amp;quot;, href = &amp;quot;https://open.ottawa.ca/datasets/bicycle-trip-counters&amp;quot;))
    str3 &amp;lt;- &amp;quot;2010-01-01 - 2019-09-30&amp;quot;
    str4 &amp;lt;- &amp;quot;Updated on 2019-10-24&amp;quot;
    HTML(paste(str1, str2, str3, str4, sep = &amp;#39;&amp;lt;br/&amp;gt;&amp;#39;))
  })
  
  observeEvent(input$map_marker_click, { # If the user clicks a marker, this line is run.
    output$timeplot &amp;lt;- renderPlot({
      if(input$average == TRUE) { # if average is selected we get average overlayed
        ggplot() +
          geom_line(data = bikes_plot[bikes_plot$lat == input$map_marker_click$lat, ], 
                    aes(x = date, y = count), size = .5, alpha = .70, color = &amp;quot;#36648B&amp;quot;) +
          geom_line(data = bikes_mean, aes(x = date, y = count), alpha = .50, color = &amp;quot;#9F79EE&amp;quot;) +
          scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
          scale_y_continuous(limits = c(0, 6000)) +
          labs(x = NULL,
               y = &amp;quot;Count&amp;quot;,
               title = paste(bikes_plot[bikes_plot$lat == input$map_marker_click$lat,]$name)) +
          theme_minimal(base_size = 16) +
          theme(plot.title = element_text(hjust = .5),
                axis.text.x = element_text(size = 16),
                text = element_text(family = &amp;quot;Verdana&amp;quot;))
      } else { # if average is not selected, then it&amp;#39;s just the total
        ggplot() +
          geom_line(data = bikes_plot[bikes_plot$lat == input$map_marker_click$lat, ], 
                    aes(x = date, y = count), size = .5, alpha = .70, color = &amp;quot;#36648B&amp;quot;) +
          scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
          scale_y_continuous(limits = c(0, 6000)) +
          labs(x = NULL,
               y = &amp;quot;Count&amp;quot;,
               title = paste(bikes_plot[bikes_plot$lat == input$map_marker_click$lat,]$name)) +
          theme_minimal(base_size = 16) +
          theme(plot.title = element_text(hjust = .5),
                axis.text.x = element_text(size = 16),
                text = element_text(family = &amp;quot;Verdana&amp;quot;))
      }
    })
    
    output$caption &amp;lt;- renderUI({ # counter specific description
      str1 &amp;lt;- coords[coords$lat == input$map_marker_click$lat, ]$desc
      HTML(str1)
    })
    
    output$stats &amp;lt;- renderUI({ # counter specific statistics
      str1 &amp;lt;- &amp;quot;&amp;lt;b&amp;gt;Statistics&amp;lt;/b&amp;gt;&amp;quot;
      str2 &amp;lt;- paste(&amp;quot;Total count: &amp;quot;, format(round(sum(bikes_plot[bikes_plot$lat == input$map_marker_click$lat,]$count, na.rm = TRUE)), big.mark = &amp;quot;,&amp;quot;))
      str3 &amp;lt;- paste(&amp;quot;Average count: &amp;quot;, format(round(mean(bikes_plot[bikes_plot$lat == input$map_marker_click$lat,]$count, na.rm = TRUE), 1), big.mark = &amp;quot;,&amp;quot;))
      str4 &amp;lt;- paste(&amp;quot;Busiest day: &amp;quot;, bikes_plot[which.max(bikes_plot[bikes_plot$lat == input$map_marker_click$lat,]$count),]$date)
      HTML(paste(str1, str2, str3, str4, sep = &amp;#39;&amp;lt;br/&amp;gt;&amp;#39;))
      })
  })
    
  observeEvent(input$map_click, ignoreNULL = FALSE, {  # If the user clicks on the map it goes back to the cumulative data
    output$timeplot &amp;lt;- renderPlot({
      if(input$average == TRUE) {  # if the average is selected, it displays average
      ggplot(data = bikes_mean, aes(x = date, y = count)) +
          geom_line(size = .5, alpha = .70, color = &amp;quot;#36648B&amp;quot;) +
          scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
          labs(x = NULL,
               y = &amp;quot;Count&amp;quot;) +
          theme_minimal(base_size = 16) +
          theme(plot.title = element_text(hjust = .5),
                axis.text.x = element_text(size = 16),
                text = element_text(family = &amp;quot;Verdana&amp;quot;))
      } else { # if average is not selected it is the total
        ggplot(data = bikes_total, aes(x = date, y = count)) +
          geom_line(size = .5, alpha = .70, color = &amp;quot;#36648B&amp;quot;) +
          scale_x_datetime(date_breaks = &amp;quot;2 years&amp;quot;, date_labels = &amp;quot;%Y&amp;quot;) +
          labs(x = NULL,
               y = &amp;quot;Count&amp;quot;) +
          theme_minimal(base_size = 16) +
          theme(plot.title = element_text(hjust = .5),
                axis.text.x = element_text(size = 16),
                text = element_text(family = &amp;quot;Verdana&amp;quot;))
      }
    })
    
    output$caption &amp;lt;- renderUI({  # the default caption
      str1 &amp;lt;- &amp;quot;Presenting data from bicycle counters across Ottawa. There are 14 counters spread across the city. The graph below displays how daily counts change over time. Click on a map marker to select a specific counter.&amp;quot;
      HTML(str1)
    })
    
    output$stats &amp;lt;- renderUI({  # Statistics output
      str1 &amp;lt;- &amp;quot;&amp;lt;b&amp;gt;Statistics&amp;lt;/b&amp;gt;&amp;quot;
      str2 &amp;lt;- paste(&amp;quot;Total count: &amp;quot;, format(round(sum(bikes_total$count, na.rm = TRUE)), big.mark = &amp;quot;,&amp;quot;))
      str3 &amp;lt;- paste(&amp;quot;Average count: &amp;quot;, format(round(mean(bikes_total$count, na.rm = TRUE), 1), big.mark = &amp;quot;,&amp;quot;))
      str4 &amp;lt;- paste(&amp;quot;Busiest day: &amp;quot;, bikes_total[which.max(bikes_total$count),]$date)
      HTML(paste(str1, str2, str3, str4, sep = &amp;#39;&amp;lt;br/&amp;gt;&amp;#39;))
    })
  })
  
  output$download &amp;lt;- downloadHandler( # download button. Will turn &amp;#39;bikes&amp;#39; object into a csv file.
    filename = function() {
      paste(&amp;quot;ottawa_bikes&amp;quot;, &amp;quot;.csv&amp;quot;, sep = &amp;quot;&amp;quot;)
    },
    
    content = function(file) {
      write.csv(bikes, file)
    }
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code for the server is much busier and it can be overwhelming. Essentially we’re just saying what we want to do with the inputs and outputs. We generate a little code chunk for each output. Look at the first one for &lt;em&gt;map&lt;/em&gt;. This is where we generate the map. We say we want to &lt;em&gt;renderLeaflet&lt;/em&gt; and then we just copy the code that we made earlier into this block.&lt;/p&gt;
&lt;p&gt;Where things get a bit more complicated is when we want our output to change based on user input. If the user selects the switch that converts the data to averages, for example. I used if and else statements to modulate the output based on whether ‘average’ is selected. What happens, is when the user clicks on the switch, the value of input$average changes to TRUE. Using if and else functions, I just say what I want to happen when ‘average’ is TRUE and what happens if it’s FALSE.&lt;/p&gt;
&lt;p&gt;Finally, we want the user to be able to click on specific markers and have the output change to that specific marker. We use the &lt;em&gt;observeEvent&lt;/em&gt; function and specify the input, ‘map_marker_click’. We also want the user to be able to click off the marker to go back to the default output. Again, we use &lt;em&gt;observeEvent&lt;/em&gt; but now with ‘click_map’.&lt;/p&gt;
&lt;p&gt;Once we have all the other outputs in place for the downloads and the captions, we put it all together using the &lt;em&gt;shinyApp&lt;/em&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shinyApp(ui, server)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And there it is, a user-friendly app for exploring bicycling data in Ottawa. Future avenues include building in some time-series forecasting. It would be cool to show the user how the trend is expected to change over time.&lt;/p&gt;
&lt;p&gt;One last shout out to &lt;a href=&#34;https://open.ottawa.ca/datasets/bicycle-trip-counters&#34;&gt;Open Data Ottawa&lt;/a&gt; for sharing this data!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>Shiny</category>
      
            <category>Tutorial</category>
      
      
            <category>R</category>
      
            <category>Shiny</category>
      
            <category>Tutorial</category>
      
    </item>
    
    <item>
      <title>Bayesian Linear Mixed Models: Random Intercepts, Slopes, and Missing Data</title>
      <link>/post/bayesian_mlm/bayesian_mlm/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian_mlm/bayesian_mlm/</guid>
      <description>


&lt;p&gt;This past summer, I watched a brilliant &lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&amp;amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&#34;&gt;lecture series by Richard McElreath&lt;/a&gt; on Bayesian statistics. It honestly changed my whole outlook on statistics, so I couldn’t recommend it more (plus, McElreath is an engaging instructor). One of the most compelling cases for using Bayesian statistics is with a collection of statistical tools called &lt;em&gt;linear mixed models&lt;/em&gt; or &lt;em&gt;multilevel/hierarchical models&lt;/em&gt;. It’s common that data are grouped or clustered in some way. Often in psychology we have repeated observations nested within participants, so we know that data coming from the same participant will share some variance. Linear mixed models are powerful tools for dealing with multilevel data, usually in the form of modeling &lt;em&gt;random intercepts&lt;/em&gt; and &lt;em&gt;random slopes&lt;/em&gt;. In this tutorial I assume familiarity with linear regression and some background knowledge in Bayesian inference, such that you should have some familiarity with &lt;em&gt;priors&lt;/em&gt; and &lt;em&gt;posterior distributions&lt;/em&gt; (if not, go &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0732118X18300746?via%3Dihub&#34;&gt;here&lt;/a&gt;) or watch McElreath’s videos.&lt;/p&gt;
&lt;p&gt;Why use Bayesian instead of Frequentist statistics? One reason is that &lt;em&gt;prior&lt;/em&gt; knowledge about a parameter - for example, it’s distribution - can be incorporated in the model. Another reason especially relevant to linear mixed models is that we can easily include multiple random intercepts and slopes without running into the same stringent sample size requirements as with frequentist approaches. This is not an exhaustive list; more can be found &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0732118X18300746?via%3Dihub&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;setting-it-all-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting it All Up&lt;/h2&gt;
&lt;p&gt;Installing and running &lt;em&gt;brms&lt;/em&gt; is a bit more complicated than your run-of-the-mill R packages. Because &lt;em&gt;brms&lt;/em&gt; uses STAN as its back-end engine to perform Bayesian analysis, you will need to install &lt;em&gt;rstan&lt;/em&gt;. Carefully follow the instructions at this &lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34;&gt;link&lt;/a&gt; and you should have no problem. Once you’ve done that you should be able to install &lt;em&gt;brms&lt;/em&gt; and load it up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Rcpp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading &amp;#39;brms&amp;#39; package (version 2.10.0). Useful instructions
## can be found by typing help(&amp;#39;brms&amp;#39;). A more detailed introduction
## to the package is available through vignette(&amp;#39;brms_overview&amp;#39;).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s normal to get some warning messages upon loading the package. That’s fine. We can now load up the &lt;em&gt;tidyverse&lt;/em&gt; for data manipulation and visualization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is available on GitHub &lt;a href=&#34;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%205/Curran/CurranLong.sav&#34;&gt;here&lt;/a&gt; in a .sav format. You’ll need the &lt;em&gt;haven&lt;/em&gt; package to read it into R. Make sure you import it from your working directory or specify the path to your downloads folder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(haven)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset was curated by &lt;a href=&#34;http://curran.web.unc.edu/&#34;&gt;Patrick Curran&lt;/a&gt; for the purpose of demonstrating different approaches for handling multilevel, longitudinal data (see &lt;a href=&#34;https://jflournoy.github.io/assets/pdf/srcdmeth.pdf&#34;&gt;here&lt;/a&gt; for more info). The dataset consists of 405 children in the first two years of elementary school measured over four assessment points on reading recognition and antisocial behaviour. As well, it included time invariant covariates, such as emotional support and cognitive stimulation. For this post, we’ll focus on reading and cognitive stimulation, and we’ll use Bayesian Linear Mixed Models to address a number of questions about children’s reading ability.&lt;/p&gt;
&lt;p&gt;For now, we’ll omit assessment periods with missing data, but we’ll return to the issue of missing data at the end of this demonstration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat &amp;lt;- read_sav(&amp;quot;CurranLong.sav&amp;quot;) %&amp;gt;%
  select(id, occasion, read, homecog) %&amp;gt;%
  filter(complete.cases(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,325 x 4
##       id occasion  read homecog
##    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1    22        0   2.1      13
##  2    22        1   3.9      13
##  3    34        0   2.1       9
##  4    34        1   2.9       9
##  5    34        2   4.5       9
##  6    34        3   4.5       9
##  7    58        0   2.3       9
##  8    58        1   4.5       9
##  9    58        2   4.2       9
## 10    58        3   4.6       9
## # ... with 1,315 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is conveniently in long form already, which means that each row represents a single &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; assessment period for an &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; child. With no other data cleaning to do, we can move ahead with our research questions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-1-one-random-effect-no-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 1: One Random Effect, No Covariates&lt;/h2&gt;
&lt;p&gt;We’ll start with a population-level intercept and a random intercept for participants. Our formula looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the population intercept, &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; is a random intercept term for an &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; participant, and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ij}\)&lt;/span&gt; is a random noise term for an &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; participant and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; assessment period.&lt;/p&gt;
&lt;p&gt;If you find formulae distasteful, that’s OK. They’re not essential to understanding how linear mixed models work. conceptually, think of the &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; in the above formula as giving a little &lt;em&gt;nudge&lt;/em&gt; to the intercept for each participant.&lt;/p&gt;
&lt;p&gt;Here I’ll walk carefully through the &lt;em&gt;brms&lt;/em&gt; code and then each subsequent model will just build one this one. We start by providing our data. The &lt;em&gt;family&lt;/em&gt; argument is where we supply a distribution for our outcome variable, reading. We’ll use the default &lt;em&gt;gaussian&lt;/em&gt; to indicate that reading should be normally distributed. The next line is where we specify our model formula. If you’re familiar with mixed effects models in the &lt;em&gt;lme4&lt;/em&gt; package you’ll be happy to see there are no differences here. I’ve only included the population level intercept ‘1’ after ‘read ~’ to be precise, but this will be included by default. The random intercept is indicated by the ‘(1 | id)’ as in typical mixed effects notation. We then specify some priors. Our first prior is for the population-level &lt;em&gt;Intercept&lt;/em&gt;. We’ll specify a fairly wide normal distribution for the intercept.&lt;/p&gt;
&lt;p&gt;If this whole business of deciding on priors seems strange and arbitrary, just note that a wide prior such as this one will have little bearing on the posterior distribution. If instead we were extremely confident that the intercept was zero, we could use a narrow prior of ‘normal(0, .05)’ which would have a strong effect on the resulting posterior distribution.&lt;/p&gt;
&lt;p&gt;The next prior is for the &lt;em&gt;standard deviation&lt;/em&gt; of the random effects. Unlike an intercept which can be positive or negative, variance (and by association, standard deviation) can only be positive, so we specify a &lt;em&gt;cauchy&lt;/em&gt; distribution that constrains the sd to be positive. We do the same for &lt;em&gt;sigma&lt;/em&gt; which is the overall variability.&lt;/p&gt;
&lt;p&gt;The rest of the code has to do with how the Markov Chain Monte Carlo (MCMC) algorithm runs. Without going too much into the &lt;em&gt;whys&lt;/em&gt; (a more detailed treatise can be found &lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/sampling-the-imaginary.html#sampling-to-simulate-prediction&#34;&gt;here&lt;/a&gt;), we will run 2,000 chained simulations. The first 1,000 of these will be in the warmup period and we will be rejected. We will run 4 parallel chains on 4 cores (if your computer has fewer cores you will want to reduce this). To help the model converge, I’ve increased the adapt_delta and max_treedepth arguments. Finally, an arbitrary seed number for reproducibility.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read1 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             formula = read ~ 1 + (1 | id),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + (1 | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.54      0.08     0.37     0.68 1.00      992     1847
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     4.11      0.05     4.01     4.21 1.00     4866     3424
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.55      0.04     1.48     1.62 1.00     2219     2827
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does the output tell us? Right away, we see an estimate of an Intercept of 4.11 within a 95% &lt;em&gt;credible interval&lt;/em&gt; (you might call it the Bayesian version of a confidence interval, although they’re not quite the same thing) of 4.01 and 4.21. What’s more interesting is the estimate of standard deviation of the random intercepts ‘sd(Intercept)’. So there is person-level variability in reading scores. In turn, &lt;em&gt;sigma&lt;/em&gt; is the estimate of the overall variability in reading scores.&lt;/p&gt;
&lt;p&gt;We can further inspect these estimates by looking at the traceplots and the posterior distributions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(read1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each row corresponds with a parameter in the model. The traceplot shows the MCMC samples. These should resemble ‘hairy caterpillars’ - we don’t want to see the chain getting stuck near a particular value.&lt;/p&gt;
&lt;p&gt;For a more intuitive understanding of what the mixed effects model is doing, let’s compare a sample of the observed values with the model predicted values. &lt;em&gt;brms&lt;/em&gt; outputs fitted values using the &lt;em&gt;fitted&lt;/em&gt; function. We’ll sample 6 participants and see their observed reading scores and model-derived reading scores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read1))) %&amp;gt;%
  group_by(id) %&amp;gt;%
  nest() %&amp;gt;%
  sample_n(6) %&amp;gt;%
  unnest() %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 1: One Random Effect, No Covariates&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that each estimate is drawn toward the population-level mean and its cluster-level mean. This phenomenon is called &lt;em&gt;partial-pooling&lt;/em&gt; or &lt;em&gt;shrinkage&lt;/em&gt;. If we’d run the model without the random effect, all of the estimates (black circles) would be on a single horizontal line. But here we have some variation between individuals. Notice too how points further away from the means (population- and cluster-level) are pulled more strongly. However, there is clearly an increase in reading ability over time that is being ignored by this model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-2-two-random-effects-no-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 2: Two Random Effects, No Covariates&lt;/h2&gt;
&lt;p&gt;Let’s add a random intercept for assessment period (labelled ‘occasion’ in this dataset). This model will recognize that observations are nested within participants &lt;em&gt;and&lt;/em&gt; assessment periods. For now, we’ll consider assessment as a random effect because we are still only interested in the variability in reading scores. Here’s our new formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+w_{0j}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we have a term &lt;span class=&#34;math inline&#34;&gt;\(w_{0j}\)&lt;/span&gt; for the random intercept for the &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; assessment period.&lt;/p&gt;
&lt;p&gt;The only change to our model code is that we’ve added a second random intercept: ‘(1 | occasion)’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read2 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             read ~ 1 + (1 | id) + (1 | occasion),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + (1 | id) + (1 | occasion) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.89      0.04     0.82     0.96 1.00     1270     2122
## 
## ~occasion (Number of levels: 4) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.71      0.86     0.79     3.99 1.00     1701     1878
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     4.32      0.89     2.46     6.08 1.00     1426     1848
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.64      0.01     0.61     0.67 1.00     3630     3056
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking again at our output now, we see variability in the random intercepts for id, but even larger variability across occasions (assessment periods). In fact, the overall variability, sigma, is quite reduced from the previous model. This demonstrates an important lesson about mixed models: that they decompose the overall variance (sigma) into cluster-level variance. If we just ran a linear regression, it wouldn’t differentiate between measurement error and variation between participants and time points.&lt;/p&gt;
&lt;p&gt;Let’s again compare the estimates with the observed values. We’ll use the same subsample as before to illustrate the effect of adding a second random effect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read2))) %&amp;gt;%
  group_by(id) %&amp;gt;%
  nest() %&amp;gt;%
  sample_n(6) %&amp;gt;%
  unnest() %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 2: Two Random Effects, No Covariates&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the model now adjusts the points based on the assessment-level intercept. The fact that the black circles follow a nearly linear increase is telling us that there is probably an effect of time, such that children get better at reading over time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-3-one-random-effect-one-covariate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 3: One Random Effect, One Covariate&lt;/h2&gt;
&lt;p&gt;Now let’s make a conceptual shift. Maybe as researchers we decide that we’re interested in the effect of time (assessment period) on reading ability. Since each assessment was spaced equally apart in time and we have good reason to think that time has a global effect on reading, it makes sense to include it as a fixed effect (this sort of conceptual decision should be made beforehand, but for illustrative purposes I’ve shown it as a random and a fixed effect).&lt;/p&gt;
&lt;p&gt;It’s always a good idea to visualize what’s going on so that we know if our assumptions are tenable. we’ll create a lineplot showing the change in reading ability over time, but with each line representing an individual in our sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  ggplot(aes(x = occasion, y = read, group = id)) +
  geom_line(size = .75, alpha = .20) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;) +
  theme_minimal(base_size = 16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s a clear upward trend - most children improve in their reading over time. We can now proceed with formally testing the growth in reading:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+\beta_1Time_j+u_{0i}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s the model code. Note that we are still modeling the random effect of id, but now occasion is a covariate (predictor). We need to specify a new prior, &lt;em&gt;b&lt;/em&gt; for beta. The beta estimate for the effect of time on reading ability is assumed to derive from a normal distribution with mean 0 and sd 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read3 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             read ~ 1 + occasion + (1 | id),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(normal(0, 1), class = b),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + occasion + (1 | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.89      0.04     0.82     0.96 1.00     1204     1906
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.70      0.05     2.60     2.81 1.00     1036     1692
## occasion      1.10      0.02     1.07     1.14 1.00     6696     3112
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.68      0.02     0.65     0.71 1.00     3004     3138
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our output gives a smaller intercept because it’s not telling us the mean any more, instead it’s the y-intercept of the regression line. We have an estimate of beta as 1.10 within a 95% credible interval of 1.07 and 1.14. So there’s strong evidence of a positive effect of time on reading ability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read3))) %&amp;gt;%
  group_by(id) %&amp;gt;%
  nest() %&amp;gt;%
  sample_n(6) %&amp;gt;%
  unnest() %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 3: One Random Effect, One Covariate&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The estimates aren’t actually that different from the model with two random effects. The main difference now is that the black circles show a strictly linear trend that is the same across participants. But maybe this assumption of equal effects across the board isn’t tenable either.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-4-one-random-slope-one-covariate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 4: One Random Slope, One Covariate&lt;/h2&gt;
&lt;p&gt;Let’s ramp things up by modeling a random slope for each participant:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+\beta_1Time_j+u_{1ij}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(u_{1ij}\)&lt;/span&gt; is the random slope for the effect of assessment period between participants.&lt;/p&gt;
&lt;p&gt;A random slope model also has a random intercept, but now, the slope for time on reading ability will be different for each participant: ‘(1 + occasion | id)’. Another change to our model code is a new prior specifying the correlation between the Intercept and the beta for occasion. For an overview of the Cholesky Distribution I’d recommend this &lt;a href=&#34;https://www.psychstatistics.com/2014/12/27/d-lkj-priors/&#34;&gt;post&lt;/a&gt;, but all you need to know here is that the higher above 1 the value is, the more “speculative” the model is of strong correlations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read4 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             read ~ 1 + occasion + (1 + occasion | id),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(normal(0, 1), class = b),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma),
                       prior(lkj_corr_cholesky(1.5), class = cor)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + occasion + (1 + occasion | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)               0.76      0.04     0.68     0.84 1.00     1279
## sd(occasion)                0.27      0.02     0.22     0.32 1.00      614
## cor(Intercept,occasion)     0.29      0.12     0.07     0.53 1.00      664
##                         Tail_ESS
## sd(Intercept)               2218
## sd(occasion)                1082
## cor(Intercept,occasion)     1070
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.70      0.05     2.60     2.79 1.00     1465     2297
## occasion      1.12      0.02     1.08     1.16 1.00     2963     3299
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.59      0.02     0.56     0.62 1.00      908     1758
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read4))) %&amp;gt;%
  group_by(id) %&amp;gt;%
  nest() %&amp;gt;%
  sample_n(6) %&amp;gt;%
  unnest() %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 4: One Random Slope, One Covariate&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I won’t belabour the output here. Just notice how the strength of the linear trend for the fitted values is different for each participant. We can see this across the whole sample with the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read4))) %&amp;gt;%
  ggplot() +
  geom_line(aes(x = occasion, y = Estimate, group = id), size = .75, alpha = .30) +
  geom_line(aes(x = occasion, y = read, group = id), size = .75, alpha = .15, color = &amp;quot;dodgerblue2&amp;quot;) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 4: One Random Slope, One Covariate&amp;quot;,
       subtitle = &amp;quot;Blue lines are observed values. Black lines are fitted.&amp;quot;) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-5-one-random-slope-two-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 5: One Random Slope, Two Covariates&lt;/h2&gt;
&lt;p&gt;Suppose we wanted to see whether there’s an effect of the amount of &lt;em&gt;cognitive stimulation&lt;/em&gt; that children are exposed to at home on reading ability over time. Maybe cognitive stimulation impacts some children more strongly than others. Our last formula would thus be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+\beta_1Time_j+u_{1ij}+\beta_{2}Cog_i+u_{2ij}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Does the hypothesis hold up to a visual inspection? For illustrative purposes, we can split the data into equal bins of low, medium, and high cognitive stimulation. Of course, because stimulation is a continuous variable, it will stay continuous in our Bayesian model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  mutate(homecog_grp = factor(ntile(homecog, 3))) %&amp;gt;%
  ggplot(aes(x = occasion, y = read)) +
  geom_line(aes(group = id, color = homecog_grp), size = .75, alpha = .50) +
  geom_smooth(method = &amp;#39;lm&amp;#39;, color = &amp;quot;black&amp;quot;, size = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Cognitive Home Environment&amp;quot;) +
  facet_wrap(~homecog_grp, labeller = as_labeller(c(&amp;quot;1&amp;quot; = &amp;quot;Low&amp;quot;,
                                                    &amp;quot;2&amp;quot; = &amp;quot;Medium&amp;quot;,
                                                    &amp;quot;3&amp;quot; = &amp;quot;High&amp;quot;))) +
  guides(color = FALSE) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5, size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If there is an effect of cognitive stimulation, it’s a small one. So let’s find out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read5 &amp;lt;- brm(data = curran_dat,
            family = gaussian,
            read ~ 1 + occasion + homecog + (1 + occasion + homecog | id),
            prior = c(prior(normal(0, 5), class = Intercept),
                      prior(normal(0, 1), class = b),
                      prior(cauchy(0, 1), class = sd),
                      prior(cauchy(0, 1), class = sigma),
                      prior(lkj_corr_cholesky(1.5), class = cor)),
            iter = 2000, warmup = 1000, chains = 4, cores = 4,
            control = list(adapt_delta = .975, max_treedepth = 20),
            seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The model has not converged (some Rhats are &amp;gt; 1.1). Do not analyse the results! 
## We recommend running more iterations and/or setting stronger priors.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: There were 150 divergent transitions after warmup. Increasing adapt_delta above 0.975 may help.
## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + occasion + homecog + (1 + occasion + homecog | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)               0.72      0.11     0.50     0.99 1.02      456
## sd(occasion)                0.27      0.03     0.22     0.32 1.00     1010
## sd(homecog)                 0.04      0.02     0.00     0.09 1.10       39
## cor(Intercept,occasion)     0.28      0.20    -0.14     0.70 1.02      178
## cor(Intercept,homecog)     -0.07      0.39    -0.73     0.73 1.05       70
## cor(occasion,homecog)      -0.02      0.38    -0.72     0.73 1.01      247
##                         Tail_ESS
## sd(Intercept)                165
## sd(occasion)                1608
## sd(homecog)                   54
## cor(Intercept,occasion)      319
## cor(Intercept,homecog)       369
## cor(occasion,homecog)        418
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.15      0.16     1.84     2.46 1.00     2211     2188
## occasion      1.12      0.02     1.07     1.16 1.00     3363     3237
## homecog       0.06      0.02     0.03     0.10 1.00     2290     2399
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.59      0.02     0.56     0.63 1.00     1120     2369
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running this model, we get a few warnings about large Rhats and divergent transitions. We would be better off running a few more iterations (maybe 4000) if we wanted more confidence in the results. Nevertheless, we see a pretty weak effect of cognitive stimulation. The credible interval for the population effect of cognitive stimulation doesn’t include zero, but the evidence is weak that it’s doing anything at all in this model. There’s some indication that the intercepts vary by cognitive stimulation, such that at higher stimulation children have a higher initial reading ability. But there’s no evidence that the slopes vary by cognitive stimulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read5))) %&amp;gt;%
  mutate(homecog_grp = factor(ntile(homecog, 3))) %&amp;gt;%
  ggplot(aes(x = occasion, y = Estimate, group = id, color = homecog_grp)) +
  geom_line(size = .75, alpha = .50) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Cognitive Home Environment&amp;quot;) +
  facet_wrap(~homecog_grp, labeller = as_labeller(c(&amp;quot;1&amp;quot; = &amp;quot;Low&amp;quot;,
                                                    &amp;quot;2&amp;quot; = &amp;quot;Medium&amp;quot;,
                                                    &amp;quot;3&amp;quot; = &amp;quot;High&amp;quot;))) +
  guides(color = FALSE) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5, size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-6-dealing-with-missing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 6: Dealing with Missing Data&lt;/h2&gt;
&lt;p&gt;So far we’ve been dealing with complete data - where each assessment has no missing values. In doing so, we are effectively claiming that the data are missing completely at random (MCAR). This may not be a tenable assumption. Missingness on one variable may be correlated with another variable (Missing at Random; MAR) or, even worse, correlated with the variable itself (Missing Not At Random; MNAR). Without diving into the theoretical aspects of missing data (a more thoughtful discussion can be found &lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-MCAR.html&#34;&gt;here&lt;/a&gt;) let’s end by running Bayesian imputation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat_missing &amp;lt;- read_sav(&amp;quot;CurranLong.sav&amp;quot;) %&amp;gt;%
  select(id, occasion, read, homecog)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;em&gt;brms&lt;/em&gt; we indicate missingness with &lt;em&gt;mi()&lt;/em&gt;. Here we’re rerunning Model 5, but we’re also imputing missingness on reading ability and cognitive stimulation. We actually have two model formulae - one for each variable in the model with missing data. I’ll also note quickly that we have hidden missingness in this data as missing assessment points do not show up as rows in the dataset. For the purpose of this demonstration, it won’t be a problem, but in the real world, we’d want to impute these as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read6 &amp;lt;- brm(data = curran_dat_missing,
             family = gaussian,
             bf(read | mi() ~ 1 + occasion + mi(homecog) + (1 + occasion | id)) +
               bf(homecog | mi() ~ 1) + 
               set_rescor(FALSE),
             prior = c(prior(normal(0, 5), class = Intercept),
                       prior(normal(0, 1), class = b),
                       prior(lkj_corr_cholesky(1.5), class = cor)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: read | mi() ~ 1 + occasion + mi(homecog) + (1 + occasion | id) 
##          homecog | mi() ~ 1 
##    Data: curran_dat_missing (Number of observations: 1393) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##                                   Estimate Est.Error l-95% CI u-95% CI
## sd(read_Intercept)                    0.75      0.04     0.68     0.83
## sd(read_occasion)                     0.27      0.02     0.22     0.32
## cor(read_Intercept,read_occasion)     0.25      0.12     0.03     0.51
##                                   Rhat Bulk_ESS Tail_ESS
## sd(read_Intercept)                1.00     1360     2987
## sd(read_occasion)                 1.01      770     1589
## cor(read_Intercept,read_occasion) 1.00      960     1492
## 
## Population-Level Effects: 
##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## read_Intercept        2.15      0.16     1.84     2.47 1.00     2028
## homecog_Intercept     8.91      0.07     8.77     9.05 1.00     9040
## read_occasion         1.12      0.02     1.08     1.16 1.00     4599
## read_mihomecog        0.06      0.02     0.03     0.09 1.00     2066
##                   Tail_ESS
## read_Intercept        2534
## homecog_Intercept     2687
## read_occasion         2956
## read_mihomecog        2596
## 
## Family Specific Parameters: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_read        0.59      0.02     0.56     0.62 1.00      943     2256
## sigma_homecog     2.57      0.05     2.47     2.66 1.00    12854     2668
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Output is all well and good, but to really see what’s going on we’ll return to our sampling technique. Here are the fitted values on 3 individuals from Model 5 (non-imputed) and the same 3 from Model 6 (imputed).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read5))) %&amp;gt;%
  filter(id %in% c(1048, 1050, 1498)) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 5, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 5, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Unimputed Data&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.1, .1), breaks = 0:3) +
  scale_y_continuous(limits = c(2, 9)) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 12) +
  theme(plot.title = element_text(hjust = .5))

p2 &amp;lt;- curran_dat_missing %&amp;gt;%
  bind_cols(as_tibble(fitted(read6))) %&amp;gt;%
  filter(id %in% c(1048, 1050, 1498)) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 5, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate.read), shape = 1, size = 5, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Bayesian Imputation&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.1, .1), breaks = 0:3) +
  scale_y_continuous(limits = c(2, 9)) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 12) +
  theme(plot.title = element_text(hjust = .5))

library(ggpubr)

ggarrange(p1, p2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we get reasonable estimates for the missing values. Our work here is done.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-words&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Words&lt;/h1&gt;
&lt;p&gt;This topic is so vast that I couldn’t possibly cover everything there is to say about Bayesian Linear Mixed Models. We only scratched the surface on interpreting Bayesian posteriors. For example, &lt;em&gt;brms&lt;/em&gt; offers formal ways of comparing models and I highly recommend looking into these.
I’m indebted to Richard McElreath’s fantastic lectures series for all that I know on the topic. Honestly, if you can spare the time, they’re so so worth it - plus they’re freely available on YouTube &lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&amp;amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&#34;&gt;here&lt;/a&gt;. Watch these and then follow along with the code in this &lt;em&gt;brms&lt;/em&gt; guide &lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/&#34;&gt;here&lt;/a&gt; and you’re golden! Finally, if you want a short primer on Bayesian MLM and an excellent guide for writing STAN code (not &lt;em&gt;brms&lt;/em&gt;), I’d highly recommend this &lt;a href=&#34;https://www.tqmp.org/RegularArticles/vol12-3/p175/p175.pdf&#34;&gt;paper&lt;/a&gt; by Sorensen, Hohenstein, and Vasishth (2016). It helped immensely in my ongoing transition from Frequentist to Bayesian statistics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.psychstatistics.com/2014/12/27/d-lkj-priors/&#34;&gt;Baldwin, S. (2014). &lt;em&gt;Visualizing the LKJ Correlation Distribution.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jflournoy.github.io/assets/pdf/srcdmeth.pdf&#34;&gt;Curran, P. J. (1997). Comparing Three Modern Approaches to Longitudinal Data Analysis: An Examination of a Single Developmental Sample. &lt;em&gt;Symposium Conducted at the 1997 Biennial Meeting of the Society for Research in Child Development, Washington, D.C.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/&#34;&gt;Kurz, A. S. (2019). &lt;em&gt;Statistical Rethinking with brms, ggplot2, and the tidyverse.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.tqmp.org/RegularArticles/vol12-3/p175/&#34;&gt;Sorensen, T., Hohenstein, S., &amp;amp; Vasishth, S. (2016). Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists. &lt;em&gt;The Quantitative Methods for Psychology&lt;/em&gt;, 175-200. doi: 10.20982/tqmp.12.3.p175&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0732118X18300746?via%3Dihub&#34;&gt;van Zyl, C. J. J. (2018). Frequentist and Bayesian inference: A conceptual primer. &lt;em&gt;New Ideas in Psychology, 51&lt;/em&gt;, 44-49.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>Bayesian</category>
      
            <category>Linear Mixed Models</category>
      
            <category>Multilevel Modeling</category>
      
            <category>Tutorial</category>
      
      
            <category>R</category>
      
            <category>Bayesian</category>
      
            <category>Linear Mixed Models</category>
      
            <category>Multilevel Modeling</category>
      
            <category>Tutorial</category>
      
    </item>
    
  </channel>
</rss>
