<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Animation on Will Hipson</title>
    <link>/categories/animation/</link>
    <description>Recent content in Animation on Will Hipson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2020</copyright>
    <lastBuildDate>Tue, 28 Jan 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/animation/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>An Intuitive Look at Binomial Probability in a Bayesian Context</title>
      <link>/post/bayesian_intro/binomial_gold/</link>
      <pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian_intro/binomial_gold/</guid>
      <description>


&lt;p&gt;Binomial probability is the relatively simple case of estimating the proportion of &lt;em&gt;successes&lt;/em&gt; in a series of yes/no trials. The perennial example is estimating the proportion of heads in a series of coin flips where each trial is independent and has possibility of heads or tails. Because of its relative simplicity, the binomial case is a great place to start when learning about Bayesian analysis. In this post, I will provide a gentle introduction to Bayesian analysis using binomial probability as an example. Instead of coin flips, we’ll imagine a scenario where we are mining for gold!&lt;/p&gt;
&lt;div id=&#34;setting-the-scene&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting the Scene&lt;/h2&gt;
&lt;p&gt;There’s talk in the town that gold is to be found in the nearby hills! We focus on a local merchant who buys gold from prospectors to trade with neighbouring towns. The merchant needs to know &lt;em&gt;how much gold is out there&lt;/em&gt; so that they can set a competitive price for buying and selling. The merchant decides to investigate the proportion of gold in the hills by collecting data. It is impossible to survey the entire landscape, but the merchant assumes that a sample of the hills will provide a good estimate of the total proportion of gold.&lt;/p&gt;
&lt;p&gt;The merchant’s goal is to estimate the true proportion of gold in the hills. We will use &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; to represent this quantity that we are trying to estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating the Problem&lt;/h2&gt;
&lt;p&gt;For this example, we will simulate the entire landscape as a 100 x 100 grid. We will allow each of the 10,000 points on the grid to have a probability of .10 (10%) of containing gold. This is our chosen value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; for the simulation. We’ll also identify a sample 10 x 10 grid. Here’s a plot of the simulated landscape identifying the location of gold and the sample area.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Remember we are omniscient but our merchant knows nothing about where the gold is and how much there truly is. Bayesian statistics is all about dealing with uncertainty by incorporating information from new data &lt;em&gt;and&lt;/em&gt; prior sources of information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes-theorem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayes’ Theorem&lt;/h2&gt;
&lt;p&gt;I’m sure that most readers have encountered Bayes’ theorem at some point in their stats journey, although maybe the notation was different. We are most interested in computing the &lt;em&gt;posterior probability&lt;/em&gt;, which we denote as &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt;. It combines information obtained from the data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and our prior information (what we already know about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;). Expressed more formally,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=\frac{p(y|\theta)p(\theta)}{p(y)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt; is the posterior probability; &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt; is the data-generating process, usually referred to as the &lt;em&gt;likelihood&lt;/em&gt;; &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; is the prior probability of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;; and &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; is a normalizing constant. We won’t worry ourselves with the normalizing constant, &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt; as it doesn’t affect our conclusions. Thus, we’ll stick with the nonnormalized posterior likelihood:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(\theta|y)=p(y|\theta)p(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;At first, this formula can be quite confusing. When I first encountered it, I was confused by what &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; represented (and this stemmed from a fuzzy understanding of likelihood estimation I had at the time). What’s important to understand is that &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; is an unknown parameter, but in order to estimate our uncertainty about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; we are going to try out &lt;strong&gt;different values&lt;/strong&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For example, we want to know the proportion of gold in the area which ranges from 0 to 1 (0 being no gold; 1 being all gold). So in our equation, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; would represent all values between 0 and 1. However, there are infinite values between 0 and 1, so in practice we could try out values between 0 and 1. In this example, we will use a relatively simple process called &lt;em&gt;grid approximation&lt;/em&gt; where we use an equally spaced grid of values between 0 and 1 for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;binomial-likelihood&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Binomial Likelihood&lt;/h3&gt;
&lt;p&gt;Let’s tackle the first piece of Bayes’ theorem: the likelihood, &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt;. Here, we are essentially asking the question: “how likely is the data, given a particular value for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;?” Here, we introduce the binomial likelihood function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta)=\theta^y(1-\theta)^{n-y}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the number of successes and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of trials.&lt;/p&gt;
&lt;p&gt;Let’s return to our gold merchant and see how we can express the likelihood in terms of the data the merchant observes. Imagine our merchant collects data by digging each of the 10 x 10 squares in the sample. The merchant finds that 14 out of 100 spaces in the sample contain gold:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s how we express the data in terms of the binomial likelihood function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p(y|\theta)=\theta^{14}(1-\theta)^{100-14}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we compute &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt; across the grid of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values from 0 to 1, we get the following probability density:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Reproducing this in R is fairly simple - we could substitute the values into the binomial formula, or just use the built-in &lt;em&gt;dbinom&lt;/em&gt; function. We then create a dataframe containing the likelihood for each theta and use &lt;em&gt;ggplot2&lt;/em&gt; from the &lt;em&gt;tidyverse&lt;/em&gt; to draw the plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

# use seq(.01, 1, .01) to generate a sequence from .01 to 1 in .01 increments
likelihood &amp;lt;- dbinom(x = 14, size = 100, prob = seq(.01, 1, .01))

# creating a data frame with &amp;#39;likelihood&amp;#39; for plotting
df &amp;lt;- data.frame(theta = seq(.01, 1, .01),
                 likelihood)

# create the plot
ggplot(df, aes(x = theta, y = likelihood)) +
  geom_line(size = 1) +
  labs(x = TeX(&amp;quot;$\\theta$&amp;quot;),
       y = &amp;quot;Likelihood&amp;quot;,
       title = TeX(&amp;quot;$p(y|\\theta) = \\theta^{14}(1 - \\theta)^{100 - 14}$&amp;quot;),
       subtitle = &amp;quot;Dashed red line indicates maximum likelihood estimate.&amp;quot;,
       caption = &amp;quot;Using nonnormalized likelihood.&amp;quot;) +
  geom_vline(xintercept = .14, size = 1.25, color = &amp;quot;red&amp;quot;, linetype = 2) +
  scale_x_continuous(breaks = seq(0, 1, by = .10)) +
  theme_minimal(base_size = 14) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.title = element_text(hjust = .5, size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not surprisingly, the most likely value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; (the maximum likelihood estimate) is .14. In Bayesian statistics, however, we aren’t content with just the point estimate and instead we use the entire density to express uncertainty about &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;incorporating-prior-information&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Incorporating Prior Information&lt;/h3&gt;
&lt;p&gt;Imagine that our merchant hears rumours that each 100 square meters of land has a 20% probability of containing gold. We now want to incorporate this prior information into our model. &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt; is the prior probability of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and it can represent past data or even subjective knowledge about the likely value and uncertainty of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A common approach when choosing priors is to identify a &lt;em&gt;conjugate prior&lt;/em&gt;: a formula for expressing the prior that has a similar data structure to that of the likelihood. In this case, a conjugate prior for the binomial likelihood is the beta function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{beta}(a,b)=\theta^{a-1}(1-\theta)^{b-1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; can represent successes and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; can represent failures.&lt;/p&gt;
&lt;p&gt;Notice the similarity between the formulas for the binomial and beta functions. They have identical data structures, which makes the beta a conjugate prior for the binomial likelihood. Let’s use a &lt;span class=&#34;math inline&#34;&gt;\(\text{beta}(2,8)\)&lt;/span&gt; as a prior, representing our knowledge of a rumored 20% probability of gold.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly, you can reproduce this using the &lt;em&gt;dbeta&lt;/em&gt; function in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# here we use x as a sequence from .01-1
b_likelihood &amp;lt;- dbeta(x = seq(.01, 1, .01), shape1 = 2, shape2 = 8)

df &amp;lt;- data.frame(theta = seq(.01, 1, .01),
                 b_likelihood)

df %&amp;gt;%
  ggplot(aes(x = theta, y = b_likelihood)) +
  geom_line(size = 1) +
  labs(x = TeX(&amp;quot;$\\theta$&amp;quot;),
       y = NULL,
       title = TeX(&amp;quot;beta(2,8)&amp;quot;)) +
  scale_x_continuous(breaks = seq(0, 1, by = .10)) +
  theme_minimal(base_size = 14) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.title = element_text(hjust = .5, size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why use a beta prior instead of another binomial density? The reason is that the binomial density is dependent on a sample size parameter &lt;em&gt;n&lt;/em&gt;, whereas the beta density is not. It is more convenient to express our prior knowledge without &lt;em&gt;n&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-the-posterior&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Getting the Posterior&lt;/h3&gt;
&lt;p&gt;The posterior probability, &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt; takes into account the data and the prior. It is often viewed as a &lt;em&gt;compromise&lt;/em&gt; between the data likelihood and the prior probability. This is more easily understood graphically:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here’s how to produce this figure:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;theta &amp;lt;- seq(.01, 1, .01)
prior &amp;lt;- dbeta(x = theta, shape1 = 2, shape2 = 8)

#multiply the binomial likelihood by 100 to express it in &amp;#39;probability terms&amp;#39; like the beta
likelihood &amp;lt;- dbinom(x = 14, size = 100, prob = theta) * 100
posterior_density &amp;lt;- likelihood * prior

df &amp;lt;- data.frame(theta = theta,
                 likelihood = likelihood,
                 prior = prior,
                 pd = posterior_density)

# use pivot_longer to create a variable out of likelihood vs. prior vs. posterior
data_long &amp;lt;- pivot_longer(df, cols = c(likelihood, prior, pd))

data_long %&amp;gt;%
  ggplot(aes(x = theta, y = value, color = name)) +
  geom_line(size = 1.5) +
  labs(x = TeX(&amp;quot;$\\theta$&amp;quot;),
       y = NULL,
       color = NULL,
       title = &amp;quot;Posterior, Likelihood, and Prior&amp;quot;) +
  scale_color_discrete(labels = c(&amp;quot;Likelihood&amp;quot;, &amp;quot;Posterior&amp;quot;, &amp;quot;Prior&amp;quot;)) +
  scale_x_continuous(breaks = seq(0, 1, by = .10)) +
  theme_minimal(base_size = 16) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        plot.title = element_text(hjust = .5, size = 20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above chart shows the posterior, likelihood, and prior side by side. Notice how the prior (in blue) contains less certainty than the likelihood. The posterior density is simply the prior multiplied by the likelihood, thus it contains information from both sources. It’s a general rule that in instances when there is a lot of data &lt;em&gt;and&lt;/em&gt; our prior is mostly uninformative that the likelihood will overwhelm the prior.&lt;/p&gt;
&lt;p&gt;If we were to extend this further to make actual conclusions about our uncertainty of the proportion of gold in the hills, we would need to draw random samples from the posterior distribution and generate interval estimates based on these draws. For this post, however, we will stick to a graphical evaluation of &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;an-animated-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Animated Example&lt;/h2&gt;
&lt;p&gt;I’m a huge proponent of using animations to illustrate complicated topics (if you haven’t yet, you should consider watching &lt;a href=&#34;https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw&#34;&gt;3Blue1Brown’s videos&lt;/a&gt; covering mathematics concepts). So I want to end by bringing to life our example of a gold miner estimating the proportion of gold in a section of land. We’ll use the same information as before, but the difference this time is that the gold miner will investigate each 1 x 1 section of land at a time and update the likelihood.&lt;/p&gt;
&lt;p&gt;The animation begins with our merchant-miner (indicated by the red square) on square 1,1. As the merchant covers the sample, the influence of the likelihood over the prior increases and there is more certainty in the estimate. If we expanded our sample to cover more of the territory, our estimate would get even closer to the true value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and we’d be more certain about it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_intro/Bayesian_Intro_files/figure-html/unnamed-chunk-10-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;The binomial likelihood serves as a great introductory case into Bayesian statistics. It may seem like overkill to use a Bayesian approach to estimate a binomial proportion, indeed the point estimate equals the sample proportion. But remember that it’s far more important to get an estimate of uncertainty as opposed to a simple point estimate.&lt;/p&gt;
&lt;p&gt;There’s a lot we didn’t cover here; namely, making inferences from the posterior distribution, which typically involves sampling from the posterior distribution. As well, I hope to soon extend into more practical cases such as logistic regression, mixture modeling, etc, with demonstrations using Stan.&lt;/p&gt;
&lt;p&gt;If you’re eager for more examples and tutorials with Bayesian statistics, I’d recommend watching this &lt;a href=&#34;https://www.youtube.com/watch?v=3OJEae7Qb_o&#34;&gt;video series&lt;/a&gt; by Rasmus Bååth. John Kruschke’s book &lt;a href=&#34;http://doingbayesiandataanalysis.blogspot.com/&#34;&gt;Doing Bayesian Data Analysis&lt;/a&gt; is a great introductory text. If you’re up for the challenge, consider also &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/book/&#34;&gt;Bayesian Data Analysis, 3rd edition&lt;/a&gt; by Gelman et al, widely considered to be the &lt;em&gt;Bayesian Bible&lt;/em&gt;. Much of the content for this blog post was inspired from the first few chapters of that book.&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>Bayesian</category>
      
            <category>Binomial</category>
      
            <category>Probability</category>
      
            <category>Animation</category>
      
      
            <category>R</category>
      
            <category>Bayesian</category>
      
            <category>Binomial</category>
      
            <category>Probability</category>
      
            <category>Animation</category>
      
    </item>
    
  </channel>
</rss>
