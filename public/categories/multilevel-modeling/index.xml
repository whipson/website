<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Multilevel Modeling on Will Hipson</title>
    <link>/categories/multilevel-modeling/</link>
    <description>Recent content in Multilevel Modeling on Will Hipson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2020</copyright>
    <lastBuildDate>Tue, 09 Jun 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/multilevel-modeling/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Bayesian Varying Effects Models in R and Stan</title>
      <link>/post/stan-random-slopes/varying_effects_stan/</link>
      <pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/post/stan-random-slopes/varying_effects_stan/</guid>
      <description>


&lt;p&gt;In psychology, we increasingly encounter data that is nested. It is to the point now where any quantitative psychologist worth their salt must know how to analyze multilevel data. A common approach to multilevel modeling is the &lt;strong&gt;varying effects&lt;/strong&gt; approach, where the relation between a predictor and an outcome variable is modeled both within clusters of data (e.g., observations within people, or children within schools) and across the sample as a whole. And there is no better way to analyze this kind of data than with Bayesian statistics. Not only does Bayesian statistics give solutions that are directly interpretable in the language of probability, but Bayesian models can be infinitely more complex than Frequentist ones. This is crucial when dealing with multilevel models, which get complex quickly.&lt;/p&gt;
&lt;p&gt;A preview of what’s to come:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; is the &lt;em&gt;lingua franca&lt;/em&gt; for programming Bayesian models. You code your model using the Stan language and then run the model using a data science language like R or Python. Stan is extremely powerful, but it is also intimidating even for an experienced programmer. In this post, I’ll demonstrate how to code, run, and evaluate multilevel models in Stan.&lt;/p&gt;
&lt;p&gt;I assume a basic grasp of Bayesian logic (i.e., you understand priors, likelihoods, and posteriors). I’ve written some content introducing these terms &lt;a href=&#34;https://willhipson.netlify.app/post/bayesian_intro/binomial_gold/&#34;&gt;here&lt;/a&gt; if you want to check that out first. I also assume familiarity with R and the &lt;em&gt;tidyverse&lt;/em&gt; packages (in particular, ggplot2, dplyr, and purrr).&lt;/p&gt;
&lt;p&gt;You’ll need to install the &lt;em&gt;rstan&lt;/em&gt; package to follow along. This installation is more involved than typical R packages. The instructions on &lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34;&gt;Stan’s website&lt;/a&gt; will help you get started.&lt;/p&gt;
&lt;p&gt;One more note before we dive in. If you’re just starting out with Bayesian statistics in R and you have some familiarity with running Frequentist models using packages like &lt;em&gt;lme4&lt;/em&gt;, &lt;em&gt;nlme&lt;/em&gt;, or the base &lt;em&gt;lm&lt;/em&gt; function, you may prefer starting out with more user-friendly packages that use Stan as a backend, but hide a lot of the complicated details. Some time back I wrote up a &lt;a href=&#34;https://willhipson.netlify.app/post/bayesian_mlm/bayesian_mlm/&#34;&gt;demonstration&lt;/a&gt; using the &lt;em&gt;brms&lt;/em&gt; package, which allows you to run Bayesian mixed models (and more) using familiar model syntax. I started with brms and am gradually building up competency in Stan. Stan is the way to go if you want more control and a deeper understanding of your models, but maybe brms is a better place to start. Regardless, I’ll try to give intuitive explanations behind the Stan code so that you can hopefully start writing it yourself someday. Don’t be discouraged if it doesn’t sink in right away.&lt;/p&gt;
&lt;div id=&#34;simulating-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulating the Data&lt;/h1&gt;
&lt;p&gt;We are creating &lt;strong&gt;repeated measures&lt;/strong&gt; data where we have several days of observations for a group of participants (each denoted by &lt;code class=&#34;R&#34;&gt;pid&lt;/code&gt;). Our outcome variable is &lt;code class=&#34;R&#34;&gt;y&lt;/code&gt; and our continuous predictor is &lt;code class=&#34;R&#34;&gt;x&lt;/code&gt;. We’ll imagine that the days of observation are random so we won’t need to model it as a grouping variable.&lt;/p&gt;
&lt;p&gt;Here’s the code to simulate the data we’ll use in this post. For now, it’s just enough to copy the code and run it so that you’ll have the data. This code will make more sense once we start working with the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) # for ggplot2, dplyr, purrr, etc.
theme_set(theme_bw(base_size = 14)) # setting ggplot2 theme

library(mvtnorm)

set.seed(12407) # for reproducibility

# unstandardized means and sds
real_mean_y &amp;lt;- 4.25
real_mean_x &amp;lt;- 3.25
real_sd_y &amp;lt;- 0.45
real_sd_x &amp;lt;- 0.54

N &amp;lt;- 30 # number of people
n_days &amp;lt;- 7 # number of days
total_obs &amp;lt;- N * n_days

sigma &amp;lt;- 1 # population sd
beta &amp;lt;- c(0, 0.15) # average intercept and slope
sigma_p &amp;lt;- c(1, 1) # intercept and slope sds
rho &amp;lt;- -0.36 # covariance between intercepts and slopes

cov_mat &amp;lt;- matrix(c(sigma_p[1]^2, sigma_p[1] * sigma_p[2] * rho, sigma_p[1] * sigma_p[2] * rho, sigma_p[2]^2), nrow = 2)
beta_p &amp;lt;- rmvnorm(N, mean = beta, sigma = cov_mat) # participant intercepts and slopes

x &amp;lt;- matrix(c(rep(1, N * n_days), rnorm(N * n_days, 0, 1)), ncol = 2) # model matrix
pid &amp;lt;- rep(1:N, each = n_days) # participant id

sim_dat &amp;lt;- map_dfr(.x = 1:(N * n_days), ~data.frame(
  mu = x[.x, 1] * beta_p[pid[.x], 1] + x[.x, 2] * beta_p[pid[.x], 2],
  pid = pid[.x],
  x = x[.x, 2]
))

sim_dat$y &amp;lt;- rnorm(210, sim_dat$mu, sigma) # creating observed y from mu and sigma

dat &amp;lt;- sim_dat %&amp;gt;%
  select(-mu) %&amp;gt;% # removing mu
  mutate(y = real_mean_y + (y * real_sd_y), # unstandardize
         x = real_mean_x + (x * real_sd_x)) # unstandardize&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a data frame that is very much like something you would come across in psychology research. Let’s perform a quick check on the data to see that the simulation did what we wanted it to.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;%
  ggplot(aes(x = x, y = y, group = pid)) +
  geom_point(color = &amp;quot;cadetblue4&amp;quot;, alpha = 0.80) +
  geom_smooth(method = &amp;#39;lm&amp;#39;, se = FALSE, color = &amp;quot;black&amp;quot;) +
  facet_wrap(~pid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see a good amount of clustering at the participant level. Ok, so we have our data, now let’s analyze it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-workflow&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bayesian Workflow&lt;/h1&gt;
&lt;p&gt;A good Bayesian analysis includes the following steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Prior predictive checks.&lt;/li&gt;
&lt;li&gt;Model execution using Markov Chain Monte Carlo.&lt;/li&gt;
&lt;li&gt;Posterior predictive checks.&lt;/li&gt;
&lt;li&gt;Model comparison&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We’ll focus in this post of the first three, saving model comparison for another day. For the models in this post, I’ll give examples of each of the three steps. Steps 1 and 3 strongly leverage plotting techniques, so we’ll make good use of ggplot2.&lt;/p&gt;
&lt;div id=&#34;example-with-simple-linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example with Simple Linear Regression&lt;/h2&gt;
&lt;p&gt;It’s been said that linear regression is the ‘Hello World’ of statistics. To see the Bayesian workflow in action and get comfortable, we’ll start with a simple (albeit inappropriate) model for this data - one in which we completely ignore the grouping of the data within participants and instead treat each observation as completely independent from the others. This is &lt;strong&gt;not&lt;/strong&gt; the way to analyze this data, but I use it as a simple demonstration of how to construct Stan code.&lt;/p&gt;
&lt;p&gt;In mathematical notation, here is our simple linear regression model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  y_i \sim \text{Normal}(\mu, \sigma) \\
  \mu_i= \beta_0 + \beta_1 x_i \\
  \beta_0 \sim \text{Normal}(0, 1) \\
  \beta_1 \sim \text{Normal}(0, 1) \\
  \sigma \sim \text{Exponential}(1)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I give full credit to McElreath’s brilliant &lt;em&gt;Statistical Rethinking&lt;/em&gt; (2020) for introducing me to this way of writing out models. It’s a bit jarring at first; myself having become accustomed to model formulas as one-liners. But once you understand it it’s a really elegant way of expressing the model. We start by expressing how the outcome variable, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, is distributed: it has a &lt;strong&gt;deterministic&lt;/strong&gt; part, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (the mean), and an error, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; (the standard deviation). &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is the expected value for each observation, and if you have encountered regressions before, you know that the expected value for a given observation is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; + &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; times the predictor &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We then assign priors to the parameters. Priors encode our knowledge and uncertainty about the data &lt;em&gt;before&lt;/em&gt; we run the model. We don’t use priors to get a desired result, we use priors because it makes sense. Without priors, our model initially ‘thinks’ that the data is just as likely to come from a normal distribution with a mean of 0 and sigma of 1 as it is to come from a distribution with a mean of 1,000 and a sigma of 400. True, the likelihood function (i.e., the probability of the data given the model) will sort things out when we have sufficient data, but we’ll see that priors play a particularly important role when we move on to varying effects models.&lt;/p&gt;
&lt;p&gt;The model formula above includes priors for &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. But how do we know these are reasonable priors? (True, these are conventional priors and pose little threat, but let’s assume we don’t know that.). We’re going to perform some visual checks on our priors to ensure they are sensible.&lt;/p&gt;
&lt;div id=&#34;prior-predictive-checks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prior Predictive Checks&lt;/h3&gt;
&lt;p&gt;First, let’s see what different prior distributions look like. We’ll use conventional normal priors on &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;. We’ll try out three different normal distributions.&lt;/p&gt;
&lt;p&gt;We create a grid of evenly spaced values. We then want to see the likelihood of these &lt;code class=&#34;R&#34;&gt;x&lt;/code&gt;s in the context of three normal distributions with the same mean but different standard deviations. We use the &lt;code class=&#34;R&#34;&gt;map_dfr&lt;/code&gt; function from the purrr package to iterate over the three different distributions and store the result in a data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid &amp;lt;- seq(-3, 3, length.out = 1000) # evenly spaced values from -3 to 3

b0_prior &amp;lt;- map_dfr(.x = c(0.5, 1, 2), ~ data.frame( # .x represents the three sigmas
  grid = grid,
  b0 = dnorm(grid, mean = 0, sd = .x)),
  .id = &amp;quot;sigma_id&amp;quot;)

# create friendlier labels
b0_prior &amp;lt;- b0_prior %&amp;gt;%
  mutate(sigma_id = factor(sigma_id, labels = c(&amp;quot;normal(0, 0.5)&amp;quot;,
                                                &amp;quot;normal(0, 1)&amp;quot;,
                                                &amp;quot;normal(0, 2)&amp;quot;)))

library(latex2exp)

ggplot(b0_prior, aes(x = grid, y = b0)) +
  geom_area(fill = &amp;quot;cadetblue4&amp;quot;, color = &amp;quot;black&amp;quot;, alpha = 0.90) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 0.85)) +
  labs(x = NULL,
       y = &amp;quot;probability density&amp;quot;,
       title = TeX(&amp;quot;Possible $\\beta_0$ (intercept) priors&amp;quot;)) +
  facet_wrap(~sigma_id, nrow = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the three distributions vary in terms of how spread out they are. We might say that the top distribution with mean 0 and sd 0.5 is more “confident” about the probability of values close to zero, or that it is more skeptical about extreme values. Flatter distributions allocate probability more evenly and are therefore more open to extreme values.&lt;/p&gt;
&lt;p&gt;This is all well and good, but looking at the raw probability densities doesn’t tell us much about what the priors assume about the data. We need to look at what kind of data is &lt;em&gt;compatible&lt;/em&gt; with the priors. So this time we’ll generate regression lines from the &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; priors. Remember, all you need to create a line is an intercept and a slope!&lt;/p&gt;
&lt;p&gt;The trick this time is to generate intercepts and slopes from the different normal distributions. Again, we’ll stick with three different distributions, but now we use &lt;code class=&#34;R&#34;&gt;map2_dfr&lt;/code&gt; to iterate over a &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; prior and a &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; prior.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b0b1 &amp;lt;- map2_df(.x = c(0.5, 1, 2), .y = c(0.25, 0.5, 1), ~ data.frame(
  b0 = rnorm(100, mean = 0, sd = .x), 
  b1 = rnorm(100, mean = 0, sd = .y)), .id = &amp;quot;sigma_id&amp;quot;
)

# create friendlier labels
b0b1 &amp;lt;- b0b1 %&amp;gt;%
  mutate(sigma_id = factor(sigma_id, labels = c(&amp;quot;b0 ~ normal(0, 0.5); b1 ~ normal(0, 0.25)&amp;quot;,
                                                &amp;quot;b0 ~ normal(0, 1); b1 ~ normal(0, 0.50)&amp;quot;,
                                                &amp;quot;b0 ~ normal(0, 2); b1 ~ normal(0, 1)&amp;quot;)))

ggplot(b0b1) +
  geom_abline(aes(intercept = b0, slope = b1), color = &amp;quot;cadetblue4&amp;quot;, alpha = 0.75) +
  scale_x_continuous(limits = c(-2, 2)) +
  scale_y_continuous(limits = c(-3, 3)) +
  labs(x = &amp;quot;x&amp;quot;,
       y = &amp;quot;y&amp;quot;,
       title = TeX(&amp;quot;Sampling of lines from $\\beta_0$ and $\\beta_1$ priors&amp;quot;)) +
  facet_wrap(~sigma_id, nrow = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The more restrictive set of priors on the top constrains the lines to have intercepts and slopes close to zero, and you can see that the lines vary little from one another. In contrast, the weaker priors allow a much greater variety of intercept/slope combinations.&lt;/p&gt;
&lt;p&gt;Later we’ll see how to generate actual data from the priors, but for now let’s go ahead with the priors I’ve provided in the model formula and begin coding the model in Stan!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;stan-basics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stan Basics&lt;/h2&gt;
&lt;p&gt;Every Stan model code must contain the following three blocks:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Data - where you define the data and the dimensions of the data.&lt;/li&gt;
&lt;li&gt;Parameters - where you describe the unknown parameters that you want to estimate.&lt;/li&gt;
&lt;li&gt;Model - where you list the priors and define the likelihood function for the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For an R user, the best way to write Stan code is to use RStudio’s built-in Stan file editor. If you have Stan installed, when you select the drop down options for a new file in RStudio, you can select ‘Stan file’. This lets you take advantage of autocompletion and syntax error detection.&lt;/p&gt;
&lt;p&gt;In the examples to follow I’ll make it clear which code snippets are in Stan code with a &lt;code class=&#34;stan&#34;&gt;// STAN CODE&lt;/code&gt; marker at the beginning of the block (&lt;code class=&#34;stan&#34;&gt;//&lt;/code&gt; denotes a comment and is not evaluated). For each model, I’ll explain each code block separately and then give the model code as a whole. Note that unlike in R, you cannot run Stan code lines one by one and see their output - a Stan file is only evaluated (compiled) when you execute the &lt;code class=&#34;R&#34;&gt;rstan&lt;/code&gt; function in R, which we’ll see later.&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;The data block is where we define our observed variables. We also need to define lengths/dimensions of stuff, which can seem strange if you’re used to R or Python. We first declare an integer variable &lt;code class=&#34;stan&#34;&gt;N&lt;/code&gt; to be the number of observations: &lt;code class=&#34;stan&#34;&gt;int N;&lt;/code&gt; (note the use of semicolon to denote the end of a line). I’m also declaring an integer &lt;code class=&#34;stan&#34;&gt;K&lt;/code&gt;, which is the number of predictors in our model. Note that I’m including the intercept in this count, so we end up with 2 predictors (2 columns in the model matrix).&lt;/p&gt;
&lt;p&gt;Now we supply actual data. We write &lt;code class=&#34;stan&#34;&gt;matrix[N, K]&lt;/code&gt; to tell Stan that &lt;code class=&#34;stan&#34;&gt;x&lt;/code&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(N \times K\)&lt;/span&gt; matrix. &lt;code class=&#34;stan&#34;&gt;y&lt;/code&gt; is easier - just a vector of length &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
data {
  int N; // number of observations
  int K; // number of predictors + intercept
  matrix[N, K] x; // x vector
  vector[N] y; // y vector
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parameters&lt;/h3&gt;
&lt;p&gt;Now we tell Stan the parameters in our model. These are the unobserved variables that we want to estimate. In this example, we have 3 parameters: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Like before, we first tell Stan the type of data this parameter will contain - in this case, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; are contained in a vector of length &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; that we will sensibly call &lt;code class=&#34;stan&#34;&gt;beta&lt;/code&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; will just be a single &lt;code class=&#34;stan&#34;&gt;real&lt;/code&gt; value (“real” means that the number can have a decimal point). We use &lt;code class=&#34;stan&#34;&gt;&amp;lt;lower=0&amp;gt;&lt;/code&gt; to constrain it to be positive because it is impossible to have negative standard deviation.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
parameters {
  vector[K] beta; // intercept and slope parameters
  real&amp;lt;lower=0&amp;gt; sigma; // constrained to be positive
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model&lt;/h3&gt;
&lt;p&gt;The model block is where the action is at. We start with our priors. There should be a prior for each parameter declared above.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
model {
  vector[N] mu; // declaring a mu vector
  
  // priors
  beta ~ normal(0, 1); // declares the same prior for intercept and slope
  sigma ~ exponential(1); // using an exponential prior on sigma
  
  // likelihood
  for(i in 1:N) {
    mu[i] = x[i] * beta; // * is matrix multiplication in this context, where x[i] is the first row of x
  }
  
  y ~ normal(mu, sigma);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The part above where we define &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; (&lt;code class=&#34;stan&#34;&gt;mu[i]&lt;/code&gt;) can seem a bit strange. You may be wondering how the intercept and slope factor into this calculation. The compact notation hides some of the magic, which really isn’t very magical since it’s just a dot product. In matrix form, it looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mu_i=\begin{bmatrix}
1&amp;amp;x_i\\
\end{bmatrix}
\begin{bmatrix}
\beta_0\\
\beta_1
\end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; is the intercept and &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; value of the variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Computing this dot product for each participant &lt;code class=&#34;stan&#34;&gt;i&lt;/code&gt; gives you an expected value &lt;code class=&#34;stan&#34;&gt;mu&lt;/code&gt; for each participant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;full-stan-code&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Full Stan Code&lt;/h3&gt;
&lt;p&gt;The full Stan code looks like this:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
data {
  int N; // number of observations
  int K; // number of predictors + intercept
  matrix[N, K] x; // matrix of predictors
  vector[N] y; // y vector
}

parameters {
  vector[K] beta; // intercept and slope parameters
  real&amp;lt;lower=0&amp;gt; sigma; // constrained to be positive
}

model {
  vector[N] mu; // declaring a mu vector
  
  // priors
  beta ~ normal(0, 1); // declares the same prior for intercept and slope
  sigma ~ exponential(1); // using an exponential prior on sigma
  
  // likelihood
  for(i in 1:N) {
    mu[i] = x[i] * beta; // * is matrix multiplication in this context
  }
  
  y ~ normal(mu, sigma);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Saving this file will give it a .stan file extension. It’s a good idea to save it in your project directory. I’ll name this model &lt;code class=&#34;R&#34;&gt;&#34;mod1.stan&#34;&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Running the Model&lt;/h3&gt;
&lt;p&gt;Now we go back to R to run the model. If you haven’t already, load up &lt;code class=&#34;R&#34;&gt;rstan&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to put the data in a &lt;strong&gt;list&lt;/strong&gt; for Stan. Everything that we declared in the data block of our Stan code should be entered into this list. We’ll also standardize the variables, so that they match up with our priors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_dat &amp;lt;- list(
  N = nrow(dat), # using nrow() to get the number of rows (observations)
  K = 2, # (1) intercept, (2) slope
  x = matrix(c(rep(1, nrow(dat)), scale(dat$x)), ncol = 2), # model matrix (x standardized)
  y = as.vector(scale(dat$y)) # standardized outcome. convert to vector to avoid issues with scale function
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we use the function &lt;code class=&#34;R&#34;&gt;stan&lt;/code&gt; to run the model. We set &lt;code class=&#34;R&#34;&gt;chains&lt;/code&gt; and &lt;code class=&#34;R&#34;&gt;cores&lt;/code&gt; to 4, which will allow us to run 4 Markov chains in parallel.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_fit1 &amp;lt;- stan(file = &amp;quot;mod1.stan&amp;quot;,
                  data = stan_dat,
                  chains = 4, cores = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluating-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluating the Model&lt;/h3&gt;
&lt;p&gt;We can take a look at the parameters in the console by printing the model fit. We get posterior means, standard errors, and quantiles for each parameter. We also get things called &lt;code class=&#34;R&#34;&gt;n_eff&lt;/code&gt; and &lt;code class=&#34;R&#34;&gt;Rhat&lt;/code&gt;. These are indicators of how well Stan’s engine explored the parameter space (if this is cryptic, that’s ok), It’s enough for now to know that when &lt;code&gt;Rhat&lt;/code&gt; is 1, things are good.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_fit1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: mod1.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##            mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat
## beta[1]    0.00    0.00 0.07   -0.13   -0.05    0.00    0.05    0.15  3541    1
## beta[2]    0.09    0.00 0.07   -0.05    0.04    0.09    0.14    0.22  3487    1
## sigma      1.00    0.00 0.05    0.91    0.97    1.00    1.03    1.11  3837    1
## lp__    -106.18    0.03 1.28 -109.33 -106.75 -105.85 -105.26 -104.76  1998    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jun 09 10:07:26 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But really, the best way to interpret the model is to &lt;em&gt;see&lt;/em&gt; it. There are many ways to plot the samples produced in the model. One of the simplest ways is to use the &lt;code class=&#34;R&#34;&gt;bayesplot&lt;/code&gt; library.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)

color_scheme_set(&amp;quot;teal&amp;quot;)

mcmc_areas(stan_fit1, pars = c(&amp;quot;beta[1]&amp;quot;, &amp;quot;beta[2]&amp;quot;, &amp;quot;sigma&amp;quot;), prob = 0.89) +
  scale_y_discrete(expand = c(0, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For regression problems, it’s also good to see the regression lines in the posterior distribution. This tells us what kind of lines are compatible with the data and the priors. We use rstan’s &lt;code class=&#34;R&#34;&gt;extract&lt;/code&gt; function to get samples from the posterior.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;samples &amp;lt;- as.data.frame(rstan::extract(stan_fit1, pars = c(&amp;quot;beta[1]&amp;quot;, &amp;quot;beta[2]&amp;quot;))) %&amp;gt;%
  rename(intercept = beta.1., slope = beta.2.) # friendlier variable names

ggplot(samples) +
  geom_abline(aes(intercept = intercept, slope = slope), color = &amp;quot;cadetblue4&amp;quot;, alpha = 0.20) +
  geom_abline(aes(intercept = mean(intercept), slope = mean(slope)), color = &amp;quot;black&amp;quot;, size = 2) +
  scale_x_continuous(limits = c(-1, 1)) +
  scale_y_continuous(limits = c(-0.45, 0.45)) +
  labs(x = &amp;quot;x&amp;quot;,
       y = &amp;quot;y&amp;quot;,
       title = &amp;quot;Samples from beta posteriors&amp;quot;,
       subtitle = TeX(&amp;quot;$N_{samples} = 4000$&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Although the average posterior regression line has positive slope, it’s clear that many lines, even some with negative slope, are compatible.&lt;/p&gt;
&lt;p&gt;It doesn’t matter anyway because we know this model is bad - it ignores the clustering. Dealing with the clustering will be the objective for the remainder of this post.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;varying-intercepts-and-slopes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Varying Intercepts and Slopes&lt;/h1&gt;
&lt;p&gt;Now we’re going to repent for our previous sins and acknowledge that there is shared information among observations of the same participant. What exactly is meant by “shared information”? It means that observations should vary systematically within people as well as across people. The model above makes biased inferences about the effect of x on y because it pools (averages) information greedily.&lt;/p&gt;
&lt;p&gt;So our alternative is to assign each participant their own intercept and slope. But we don’t just want to treat participants as completely independent factors - there should be some common variability across all participants in the sample. So we need to walk a fine balance between pooling all the information and considering each participant as independent. This is where we use &lt;strong&gt;regularization&lt;/strong&gt;. Regularization essentially keeps model parameters from getting too large or too certain. In the case of multilevel models, regularization uses some information from the population-level parameters (e.g., the grand mean) to estimate the cluster-specific parameters (e.g., individual participant intercepts and slopes).&lt;/p&gt;
&lt;p&gt;It may help to see the model formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
y_i \sim \text{Normal}(\mu, \sigma) \\
\mu_i = \beta_{0,\text{pid}} + \beta_{1, \text{pid}}x_i\\
\begin{bmatrix}
\beta_{0,\text{pid}}\\
\beta_{1, \text{pid}}
\end{bmatrix}
\sim \text{MVNormal}\bigg(
\begin{bmatrix}
\beta_0\\
\beta_1
\end{bmatrix}
, \Sigma
\bigg)\\
\Sigma = 
\left(\begin{array}{cc}
\sigma_{\beta_0}&amp;amp;0\\
0&amp;amp;\sigma_{\beta_1}
\end{array}\right)\Omega
\left(\begin{array}{cc}
\sigma_{\beta_0}&amp;amp;0\\
0&amp;amp;\sigma_{\beta_1}
\end{array}\right)
\\
\beta_0 \sim \text{Normal}(0, 1)\\
\beta_1 \sim \text{Normal}(0, 1)\\
\sigma_{\beta_0} \sim \text{Exponential}(1)\\
\sigma_{\beta_1} \sim \text{Exponential}(1)\\
\sigma \sim \text{Exponential}(1)\\
\Omega \sim \text{LKJcorr(2)}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is nightmare inducing. I’ll go through this slowly, but just know that it really takes time for this stuff to sink it. Don’t expect it to make perfect sense the first (or tenth) time.&lt;/p&gt;
&lt;p&gt;The definition of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is thankfully familiar-ish. We now allow each participant (again, denoted by &lt;span class=&#34;math inline&#34;&gt;\(\text{pid}\)&lt;/span&gt;) to have their own intercept and slope. But think for a moment… these intercepts and slopes don’t just come from nowhere. No, they come from a &lt;strong&gt;common distribution&lt;/strong&gt; of intercepts and slopes. In other words, they come from a &lt;strong&gt;multivariate normal distribution&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;Aha! So that’s this unpleasant &lt;span class=&#34;math inline&#34;&gt;\(\text{MVNormal}\)&lt;/span&gt; thing. A multivariate normal distribution takes a &lt;em&gt;vector&lt;/em&gt; of mean parameters and a &lt;em&gt;covariance matrix&lt;/em&gt; of standard deviations. (Contrast this with the standard normal distribution which takes a single mean parameter and a single SD).&lt;/p&gt;
&lt;p&gt;So then what’s this bizarre definition of &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;? We want &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; to be a covariance matrix, but how do we assign a prior to a covariance matrix, which has arbitrary scale and location? The trick is to assign the prior to a &lt;em&gt;correlation matrix&lt;/em&gt; instead. This may make intuitive sense to you if you think of a correlation matrix as a standardized version of a covariance matrix. The prior for a correlation matrix is called an LKJ prior (you can see it at the bottom there, &lt;span class=&#34;math inline&#34;&gt;\(\text{LKJcorr}\)&lt;/span&gt;). Values closer to 1 are less skeptical of strong correlations (-1, +1), whereas higher values (e.g., 2, 4) are more skeptical of strong correlation coefficients.&lt;/p&gt;
&lt;p&gt;Ok, so what’s this business with &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; sandwiched between these other matrices? These are copies of the same diagonal matrix, containing variances of the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters on the diagonal. When you perform this weird-looking matrix multiplication you get a covariance matrix.&lt;/p&gt;
&lt;p&gt;What remain are the &lt;strong&gt;hyper-priors&lt;/strong&gt;. This is where the adaptive regularization that I mentioned earlier happens. Take, for example, the prior &lt;span class=&#34;math inline&#34;&gt;\(\beta_0 \sim \text{Normal}(0, 1)\)&lt;/span&gt;. This is the population-level intercept. But note (and this is super important!) that &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is a multinormal prior for &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0, \text{pid}}\)&lt;/span&gt;. So you can think of it as saying that the individual &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0, \text{pid}}\)&lt;/span&gt;s come from a common distribution defined by the hyper-prior &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; (and its covariance with &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;). By assigning a prior to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; we make the model somewhat skeptical of individual intercepts that vary strongly from the average.&lt;/p&gt;
&lt;p&gt;You can see similar patterns in the remaining hyper-priors. Now let’s move on to coding the model.&lt;/p&gt;
&lt;div id=&#34;prior-predictive-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prior Predictive Simulation&lt;/h3&gt;
&lt;p&gt;Like we did with the simple linear regression, we should check that my choice of priors is reasonable. I chose conventional priors so we could probably get away without checking them. But it’s good to get into the practice of checking your priors, because there may be times that you have theoretical reasons to choose unconventional priors. Also, prior predictive checking makes it seem like you know what you’re doing and am I’m all for that.&lt;/p&gt;
&lt;p&gt;Things unfortunately get more complicated when we simulate data from a varying slopes model. Remember that we’re running the Bayesian inference process &lt;em&gt;in reverse&lt;/em&gt;, so we start by simulating hyper-priors and then use those hyper-priors to generate &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. If you want to run this code yourself, you’ll need to install and load the &lt;code class=&#34;R&#34;&gt;rethinking&lt;/code&gt; package to use &lt;code class=&#34;R&#34;&gt;rlkjcorr&lt;/code&gt;. As of writing, it’s not on CRAN, so you’ll need to install it from GitHub (just uncomment the first couple of lines).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#library(devtools)
#install_github(&amp;quot;rmcelreath/rethinking&amp;quot;)

beta &amp;lt;- rnorm(2, mean = 0, sd = 1) # b0 and b1
sigma &amp;lt;- rexp(1, 1) # population error
sigma_p &amp;lt;- rexp(2, 1) # sigma0 and sigma1
Omega &amp;lt;- rethinking::rlkjcorr(n = 1, K = 2, eta = 4) # from McElreath&amp;#39;s rethinking package

Sigma &amp;lt;- diag(sigma_p) %*% Omega %*% diag(sigma_p)
beta_p &amp;lt;- rmvnorm(N, mean = beta, sigma = Sigma)

x &amp;lt;- matrix(c(rep(1, N * n_days), rnorm(N * n_days, mean = 0, sd = 1)), ncol = 2)
pid &amp;lt;- rep(1:30, each = 7)

sim_dat &amp;lt;- map_dfr(.x = 1:(N * n_days), ~data.frame(
  mu = x[.x, 1] * beta_p[pid[.x], 1] + x[.x, 2] * beta_p[pid[.x], 2],
  pid = pid[.x],
  x = x[.x, 2]
))

sim_dat$y &amp;lt;- rnorm(210, sim_dat$mu, sigma) # creating observed y from mu and sigma

sim_dat &amp;lt;- sim_dat %&amp;gt;%
  mutate(y = real_mean_y + (y * real_sd_y), # unstandardize
         x = real_mean_x + (x * real_sd_x)) # unstandardize&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After running this code, you will have an artificial dataset that was generated from one realization of the priors. We want to ensure that our choice of priors is reasonable - that they don’t result in wildly unexpected datasets. So we’ll visualize the scatter of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gghighlight)

sim_dat %&amp;gt;%
  ggplot(aes(x = x, y = y)) +
  geom_point(size = 3, color = &amp;quot;cadetblue4&amp;quot;) +
  gghighlight(pid == 4, use_group_by = FALSE, max_highlight = Inf, use_direct_label = FALSE) +
  labs(title = &amp;quot;Data simulated from priors&amp;quot;,
       subtitle = &amp;quot;Green points are from a random participant.\nGrey points are from all other participants.&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that I’ve highlighted one participant. We should see some degree of clustering of the highlighted points, but not always. Rerun the simulation and visualization several times to see how the priors can produce different datasets - some of which seem slightly implausible. If we saw anything truly bizarre, it would be cause to change our priors before analyzing the data for real.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coding-the-model-in-stan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coding the Model in Stan&lt;/h2&gt;
&lt;p&gt;We return to Stan to code the model. Now is a good time to revisit the model formula because those characters will make an appearance here. The Stan code can be highly discomforting - I know, I’ve been there (and still am to some degree). But I’ll walk through it slowly.&lt;/p&gt;
&lt;div id=&#34;data-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;p&gt;The data block shouldn’t look too daunting compared to before. What’s new is that we have a number of observations &lt;code class=&#34;stan&#34;&gt;int N_obs&lt;/code&gt; and a number of participants &lt;code class=&#34;stan&#34;&gt;int N_pts&lt;/code&gt;. We also have a vector of participant ids &lt;code class=&#34;stan&#34;&gt;int pid[N_obs]&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
data {
  int N_obs; // number of observations
  int N_pts; // number of participants
  int K; // number of predictors + intercept
  int pid[N_obs]; // participant id vector
  matrix[N_obs, K] x; // matrix of predictors
  real y[N_obs]; // y vector
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At first it may seem weird that &lt;code class=&#34;stan&#34;&gt;pid&lt;/code&gt; has a length of &lt;code class=&#34;stan&#34;&gt;N_obs&lt;/code&gt;; you may wonder why it’s not the length of &lt;code class=&#34;stan&#34;&gt;N_pts&lt;/code&gt;. This is because this vector will have the participant identifier &lt;em&gt;for each&lt;/em&gt; participant in the dataset. (Look at the column &lt;code class=&#34;R&#34;&gt;pid&lt;/code&gt; in actual data to see what I mean).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameters-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parameters&lt;/h3&gt;
&lt;p&gt;Plenty different here. We start with &lt;code class=&#34;stan&#34;&gt;vector[K] beta_p[N_pts]&lt;/code&gt;, which describes a vector of vectors. Specifically, we’re declaring an intercept and slope for each participant (&lt;code class=&#34;stan&#34;&gt;N_pts&lt;/code&gt;). We then have &lt;code class=&#34;stan&#34;&gt;vector&amp;lt;lower=0&amp;gt;[K] sigma_p&lt;/code&gt; which describes the SD for the participant intercepts and slopes. Now for the hyper-priors, &lt;code class=&#34;stan&#34;&gt;vector[K] beta&lt;/code&gt; will hold the means for our intercept and slope hyper-priors and &lt;code&gt;corr_matrix[K] Omega&lt;/code&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(2 \times 2\)&lt;/span&gt; correlation matrix that will be in the multivariate normal prior. Finally, &lt;code&gt;real&amp;lt;lower=0&amp;gt; sigma&lt;/code&gt; is the population error that will be in the likelihood of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
parameters {
  vector[K] beta_p[N_pts]; // ind participant intercept and slope coefficients by group
  vector&amp;lt;lower=0&amp;gt;[K] sigma_p; // sd for intercept and slope
  vector[K] beta; // intercept and slope hyper-priors
  corr_matrix[K] Omega; // correlation matrix
  real&amp;lt;lower=0&amp;gt; sigma; // population sigma
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model&lt;/h3&gt;
&lt;p&gt;There are two novelties in this model compared to the simple regression. First, assigning a multivariate normal prior to &lt;code class=&#34;stan&#34;&gt;beta_p&lt;/code&gt;, our participant intercepts and slopes, requires some unfamiliar notation. We use the &lt;code class=&#34;stan&#34;&gt;multi_normal&lt;/code&gt; command. The first argument is &lt;code class=&#34;stan&#34;&gt;beta&lt;/code&gt; because beta is our hyper-prior vector for the mean intercept and slope. What’s this &lt;code class=&#34;stan&#34;&gt;quad_form_diag&lt;/code&gt; thing? It is a compact and efficient way of expressing &lt;code&gt;diag(sigma_p) * Omega * diag(sigma_p)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The likelihood looks more or less the same as before. The main difference is that we multiply &lt;code class=&#34;stan&#34;&gt;x&lt;/code&gt; by the &lt;code class=&#34;stan&#34;&gt;beta_p&lt;/code&gt; parameter. The bracket indexing can be a bit confusing. The expression &lt;code&gt;beta_p[pid[i]]&lt;/code&gt; is like saying “the intercept and slope for the &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; participant id”.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
model {
  vector[N_obs] mu;
  
  // priors
  beta ~ normal(0, 1);
  Omega ~ lkj_corr(2);
  sigma_p ~ exponential(1);
  sigma ~ exponential(1);
  beta_p ~ multi_normal(beta, quad_form_diag(Omega, sigma_p));
  
  // likelihood
  for(i in 1:N_obs) {
    mu[i] = x[i] * (beta_p[pid[i]]); // * is matrix multiplication in this context
  }
  
  y ~ normal(mu, sigma);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;full-stan-code-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Full Stan Code&lt;/h3&gt;
&lt;p&gt;Here is the full Stan code. I’ll save the file as “mod2.stan”.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
data {
  int N_obs; // number of observations
  int N_pts; // number of participants
  int K; // number of predictors + intercept
  int pid[N_obs]; // participant id vector
  matrix[N_obs, K] x; // matrix of predictors
  real y[N_obs]; // y vector
}

parameters {
  vector[K] beta_p[N_pts]; // ind participant intercept and slope coefficients by group
  vector&amp;lt;lower=0&amp;gt;[K] sigma_p; // sd for intercept and slope
  vector[K] beta; // intercept and slope hyper-priors
  corr_matrix[K] Omega; // correlation matrix
  real&amp;lt;lower=0&amp;gt; sigma; // population sigma
}

model {
  vector[N_obs] mu;
  
  // priors
  beta ~ normal(0, 1);
  Omega ~ lkj_corr(2);
  sigma_p ~ exponential(1);
  sigma ~ exponential(1);
  beta_p ~ multi_normal(beta, quad_form_diag(Omega, sigma_p));
  
  // likelihood
  for(i in 1:N_obs) {
    mu[i] = x[i] * (beta_p[pid[i]]); // * is matrix multiplication in this context
  }
  
  y ~ normal(mu, sigma);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-model-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running the Model&lt;/h2&gt;
&lt;p&gt;Running the model is the same as before. We put the data in a list and point Stan to the model file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_dat2 &amp;lt;- list(
  N_obs = nrow(dat),
  N_pts = max(as.numeric(dat$pid)),
  K = 2, # intercept + slope
  pid = as.numeric(dat$pid),
  x = matrix(c(rep(1, nrow(dat)), (dat$x - mean(dat$x)) / sd(dat$x)), ncol = 2), # z-score for x
  y = (dat$y - mean(dat$y)) / sd(dat$y) # z-score for y
)

stan_fit2 &amp;lt;- stan(file = &amp;quot;mod2.stan&amp;quot;,
                  data = stan_dat2,
                  chains = 4, cores = 4)

stan_fit2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The largest R-hat is NA, indicating chains have not mixed.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#r-hat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Inference for Stan model: mod2.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##               mean se_mean   sd   2.5%   25%   50%   75% 97.5% n_eff Rhat
## beta_p[1,1]   0.00    0.00 0.23  -0.45 -0.15  0.00  0.15  0.44  6242    1
## beta_p[1,2]  -0.86    0.00 0.21  -1.26 -1.00 -0.85 -0.72 -0.45  7097    1
## beta_p[2,1]   0.04    0.00 0.21  -0.37 -0.10  0.04  0.18  0.44  7101    1
## beta_p[2,2]  -0.30    0.00 0.24  -0.76 -0.47 -0.30 -0.13  0.15  6636    1
## beta_p[3,1]  -0.91    0.00 0.22  -1.35 -1.06 -0.91 -0.76 -0.48  5844    1
## beta_p[3,2]   0.59    0.00 0.28   0.02  0.40  0.59  0.78  1.13  6770    1
## beta_p[4,1]  -0.18    0.00 0.22  -0.61 -0.33 -0.19 -0.04  0.25  5989    1
## beta_p[4,2]   0.38    0.00 0.19   0.02  0.26  0.38  0.51  0.75  6585    1
## beta_p[5,1]  -0.21    0.00 0.23  -0.64 -0.37 -0.21 -0.06  0.23  5872    1
## beta_p[5,2]   0.65    0.00 0.30   0.07  0.45  0.65  0.85  1.24  4866    1
## beta_p[6,1]   0.06    0.00 0.27  -0.47 -0.12  0.06  0.24  0.58  5951    1
## beta_p[6,2]   0.17    0.00 0.36  -0.52 -0.07  0.18  0.42  0.88  5814    1
## beta_p[7,1]   0.28    0.00 0.22  -0.14  0.13  0.28  0.42  0.70  7576    1
## beta_p[7,2]  -0.38    0.00 0.18  -0.74 -0.51 -0.38 -0.25 -0.02  5964    1
## beta_p[8,1]   0.84    0.00 0.23   0.38  0.68  0.84  1.00  1.29  5784    1
## beta_p[8,2]  -0.26    0.00 0.26  -0.77 -0.42 -0.26 -0.09  0.24  7267    1
## beta_p[9,1]  -0.15    0.00 0.21  -0.57 -0.29 -0.16 -0.01  0.28  7067    1
## beta_p[9,2]   0.36    0.00 0.22  -0.06  0.21  0.36  0.51  0.78  6803    1
## beta_p[10,1]  0.45    0.00 0.22   0.02  0.30  0.45  0.60  0.87  7118    1
## beta_p[10,2]  0.42    0.00 0.35  -0.27  0.19  0.42  0.65  1.12  6263    1
## beta_p[11,1]  0.79    0.00 0.24   0.32  0.63  0.79  0.95  1.25  6972    1
## beta_p[11,2] -0.25    0.00 0.27  -0.79 -0.44 -0.25 -0.07  0.28  6911    1
## beta_p[12,1]  0.42    0.00 0.21   0.01  0.28  0.42  0.56  0.84  6710    1
## beta_p[12,2] -0.28    0.00 0.20  -0.66 -0.41 -0.28 -0.14  0.11  7242    1
## beta_p[13,1] -0.13    0.00 0.21  -0.54 -0.27 -0.13  0.01  0.27  6809    1
## beta_p[13,2]  0.80    0.00 0.23   0.36  0.65  0.80  0.95  1.25  7556    1
## beta_p[14,1] -0.10    0.00 0.25  -0.61 -0.27 -0.11  0.07  0.39  6420    1
## beta_p[14,2] -0.20    0.00 0.22  -0.65 -0.35 -0.20 -0.05  0.24  5735    1
## beta_p[15,1] -0.39    0.00 0.21  -0.81 -0.53 -0.39 -0.24  0.02  7247    1
## beta_p[15,2]  0.04    0.00 0.18  -0.31 -0.09  0.04  0.17  0.39  6130    1
## beta_p[16,1]  0.70    0.00 0.22   0.27  0.55  0.70  0.85  1.13  5340    1
## beta_p[16,2]  0.05    0.00 0.28  -0.49 -0.14  0.05  0.24  0.61  5202    1
## beta_p[17,1] -0.03    0.00 0.22  -0.46 -0.18 -0.03  0.11  0.38  6195    1
## beta_p[17,2] -0.52    0.00 0.25  -1.00 -0.69 -0.52 -0.35 -0.04  6510    1
## beta_p[18,1] -0.02    0.00 0.21  -0.43 -0.16 -0.02  0.12  0.40  6346    1
## beta_p[18,2]  0.16    0.00 0.19  -0.22  0.03  0.16  0.29  0.54  6541    1
## beta_p[19,1] -0.17    0.00 0.21  -0.60 -0.31 -0.17 -0.02  0.24  7586    1
## beta_p[19,2]  0.28    0.00 0.29  -0.29  0.08  0.28  0.47  0.85  6551    1
## beta_p[20,1]  0.71    0.00 0.21   0.31  0.57  0.70  0.85  1.12  7153    1
## beta_p[20,2] -0.95    0.00 0.28  -1.50 -1.13 -0.96 -0.77 -0.38  6444    1
## beta_p[21,1] -0.88    0.00 0.22  -1.33 -1.03 -0.88 -0.73 -0.45  5982    1
## beta_p[21,2]  0.67    0.00 0.29   0.12  0.48  0.66  0.86  1.23  6153    1
## beta_p[22,1] -0.14    0.00 0.21  -0.56 -0.28 -0.15  0.00  0.27  6553    1
## beta_p[22,2] -0.40    0.00 0.17  -0.74 -0.52 -0.40 -0.29 -0.08  6962    1
## beta_p[23,1] -0.70    0.00 0.22  -1.13 -0.84 -0.70 -0.55 -0.27  6758    1
## beta_p[23,2]  1.47    0.00 0.19   1.09  1.34  1.47  1.60  1.85  5822    1
## beta_p[24,1] -0.02    0.00 0.21  -0.44 -0.16 -0.02  0.12  0.40  6786    1
## beta_p[24,2]  0.57    0.00 0.19   0.20  0.44  0.57  0.70  0.94  6184    1
## beta_p[25,1]  0.39    0.00 0.23  -0.05  0.24  0.39  0.55  0.84  5859    1
## beta_p[25,2] -0.26    0.00 0.25  -0.74 -0.43 -0.26 -0.09  0.23  5833    1
## beta_p[26,1] -0.23    0.00 0.22  -0.66 -0.38 -0.23 -0.09  0.19  6953    1
## beta_p[26,2] -0.01    0.00 0.24  -0.48 -0.17 -0.01  0.15  0.47  6698    1
## beta_p[27,1] -0.27    0.00 0.22  -0.70 -0.42 -0.27 -0.13  0.17  6448    1
## beta_p[27,2] -0.18    0.00 0.16  -0.51 -0.30 -0.18 -0.08  0.13  6367    1
## beta_p[28,1]  0.84    0.00 0.21   0.43  0.70  0.83  0.98  1.27  5866    1
## beta_p[28,2] -0.50    0.00 0.21  -0.91 -0.64 -0.50 -0.36 -0.08  8503    1
## beta_p[29,1] -0.03    0.00 0.22  -0.46 -0.18 -0.03  0.12  0.42  6733    1
## beta_p[29,2]  0.55    0.00 0.27   0.04  0.38  0.56  0.73  1.08  5886    1
## beta_p[30,1] -0.19    0.00 0.21  -0.61 -0.33 -0.19 -0.04  0.22  6626    1
## beta_p[30,2]  1.10    0.00 0.28   0.55  0.91  1.10  1.29  1.66  6801    1
## sigma_p[1]    0.53    0.00 0.08   0.38  0.47  0.52  0.58  0.72  4455    1
## sigma_p[2]    0.63    0.00 0.10   0.46  0.56  0.62  0.69  0.85  4264    1
## beta[1]       0.02    0.00 0.11  -0.18 -0.05  0.02  0.09  0.23  4793    1
## beta[2]       0.10    0.00 0.13  -0.16  0.01  0.10  0.18  0.35  5562    1
## Omega[1,1]    1.00     NaN 0.00   1.00  1.00  1.00  1.00  1.00   NaN  NaN
## Omega[1,2]   -0.44    0.00 0.17  -0.74 -0.56 -0.45 -0.33 -0.07  3723    1
## Omega[2,1]   -0.44    0.00 0.17  -0.74 -0.56 -0.45 -0.33 -0.07  3723    1
## Omega[2,2]    1.00    0.00 0.00   1.00  1.00  1.00  1.00  1.00  1242    1
## sigma         0.62    0.00 0.04   0.56  0.60  0.62  0.65  0.70  3797    1
## lp__         -0.31    0.17 6.63 -14.36 -4.60  0.19  4.32 11.32  1529    1
## 
## Samples were drawn using NUTS(diag_e) at Tue Jun 09 10:08:29 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get a warning about some NA R-hat values and low Effective Samples Size. In this case, these are false alarms - they are merely artifacts of the correlation matrix &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt; having values that are invariably 1 (like a good correlation matrix should).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mcmc_areas(stan_fit2, pars = c(&amp;quot;beta[1]&amp;quot;, &amp;quot;beta[2]&amp;quot;, &amp;quot;sigma_p[1]&amp;quot;, &amp;quot;sigma_p[2]&amp;quot;, &amp;quot;sigma&amp;quot;), prob = 0.89) +
  scale_y_discrete(expand = c(0, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;When we look at the posterior densities of some of the parameters. We can see that the overall error, sigma, is lower than in the simple regression. This is because the variance in &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is now shared across population &lt;em&gt;and&lt;/em&gt; participant-level error. It’s also worth noting that the model is less certain about the population intercept and slope. The means are roughly the same, but the distribution is more spread out. Because there is participant-level variation in the intercepts and slopes, the model views the population estimates with greater uncertainty.&lt;/p&gt;
&lt;div id=&#34;visualizing-correlated-slopes-and-intercepts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualizing Correlated Slopes and Intercepts&lt;/h3&gt;
&lt;p&gt;If you look further down the list of parameters in the model output, you’ll see the four &lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;, &lt;code class=&#34;stan&#34;&gt;Omega&lt;/code&gt; parameters. These describe the correlation matrix for participant intercepts and slopes. A negative correlation here, tells us that as a participant’s intercept increases, their slope decreases. Let’s plot the participant-specific intercepts and slopes to see this.&lt;/p&gt;
&lt;p&gt;We use &lt;code class=&#34;R&#34;&gt;extract&lt;/code&gt; to get the &lt;code class=&#34;R&#34;&gt;beta_p&lt;/code&gt; parameters from the model. Then, using the &lt;code class=&#34;R&#34;&gt;apply&lt;/code&gt; function, we calculate the average of the samples for each &lt;code&gt;beta_p&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta_p_samples &amp;lt;- rstan::extract(stan_fit2, pars = &amp;quot;beta_p&amp;quot;)
beta_p &amp;lt;- as.data.frame(apply(beta_p_samples$beta_p, 2:3, mean)) %&amp;gt;%
  rename(intercept = V1, slope = V2)

beta_p %&amp;gt;%
  ggplot(aes(x = intercept, y = slope)) +
  geom_point(color = &amp;quot;cadetblue4&amp;quot;, alpha = 0.75, size = 3) +
  labs(title = &amp;quot;Participant intercept and slope are negatively correlated&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We want to infer beyond the participants in our sample though, so let’s look at the posterior samples for our &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; parameters. Like with the simple linear regression, we’re going to visualize all the regression lines in the posterior distribution - only now partitioned by intercept. We’ll split the samples into three equal groups based on their intercept. This is of course an arbitrary choice and we’re not implying that there are three distinct groups here. We just want to see how the slope varies by intercept and it’s a lot easier to do this by splitting the samples.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta_sim &amp;lt;- as.data.frame(rstan::extract(stan_fit2, pars = c(&amp;quot;beta[1]&amp;quot;, &amp;quot;beta[2]&amp;quot;))) %&amp;gt;%
  rename(intercept = beta.1., slope = beta.2.)

beta_sim %&amp;gt;%
  mutate(int_grp = factor(ntile(intercept, 3), labels = c(&amp;quot;[0-33%)&amp;quot;, &amp;quot;[33%-66%)&amp;quot;, &amp;quot;[66%-100%)&amp;quot;))) %&amp;gt;%
  group_by(int_grp) %&amp;gt;%
  mutate(av_int = mean(intercept),
         av_slope = mean(slope)) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot() +
  geom_abline(aes(intercept = intercept, slope = slope), color = &amp;quot;cadetblue4&amp;quot;, alpha = 0.20) +
  geom_abline(aes(intercept = av_int, slope = av_slope), color = &amp;quot;black&amp;quot;, size = 2) +
  scale_x_continuous(limits = c(-1, 1)) +
  scale_y_continuous(limits = c(-0.75, 0.75)) +
  labs(x = &amp;quot;x&amp;quot;,
       y = &amp;quot;y&amp;quot;,
       title = &amp;quot;Relation between x and y as a function of intercept.&amp;quot;,
       subtitle = &amp;quot;Intercept percentile&amp;quot;) +
  facet_wrap(~int_grp) +
  theme(plot.subtitle = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yes, the slope seems to be getting shallower as the intercept increases.&lt;/p&gt;
&lt;p&gt;Although I find the ‘many lines’ approach to be appealing, it is more common to present figures displaying means and &lt;strong&gt;compatibility intervals&lt;/strong&gt; (Bayesian equivalent of a confidence interval). We start by creating a sequence of &lt;code class=&#34;R&#34;&gt;x&lt;/code&gt; values over a specified range. Since we’re dealing in standardized units, we’ll make it from -1 to 1. Then for each value of &lt;code class=&#34;R&#34;&gt;x&lt;/code&gt; in this sequence we calculate the expected value of y using the &lt;code class=&#34;R&#34;&gt;beta&lt;/code&gt; samples.&lt;/p&gt;
&lt;p&gt;I again make use of dplyr’s &lt;code class=&#34;R&#34;&gt;map_dfr&lt;/code&gt; function to iterate over each value of &lt;code class=&#34;R&#34;&gt;x&lt;/code&gt; and I also bin the intercept into the three groups from before. After this step, we have a large dataframe of 100 &lt;code class=&#34;R&#34;&gt;x&lt;/code&gt; values for each of 4000 samples. Then again for each value of &lt;code class=&#34;R&#34;&gt;x&lt;/code&gt; we calculate the mean (mu) of the samples and its lower and upper bound for the compatiblity interval. Here I adopt McElreath’s convention of 89% compatability interval, but there’s nothing more special about this value than, say, 95%.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;seq(-1, 1, length.out = 100) %&amp;gt;% # create a sequence of evenly spaced numbers from -1 to 1
  map_dfr(~ data.frame(y = beta_sim$intercept + .x * beta_sim$slope,
                       x = .x,
                       int = factor(ntile(beta_sim$intercept, 3), levels = c(1, 2, 3),
                                    labels = c(&amp;quot;[0-33%)&amp;quot;, &amp;quot;[33%-66%)&amp;quot;, &amp;quot;[66%-100%)&amp;quot;)))) %&amp;gt;%
  group_by(x, int) %&amp;gt;%
  summarise(mu = mean(y),
            lower = quantile(y, probs = 0.055),
            upper = quantile(y, probs = 0.945)) %&amp;gt;%
  ungroup() %&amp;gt;%
  ggplot() +  
  geom_ribbon(aes(x = x, ymin = lower, ymax = upper), fill = &amp;quot;cadetblue4&amp;quot;, alpha = 0.50) +
  geom_line(aes(x = x, y = mu), color = &amp;quot;black&amp;quot;, size = 2) +
  labs(title = &amp;quot;Relation between x and mu as a function of intercept&amp;quot;,
       subtitle = &amp;quot;Intercept percentile&amp;quot;,
       caption = &amp;quot;Shaded area = 89% compatibility interval&amp;quot;) +
  facet_wrap(~int) +
  theme(plot.subtitle = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now that’s a publishable figure!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-more-efficient-varying-slopes-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A More Efficient Varying Slopes Model&lt;/h2&gt;
&lt;p&gt;Our toy data played nicely with us, but this is rarely the case in the real world. Chances are if you perform the varying slopes model on your own data you’ll encounter efficiency issues and errors. Namely, you’ll be confronted with &lt;strong&gt;divergent transitions&lt;/strong&gt;. These occur when the Hamiltonian Monte Carlo simulation (Stan’s engine) sort of “falls off the track”, so to speak. I won’t elaborate on the math behind divergent transitions (you can find out more &lt;a href=&#34;https://stats.stackexchange.com/questions/432479/divergent-transitions-in-stan#:~:text=1%20Answer&amp;amp;text=A%20divergent%20transition%20in%20Stan,is%20geometrically%20difficult%20to%20explore.&#34;&gt;here&lt;/a&gt;), but I will show how to avoid them by rewriting the model.&lt;/p&gt;
&lt;p&gt;You’ll hear people say &lt;strong&gt;re-parameterization&lt;/strong&gt; when they’re talking about this. The more efficient, so-called &lt;strong&gt;non-centered parameterization&lt;/strong&gt; is certainly more efficient, but has some features that initially seem arbitrary. Although the model has different parameters, it is still mathematically the same model. Let’s walk through the Stan code, highlighting the new features.&lt;/p&gt;
&lt;p&gt;The data block is the same. In the parameters block we’re going to create a matrix, &lt;code class=&#34;stan&#34;&gt;z_p&lt;/code&gt;, that will hold our standardized intercepts and slopes (that’s what the z stands for). The big novelty though is that we’re expressing the correlation matrix as a &lt;code class=&#34;stan&#34;&gt;cholesky_factor_corr&lt;/code&gt;. Without going deep into the weeds, a &lt;strong&gt;Cholesky factorization&lt;/strong&gt; of a matrix takes a positive definite matrix (like a correlation matrix) and decomposes it into a product of a lower triangular matrix and its transpose. For linear algebraic reasons, this speeds up the efficiency.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
data {
  int N_obs; // number of observations
  int N_pts; // number of participants
  int K; // number of predictors + intercept
  int pid[N_obs]; // participant id vector
  matrix[N_obs, K] x; // matrix of predictors
  real y[N_obs]; // y vector
}

parameters {
  matrix[K, N_pts] z_p; // matrix of intercepts and slope
  vector&amp;lt;lower=0&amp;gt;[K] sigma_p; // sd for intercept and slope
  vector[K] beta; // intercept and slope hyper-priors
  cholesky_factor_corr[K] L_p; // Cholesky correlation matrix
  real&amp;lt;lower=0&amp;gt; sigma; // population sigma
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have to add a new block called &lt;code class=&#34;stan&#34;&gt;transformed parameters&lt;/code&gt;. In this block we can apply transformations to our parameters before calculating the likelihood. What I’m doing here is creating a new matrix of intercepts and slopes called &lt;code class=&#34;stan&#34;&gt;z&lt;/code&gt; and then performing some matrix algebra. &lt;code class=&#34;stan&#34;&gt;diag_pre_multiply&lt;/code&gt; is an efficient way of doing &lt;code class=&#34;stan&#34;&gt;diag(sigma_p) * L_p&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
transformed parameters {
  matrix[K, N_pts] z; // non-centered version of beta_p
  z = diag_pre_multiply(sigma_p, L_p) * z_p; 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the model block we don’t need to change very much. We use a &lt;code class=&#34;stan&#34;&gt;lkj_corr_cholesky(2)&lt;/code&gt; prior for &lt;code class=&#34;stan&#34;&gt;L_p&lt;/code&gt;. The likelihood itself is less elegantly expressed than before. Again, the bracket indexing can be confusing. Just remember that &lt;code class=&#34;stan&#34;&gt;z&lt;/code&gt; is a matrix where column 1 has the participant intercepts and column 2 has the participant slopes. We still have &lt;code class=&#34;stan&#34;&gt;x&lt;/code&gt; as a model matrix but we’re only using the second column, so if we wanted to just have it as a vector in the data that would be fine.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
model {
  vector[N_obs] mu;
  
  // priors
  beta ~ normal(0, 1);
  sigma ~ exponential(1);
  sigma_p ~ exponential(1);
  L_p ~ lkj_corr_cholesky(2);
  to_vector(z_p) ~ normal(0, 1);
  
  // likelihood
  for(i in 1:N_obs) {
    mu[i] = beta[1] + z[1, pid[i]] + (beta[2] + z[2, pid[i]]) * x[i, 2];
  }
  y ~ normal(mu, sigma);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, to get back the correlation matrix for the intercepts and slopes, we add one more block to the code: &lt;code class=&#34;stan&#34;&gt;generated quantities&lt;/code&gt;. We want to get back &lt;code&gt;Omega&lt;/code&gt;, so we use &lt;code&gt;multiply_lower_tri_self_transpose(L_p)&lt;/code&gt; to ‘multiply the lower triangular matrix by itself transposed’ (remember what I said about Cholesky factorization).&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
generated quantities {
  matrix[2, 2] Omega;
  Omega = multiply_lower_tri_self_transpose(L_p);
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;full-stan-code-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Full Stan Code&lt;/h3&gt;
&lt;p&gt;Here’s the full non-centered parameterization code.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;// STAN CODE
data {
  int N_obs; // number of observations
  int N_pts; // number of participants
  int K; // number of predictors + intercept
  int pid[N_obs]; // participant id vector
  matrix[N_obs, K] x; // matrix of predictors
  real y[N_obs]; // y vector
}

parameters {
  matrix[K, N_pts] z_p; // matrix of intercepts and slope
  vector&amp;lt;lower=0&amp;gt;[K] sigma_p; // sd for intercept and slope
  vector[K] beta; // intercept and slope hyper-priors
  cholesky_factor_corr[K] L_p; // Cholesky correlation matrix
  real&amp;lt;lower=0&amp;gt; sigma; // population sigma
}

transformed parameters {
  matrix[K, N_pts] z; // non-centered version of beta_p
  z = diag_pre_multiply(sigma_p, L_p) * z_p; 
}

model {
  vector[N_obs] mu;
  
  // priors
  beta ~ normal(0, 1);
  sigma ~ exponential(1);
  sigma_p ~ exponential(1);
  L_p ~ lkj_corr_cholesky(2);
  to_vector(z_p) ~ normal(0, 1);
  
  // likelihood
  for(i in 1:N_obs) {
    mu[i] = beta[1] + z[1, pid[i]] + (beta[2] + z[2, pid[i]]) * x[i, 2];
  }
  y ~ normal(mu, sigma);
}

generated quantities {
  matrix[2, 2] Omega;
  Omega = multiply_lower_tri_self_transpose(L_p);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ll save it to the working directory as “mod2-nc.stan”.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-the-model-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running the Model&lt;/h2&gt;
&lt;p&gt;Running the model is the same as before. The data is exactly the same, but for symmetry I’ve renamed it &lt;code class=&#34;R&#34;&gt;stan_dat3&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_dat3 &amp;lt;- list(
  N_obs = nrow(dat),
  N_pts = max(as.numeric(dat$pid)),
  K = 2, # intercept + slope
  pid = as.numeric(dat$pid),
  x = matrix(c(rep(1, nrow(dat)), (dat$x - mean(dat$x)) / sd(dat$x)), ncol = 2), # z-score for x
  y = (dat$y - mean(dat$y)) / sd(dat$y) # z-score for y
)

stan_fit3 &amp;lt;- stan(file = &amp;quot;mod2-nc.stan&amp;quot;,
                  data = stan_dat3,
                  chains = 4, cores = 4)

stan_fit3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stan-random-slopes/stan-random-slopes_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looking at the posterior densities for the population betas, sigma, and participant sigmas, they are same as the previous model with only minor differences in sampling error.&lt;/p&gt;
&lt;p&gt;In summary, use the non-centered parameterization (the one with the Cholesky factorization) when you find your varying effects models misbehaving.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;resources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Resources&lt;/h1&gt;
&lt;p&gt;I couldn’t end this post without recommending Richard McElreath’s &lt;a href=&#34;https://xcelab.net/rm/statistical-rethinking/&#34;&gt;&lt;em&gt;Statistical Rethinking&lt;/em&gt;&lt;/a&gt;. It is a bastion of Bayesian knowledge and truly a joy to read. There are also several blog posts that I recommend if you’re looking for more multilevel Stan fun:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alexpghayes.com/blog/some-things-ive-learned-about-stan/&#34;&gt;SOME THINGS I’VE LEARNED ABOUT STAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mlisi.xyz/post/bayesian-multilevel-models-r-stan/&#34;&gt;Bayesian multilevel models using R and Stan (part 1)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>Stan</category>
      
            <category>Bayesian</category>
      
            <category>Linear Mixed Models</category>
      
            <category>Multilevel Modeling</category>
      
            <category>Tutorial</category>
      
      
            <category>R</category>
      
            <category>Stan</category>
      
            <category>Bayesian</category>
      
            <category>Linear Mixed Models</category>
      
            <category>Multilevel Modeling</category>
      
            <category>Tutorial</category>
      
    </item>
    
    <item>
      <title>Bayesian Linear Mixed Models: Random Intercepts, Slopes, and Missing Data</title>
      <link>/post/bayesian_mlm/bayesian_mlm/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/bayesian_mlm/bayesian_mlm/</guid>
      <description>


&lt;p&gt;This past summer, I watched a brilliant &lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&amp;amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&#34;&gt;lecture series by Richard McElreath&lt;/a&gt; on Bayesian statistics. It honestly changed my whole outlook on statistics, so I couldn’t recommend it more (plus, McElreath is an engaging instructor). One of the most compelling cases for using Bayesian statistics is with a collection of statistical tools called &lt;em&gt;linear mixed models&lt;/em&gt; or &lt;em&gt;multilevel/hierarchical models&lt;/em&gt;. It’s common that data are grouped or clustered in some way. Often in psychology we have repeated observations nested within participants, so we know that data coming from the same participant will share some variance. Linear mixed models are powerful tools for dealing with multilevel data, usually in the form of modeling &lt;em&gt;random intercepts&lt;/em&gt; and &lt;em&gt;random slopes&lt;/em&gt;. In this tutorial I assume familiarity with linear regression and some background knowledge in Bayesian inference, such that you should have some familiarity with &lt;em&gt;priors&lt;/em&gt; and &lt;em&gt;posterior distributions&lt;/em&gt; (if not, go &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0732118X18300746?via%3Dihub&#34;&gt;here&lt;/a&gt;) or watch McElreath’s videos.&lt;/p&gt;
&lt;p&gt;Why use Bayesian instead of Frequentist statistics? One reason is that &lt;em&gt;prior&lt;/em&gt; knowledge about a parameter - for example, it’s distribution - can be incorporated in the model. Another reason especially relevant to linear mixed models is that we can easily include multiple random intercepts and slopes without running into the same stringent sample size requirements as with frequentist approaches. This is not an exhaustive list; more can be found &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0732118X18300746?via%3Dihub&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;setting-it-all-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting it All Up&lt;/h2&gt;
&lt;p&gt;Installing and running &lt;em&gt;brms&lt;/em&gt; is a bit more complicated than your run-of-the-mill R packages. Because &lt;em&gt;brms&lt;/em&gt; uses Stan as its back-end engine to perform Bayesian analysis, you will need to install &lt;em&gt;rstan&lt;/em&gt;. Carefully follow the instructions at this &lt;a href=&#34;https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started&#34;&gt;link&lt;/a&gt; and you should have no problem. Once you’ve done that you should be able to install &lt;em&gt;brms&lt;/em&gt; and load it up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Rcpp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading &amp;#39;brms&amp;#39; package (version 2.11.1). Useful instructions
## can be found by typing help(&amp;#39;brms&amp;#39;). A more detailed introduction
## to the package is available through vignette(&amp;#39;brms_overview&amp;#39;).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s normal to get some warning messages upon loading the package. That’s fine. We can now load up the &lt;em&gt;tidyverse&lt;/em&gt; for data manipulation and visualization.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is available on GitHub &lt;a href=&#34;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%205/Curran/CurranLong.sav&#34;&gt;here&lt;/a&gt; in a .sav format. You’ll need the &lt;em&gt;haven&lt;/em&gt; package to read it into R. Make sure you import it from your working directory or specify the path to your downloads folder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(haven)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The dataset was curated by &lt;a href=&#34;http://curran.web.unc.edu/&#34;&gt;Patrick Curran&lt;/a&gt; for the purpose of demonstrating different approaches for handling multilevel, longitudinal data (see &lt;a href=&#34;https://jflournoy.github.io/assets/pdf/srcdmeth.pdf&#34;&gt;here&lt;/a&gt; for more info). The dataset consists of 405 children in the first two years of elementary school measured over four assessment points on reading recognition and antisocial behaviour. As well, it included time invariant covariates, such as emotional support and cognitive stimulation. For this post, we’ll focus on reading and cognitive stimulation, and we’ll use Bayesian Linear Mixed Models to address a number of questions about children’s reading ability.&lt;/p&gt;
&lt;p&gt;For now, we’ll omit assessment periods with missing data, but we’ll return to the issue of missing data at the end of this demonstration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat &amp;lt;- read_sav(&amp;quot;CurranLong.sav&amp;quot;) %&amp;gt;%
  select(id, occasion, read, homecog) %&amp;gt;%
  filter(complete.cases(.))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,325 x 4
##       id occasion  read homecog
##    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1    22        0   2.1      13
##  2    22        1   3.9      13
##  3    34        0   2.1       9
##  4    34        1   2.9       9
##  5    34        2   4.5       9
##  6    34        3   4.5       9
##  7    58        0   2.3       9
##  8    58        1   4.5       9
##  9    58        2   4.2       9
## 10    58        3   4.6       9
## # ... with 1,315 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is conveniently in long form already, which means that each row represents a single &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; assessment period for an &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; child. With no other data cleaning to do, we can move ahead with our research questions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-1-one-random-effect-no-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 1: One Random Effect, No Covariates&lt;/h2&gt;
&lt;p&gt;We’ll start with a population-level intercept and a random intercept for participants. Our formula looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the population intercept, &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; is a random intercept term for an &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; participant, and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ij}\)&lt;/span&gt; is a random noise term for an &lt;span class=&#34;math inline&#34;&gt;\(i^{th}\)&lt;/span&gt; participant and &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; assessment period.&lt;/p&gt;
&lt;p&gt;If you find formulae distasteful, that’s OK. They’re not essential to understanding how linear mixed models work. conceptually, think of the &lt;span class=&#34;math inline&#34;&gt;\(u_{0i}\)&lt;/span&gt; in the above formula as giving a little &lt;em&gt;nudge&lt;/em&gt; to the intercept for each participant.&lt;/p&gt;
&lt;p&gt;Here I’ll walk carefully through the &lt;em&gt;brms&lt;/em&gt; code and then each subsequent model will just build one this one. We start by providing our data. The &lt;em&gt;family&lt;/em&gt; argument is where we supply a distribution for our outcome variable, reading. We’ll use the default &lt;em&gt;gaussian&lt;/em&gt; to indicate that reading should be normally distributed. The next line is where we specify our model formula. If you’re familiar with mixed effects models in the &lt;em&gt;lme4&lt;/em&gt; package you’ll be happy to see there are no differences here. I’ve only included the population level intercept ‘1’ after ‘read ~’ to be precise, but this will be included by default. The random intercept is indicated by the ‘(1 | id)’ as in typical mixed effects notation. We then specify some priors. Our first prior is for the population-level &lt;em&gt;Intercept&lt;/em&gt;. We’ll specify a fairly wide normal distribution for the intercept.&lt;/p&gt;
&lt;p&gt;If this whole business of deciding on priors seems strange and arbitrary, just note that a wide prior such as this one will have little bearing on the posterior distribution. If instead we were extremely confident that the intercept was zero, we could use a narrow prior of ‘normal(0, .05)’ which would have a strong effect on the resulting posterior distribution.&lt;/p&gt;
&lt;p&gt;The next prior is for the &lt;em&gt;standard deviation&lt;/em&gt; of the random effects. Unlike an intercept which can be positive or negative, variance (and by association, standard deviation) can only be positive, so we specify a &lt;em&gt;cauchy&lt;/em&gt; distribution that constrains the sd to be positive. We do the same for &lt;em&gt;sigma&lt;/em&gt; which is the overall variability.&lt;/p&gt;
&lt;p&gt;The rest of the code has to do with how the Markov Chain Monte Carlo (MCMC) algorithm runs. Without going too much into the &lt;em&gt;whys&lt;/em&gt; (a more detailed treatise can be found &lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/sampling-the-imaginary.html#sampling-to-simulate-prediction&#34;&gt;here&lt;/a&gt;), we will run 2,000 chained simulations. The first 1,000 of these will be in the warmup period and we will be rejected. We will run 4 parallel chains on 4 cores (if your computer has fewer cores you will want to reduce this). To help the model converge, I’ve increased the adapt_delta and max_treedepth arguments. Finally, an arbitrary seed number for reproducibility.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read1 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             formula = read ~ 1 + (1 | id),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + (1 | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.54      0.08     0.37     0.68 1.00      992     1847
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     4.11      0.05     4.01     4.21 1.00     4866     3424
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.55      0.04     1.48     1.62 1.00     2219     2827
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does the output tell us? Right away, we see an estimate of an Intercept of 4.11 within a 95% &lt;em&gt;credible interval&lt;/em&gt; (you might call it the Bayesian version of a confidence interval, although they’re not quite the same thing) of 4.01 and 4.21. What’s more interesting is the estimate of standard deviation of the random intercepts ‘sd(Intercept)’. So there is person-level variability in reading scores. In turn, &lt;em&gt;sigma&lt;/em&gt; is the estimate of the overall variability in reading scores.&lt;/p&gt;
&lt;p&gt;We can further inspect these estimates by looking at the traceplots and the posterior distributions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(read1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each row corresponds with a parameter in the model. The traceplot shows the MCMC samples. These should resemble ‘hairy caterpillars’ - we don’t want to see the chain getting stuck near a particular value.&lt;/p&gt;
&lt;p&gt;For a more intuitive understanding of what the mixed effects model is doing, let’s compare a sample of the observed values with the model predicted values. &lt;em&gt;brms&lt;/em&gt; outputs fitted values using the &lt;em&gt;fitted&lt;/em&gt; function. We’ll sample 6 participants and see their observed reading scores and model-derived reading scores.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

ids &amp;lt;- sample(unique(curran_dat$id), 6)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read1))) %&amp;gt;%
  filter(id %in% ids) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 1: One Random Effect, No Covariates&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that each estimate is drawn toward the population-level mean and its cluster-level mean. This phenomenon is called &lt;em&gt;partial-pooling&lt;/em&gt; or &lt;em&gt;shrinkage&lt;/em&gt;. If we’d run the model without the random effect, all of the estimates (black circles) would be on a single horizontal line. But here we have some variation between individuals. Notice too how points further away from the means (population- and cluster-level) are pulled more strongly. However, there is clearly an increase in reading ability over time that is being ignored by this model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-2-two-random-effects-no-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 2: Two Random Effects, No Covariates&lt;/h2&gt;
&lt;p&gt;Let’s add a random intercept for assessment period (labelled ‘occasion’ in this dataset). This model will recognize that observations are nested within participants &lt;em&gt;and&lt;/em&gt; assessment periods. For now, we’ll consider assessment as a random effect because we are still only interested in the variability in reading scores. Here’s our new formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+w_{0j}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now we have a term &lt;span class=&#34;math inline&#34;&gt;\(w_{0j}\)&lt;/span&gt; for the random intercept for the &lt;span class=&#34;math inline&#34;&gt;\(j^{th}\)&lt;/span&gt; assessment period.&lt;/p&gt;
&lt;p&gt;The only change to our model code is that we’ve added a second random intercept: ‘(1 | occasion)’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read2 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             read ~ 1 + (1 | id) + (1 | occasion),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + (1 | id) + (1 | occasion) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.89      0.04     0.82     0.96 1.00     1270     2122
## 
## ~occasion (Number of levels: 4) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.71      0.86     0.79     3.99 1.00     1701     1878
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     4.32      0.89     2.46     6.08 1.00     1426     1848
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.64      0.01     0.61     0.67 1.00     3630     3056
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking again at our output now, we see variability in the random intercepts for id, but even larger variability across occasions (assessment periods). In fact, the overall variability, sigma, is quite reduced from the previous model. This demonstrates an important lesson about mixed models: that they decompose the overall variance (sigma) into cluster-level variance. If we just ran a linear regression, it wouldn’t differentiate between measurement error and variation between participants and time points.&lt;/p&gt;
&lt;p&gt;Let’s again compare the estimates with the observed values. We’ll use the same subsample as before to illustrate the effect of adding a second random effect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read2))) %&amp;gt;%
  filter(id %in% ids) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 2: Two Random Effects, No Covariates&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the model now adjusts the points based on the assessment-level intercept. The fact that the black circles follow a nearly linear increase is telling us that there is probably an effect of time, such that children get better at reading over time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-3-one-random-effect-one-covariate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 3: One Random Effect, One Covariate&lt;/h2&gt;
&lt;p&gt;Now let’s make a conceptual shift. Maybe as researchers we decide that we’re interested in the effect of time (assessment period) on reading ability. Since each assessment was spaced equally apart in time and we have good reason to think that time has a global effect on reading, it makes sense to include it as a fixed effect (this sort of conceptual decision should be made beforehand, but for illustrative purposes I’ve shown it as a random and a fixed effect).&lt;/p&gt;
&lt;p&gt;It’s always a good idea to visualize what’s going on so that we know if our assumptions are tenable. we’ll create a lineplot showing the change in reading ability over time, but with each line representing an individual in our sample.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  ggplot(aes(x = occasion, y = read, group = id)) +
  geom_line(size = .75, alpha = .20) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;) +
  theme_minimal(base_size = 16)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s a clear upward trend - most children improve in their reading over time. We can now proceed with formally testing the growth in reading:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+\beta_1Time_j+u_{0i}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s the model code. Note that we are still modeling the random effect of id, but now occasion is a covariate (predictor). We need to specify a new prior, &lt;em&gt;b&lt;/em&gt; for beta. The beta estimate for the effect of time on reading ability is assumed to derive from a normal distribution with mean 0 and sd 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read3 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             read ~ 1 + occasion + (1 | id),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(normal(0, 1), class = b),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + occasion + (1 | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.89      0.04     0.82     0.96 1.00     1204     1906
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.70      0.05     2.60     2.81 1.00     1036     1692
## occasion      1.10      0.02     1.07     1.14 1.00     6696     3112
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.68      0.02     0.65     0.71 1.00     3004     3138
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our output gives a smaller intercept because it’s not telling us the mean any more, instead it’s the y-intercept of the regression line. We have an estimate of beta as 1.10 within a 95% credible interval of 1.07 and 1.14. So there’s strong evidence of a positive effect of time on reading ability.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read3))) %&amp;gt;%
  filter(id %in% ids) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 3: One Random Effect, One Covariate&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The estimates aren’t actually that different from the model with two random effects. The main difference now is that the black circles show a strictly linear trend that is the same across participants. But maybe this assumption of equal effects across the board isn’t tenable either.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-4-one-random-slope-one-covariate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 4: One Random Slope, One Covariate&lt;/h2&gt;
&lt;p&gt;Let’s ramp things up by modeling a random slope for each participant:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+\beta_1Time_j+u_{1ij}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(u_{1ij}\)&lt;/span&gt; is the random slope for the effect of assessment period between participants.&lt;/p&gt;
&lt;p&gt;A random slope model also has a random intercept, but now, the slope for time on reading ability will be different for each participant: ‘(1 + occasion | id)’. Another change to our model code is a new prior specifying the correlation between the Intercept and the beta for occasion. For an overview of the Cholesky Distribution I’d recommend this &lt;a href=&#34;https://www.psychstatistics.com/2014/12/27/d-lkj-priors/&#34;&gt;post&lt;/a&gt;, but all you need to know here is that the higher above 1 the value is, the more “speculative” the model is of strong correlations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read4 &amp;lt;- brm(data = curran_dat,
             family = gaussian,
             read ~ 1 + occasion + (1 + occasion | id),
             prior = c(prior(normal(0, 10), class = Intercept),
                       prior(normal(0, 1), class = b),
                       prior(cauchy(0, 1), class = sd),
                       prior(cauchy(0, 1), class = sigma),
                       prior(lkj_corr_cholesky(1.5), class = cor)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + occasion + (1 + occasion | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)               0.76      0.04     0.68     0.84 1.00     1279
## sd(occasion)                0.27      0.02     0.22     0.32 1.00      614
## cor(Intercept,occasion)     0.29      0.12     0.07     0.53 1.00      664
##                         Tail_ESS
## sd(Intercept)               2218
## sd(occasion)                1082
## cor(Intercept,occasion)     1070
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.70      0.05     2.60     2.79 1.00     1465     2297
## occasion      1.12      0.02     1.08     1.16 1.00     2963     3299
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.59      0.02     0.56     0.62 1.00      908     1758
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(25)

curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read4))) %&amp;gt;%
  filter(id %in% ids) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 4, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 4, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 4: One Random Slope, One Covariate&amp;quot;,
       subtitle = &amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;) +
  scale_x_continuous(expand = c(.075, .075), breaks = 0:3) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 14) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I won’t belabour the output here. Just notice how the strength of the linear trend for the fitted values is different for each participant. We can see this across the whole sample with the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read4))) %&amp;gt;%
  ggplot() +
  geom_line(aes(x = occasion, y = Estimate, group = id), size = .75, alpha = .30) +
  geom_line(aes(x = occasion, y = read, group = id), size = .75, alpha = .15, color = &amp;quot;dodgerblue2&amp;quot;) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Model 4: One Random Slope, One Covariate&amp;quot;,
       subtitle = &amp;quot;Blue lines are observed values. Black lines are fitted.&amp;quot;) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-5-one-random-slope-two-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 5: One Random Slope, Two Covariates&lt;/h2&gt;
&lt;p&gt;Suppose we wanted to see whether there’s an effect of the amount of &lt;em&gt;cognitive stimulation&lt;/em&gt; that children are exposed to at home on reading ability over time. Maybe cognitive stimulation impacts some children more strongly than others. Our last formula would thus be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[READ_{ij} = \beta_0+u_{0i}+\beta_1Time_j+u_{1ij}+\beta_{2}Cog_i+u_{2ij}+\epsilon_{ij}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Does the hypothesis hold up to a visual inspection? For illustrative purposes, we can split the data into equal bins of low, medium, and high cognitive stimulation. Of course, because stimulation is a continuous variable, it will stay continuous in our Bayesian model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  mutate(homecog_grp = factor(ntile(homecog, 3))) %&amp;gt;%
  ggplot(aes(x = occasion, y = read)) +
  geom_line(aes(group = id, color = homecog_grp), size = .75, alpha = .50) +
  geom_smooth(method = &amp;#39;lm&amp;#39;, color = &amp;quot;black&amp;quot;, size = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Cognitive Home Environment&amp;quot;) +
  facet_wrap(~homecog_grp, labeller = as_labeller(c(&amp;quot;1&amp;quot; = &amp;quot;Low&amp;quot;,
                                                    &amp;quot;2&amp;quot; = &amp;quot;Medium&amp;quot;,
                                                    &amp;quot;3&amp;quot; = &amp;quot;High&amp;quot;))) +
  guides(color = FALSE) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5, size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If there is an effect of cognitive stimulation, it’s a small one. So let’s find out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read5 &amp;lt;- brm(data = curran_dat,
            family = gaussian,
            read ~ 1 + occasion + homecog + (1 + occasion + homecog | id),
            prior = c(prior(normal(0, 5), class = Intercept),
                      prior(normal(0, 1), class = b),
                      prior(cauchy(0, 1), class = sd),
                      prior(cauchy(0, 1), class = sigma),
                      prior(lkj_corr_cholesky(1.5), class = cor)),
            iter = 2000, warmup = 1000, chains = 4, cores = 4,
            control = list(adapt_delta = .975, max_treedepth = 20),
            seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The model has not converged (some Rhats are &amp;gt; 1.05). Do not analyse the results! 
## We recommend running more iterations and/or setting stronger priors.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: There were 150 divergent transitions after warmup. Increasing adapt_delta above 0.975 may help.
## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: read ~ 1 + occasion + homecog + (1 + occasion + homecog | id) 
##    Data: curran_dat (Number of observations: 1325) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##                         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)               0.72      0.11     0.50     0.99 1.02      456
## sd(occasion)                0.27      0.03     0.22     0.32 1.00     1010
## sd(homecog)                 0.04      0.02     0.00     0.09 1.10       39
## cor(Intercept,occasion)     0.28      0.20    -0.14     0.70 1.02      178
## cor(Intercept,homecog)     -0.07      0.39    -0.73     0.73 1.05       70
## cor(occasion,homecog)      -0.02      0.38    -0.72     0.73 1.01      247
##                         Tail_ESS
## sd(Intercept)                165
## sd(occasion)                1608
## sd(homecog)                   54
## cor(Intercept,occasion)      319
## cor(Intercept,homecog)       369
## cor(occasion,homecog)        418
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     2.15      0.16     1.84     2.46 1.00     2211     2188
## occasion      1.12      0.02     1.07     1.16 1.00     3363     3237
## homecog       0.06      0.02     0.03     0.10 1.00     2290     2399
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.59      0.02     0.56     0.63 1.00     1120     2369
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running this model, we get a few warnings about large Rhats and divergent transitions. We would be better off running a few more iterations (maybe 4000) if we wanted more confidence in the results. Nevertheless, we see a pretty weak effect of cognitive stimulation. The credible interval for the population effect of cognitive stimulation doesn’t include zero, but the evidence is weak that it’s doing anything at all in this model. There’s some indication that the intercepts vary by cognitive stimulation, such that at higher stimulation children have a higher initial reading ability. But there’s no evidence that the slopes vary by cognitive stimulation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read5))) %&amp;gt;%
  mutate(homecog_grp = factor(ntile(homecog, 3))) %&amp;gt;%
  ggplot(aes(x = occasion, y = Estimate, group = id, color = homecog_grp)) +
  geom_line(size = .75, alpha = .50) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Cognitive Home Environment&amp;quot;) +
  facet_wrap(~homecog_grp, labeller = as_labeller(c(&amp;quot;1&amp;quot; = &amp;quot;Low&amp;quot;,
                                                    &amp;quot;2&amp;quot; = &amp;quot;Medium&amp;quot;,
                                                    &amp;quot;3&amp;quot; = &amp;quot;High&amp;quot;))) +
  guides(color = FALSE) +
  theme_minimal(base_size = 16) +
  theme(plot.title = element_text(hjust = .5, size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-6-dealing-with-missing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 6: Dealing with Missing Data&lt;/h2&gt;
&lt;p&gt;So far we’ve been dealing with complete data - where each assessment has no missing values. In doing so, we are effectively claiming that the data are missing completely at random (MCAR). This may not be a tenable assumption. Missingness on one variable may be correlated with another variable (Missing at Random; MAR) or, even worse, correlated with the variable itself (Missing Not At Random; MNAR). Without diving into the theoretical aspects of missing data (a more thoughtful discussion can be found &lt;a href=&#34;https://stefvanbuuren.name/fimd/sec-MCAR.html&#34;&gt;here&lt;/a&gt;) let’s end by running Bayesian imputation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;curran_dat_missing &amp;lt;- read_sav(&amp;quot;CurranLong.sav&amp;quot;) %&amp;gt;%
  select(id, occasion, read, homecog)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In &lt;em&gt;brms&lt;/em&gt; we indicate missingness with &lt;em&gt;mi()&lt;/em&gt;. Here we’re rerunning Model 5, but we’re also imputing missingness on reading ability and cognitive stimulation. We actually have two model formulae - one for each variable in the model with missing data. I’ll also note quickly that we have hidden missingness in this data as missing assessment points do not show up as rows in the dataset. For the purpose of this demonstration, it won’t be a problem, but in the real world, we’d want to impute these as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read6 &amp;lt;- brm(data = curran_dat_missing,
             family = gaussian,
             bf(read | mi() ~ 1 + occasion + mi(homecog) + (1 + occasion | id)) +
               bf(homecog | mi() ~ 1) + 
               set_rescor(FALSE),
             prior = c(prior(normal(0, 5), class = Intercept),
                       prior(normal(0, 1), class = b),
                       prior(lkj_corr_cholesky(1.5), class = cor)),
             iter = 2000, warmup = 1000, chains = 4, cores = 4,
             control = list(adapt_delta = .975, max_treedepth = 20),
             seed = 190831)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(read6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: MV(gaussian, gaussian) 
##   Links: mu = identity; sigma = identity
##          mu = identity; sigma = identity 
## Formula: read | mi() ~ 1 + occasion + mi(homecog) + (1 + occasion | id) 
##          homecog | mi() ~ 1 
##    Data: curran_dat_missing (Number of observations: 1393) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 405) 
##                                   Estimate Est.Error l-95% CI u-95% CI Rhat
## sd(read_Intercept)                    0.75      0.04     0.68     0.83 1.00
## sd(read_occasion)                     0.27      0.02     0.22     0.32 1.01
## cor(read_Intercept,read_occasion)     0.25      0.12     0.03     0.51 1.00
##                                   Bulk_ESS Tail_ESS
## sd(read_Intercept)                    1360     2987
## sd(read_occasion)                      770     1589
## cor(read_Intercept,read_occasion)      960     1492
## 
## Population-Level Effects: 
##                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## read_Intercept        2.15      0.16     1.84     2.47 1.00     2028     2534
## homecog_Intercept     8.91      0.07     8.77     9.05 1.00     9040     2687
## read_occasion         1.12      0.02     1.08     1.16 1.00     4599     2956
## read_mihomecog        0.06      0.02     0.03     0.09 1.00     2066     2596
## 
## Family Specific Parameters: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma_read        0.59      0.02     0.56     0.62 1.00      943     2256
## sigma_homecog     2.57      0.05     2.47     2.66 1.00    12854     2668
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Output is all well and good, but to really see what’s going on we’ll return to our sampling technique. Here are the fitted values on 3 individuals from Model 5 (non-imputed) and the same 3 from Model 6 (imputed).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- curran_dat %&amp;gt;%
  bind_cols(as_tibble(fitted(read5))) %&amp;gt;%
  filter(id %in% c(1048, 1050, 1498)) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 5, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate), shape = 1, size = 5, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Unimputed Data&amp;quot;) +
  scale_x_continuous(expand = c(.1, .1), breaks = 0:3) +
  scale_y_continuous(limits = c(2, 9)) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 12) +
  theme(plot.title = element_text(hjust = .5))

p2 &amp;lt;- curran_dat_missing %&amp;gt;%
  bind_cols(as_tibble(fitted(read6))) %&amp;gt;%
  filter(id %in% c(1048, 1050, 1498)) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = occasion, y = read), size = 5, alpha = .75, color = &amp;quot;dodgerblue2&amp;quot;) +
  geom_point(aes(x = occasion, y = Estimate.read), shape = 1, size = 5, stroke = 1.5) +
  labs(x = &amp;quot;Assessment Period&amp;quot;,
       y = &amp;quot;Reading Ability&amp;quot;,
       title = &amp;quot;Bayesian Imputation&amp;quot;) +
  scale_x_continuous(expand = c(.1, .1), breaks = 0:3) +
  scale_y_continuous(limits = c(2, 9)) +
  facet_wrap(~id, nrow = 1) +
  theme_bw(base_size = 12) +
  theme(plot.title = element_text(hjust = .5))

library(ggpubr)

figure &amp;lt;- ggarrange(p1, p2)

annotate_figure(figure, bottom = text_grob(&amp;quot;Blue points are observed values. Black circles are fitted values.&amp;quot;, hjust = 1, x = 1, face = &amp;quot;italic&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bayesian_mlm/Bayesian_MLM_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So we get reasonable estimates for the missing values. Our work here is done.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-words&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final Words&lt;/h1&gt;
&lt;p&gt;This topic is so vast that I couldn’t possibly cover everything there is to say about Bayesian Linear Mixed Models. We only scratched the surface on interpreting Bayesian posteriors. For example, &lt;em&gt;brms&lt;/em&gt; offers formal ways of comparing models and I highly recommend looking into these.
I’m indebted to Richard McElreath’s fantastic lectures series for all that I know on the topic. Honestly, if you can spare the time, they’re so so worth it - plus they’re freely available on YouTube &lt;a href=&#34;https://www.youtube.com/watch?v=4WVelCswXo4&amp;amp;list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI&#34;&gt;here&lt;/a&gt;. Watch these and then follow along with the code in this &lt;em&gt;brms&lt;/em&gt; guide &lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/&#34;&gt;here&lt;/a&gt; and you’re golden! Finally, if you want a short primer on Bayesian MLM and an excellent guide for writing Stan code (not &lt;em&gt;brms&lt;/em&gt;), I’d highly recommend this &lt;a href=&#34;https://www.tqmp.org/RegularArticles/vol12-3/p175/p175.pdf&#34;&gt;paper&lt;/a&gt; by Sorensen, Hohenstein, and Vasishth (2016). It helped immensely in my ongoing transition from Frequentist to Bayesian statistics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.psychstatistics.com/2014/12/27/d-lkj-priors/&#34;&gt;Baldwin, S. (2014). &lt;em&gt;Visualizing the LKJ Correlation Distribution.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jflournoy.github.io/assets/pdf/srcdmeth.pdf&#34;&gt;Curran, P. J. (1997). Comparing Three Modern Approaches to Longitudinal Data Analysis: An Examination of a Single Developmental Sample. &lt;em&gt;Symposium Conducted at the 1997 Biennial Meeting of the Society for Research in Child Development, Washington, D.C.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/&#34;&gt;Kurz, A. S. (2019). &lt;em&gt;Statistical Rethinking with brms, ggplot2, and the tidyverse.&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.tqmp.org/RegularArticles/vol12-3/p175/&#34;&gt;Sorensen, T., Hohenstein, S., &amp;amp; Vasishth, S. (2016). Bayesian linear mixed models using Stan: A tutorial for psychologists, linguists, and cognitive scientists. &lt;em&gt;The Quantitative Methods for Psychology&lt;/em&gt;, 175-200. doi: 10.20982/tqmp.12.3.p175&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0732118X18300746?via%3Dihub&#34;&gt;van Zyl, C. J. J. (2018). Frequentist and Bayesian inference: A conceptual primer. &lt;em&gt;New Ideas in Psychology, 51&lt;/em&gt;, 44-49.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>R</category>
      
            <category>Bayesian</category>
      
            <category>Linear Mixed Models</category>
      
            <category>Multilevel Modeling</category>
      
            <category>Tutorial</category>
      
      
            <category>R</category>
      
            <category>Bayesian</category>
      
            <category>Linear Mixed Models</category>
      
            <category>Multilevel Modeling</category>
      
            <category>Tutorial</category>
      
    </item>
    
  </channel>
</rss>
